{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6afd852",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16073aaf",
   "metadata": {},
   "source": [
    "#### colab setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73153521",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "pip install -q \\\n",
    "    torch>=2.9.1 \\\n",
    "    torchaudio>=2.9.1 \\\n",
    "    transformers>=4.57.0 \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad2fb727",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "12.8\n"
     ]
    }
   ],
   "source": [
    "# check cuda version colab and set as variable\n",
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.version.cuda)\n",
    "cuda_version = torch.version.cuda\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125933bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# colab only setup uncomment to install dependencies\n",
    "%%bash\n",
    "cuda_version=$(python -c \"import torch; print(torch.version.cuda)\")\n",
    "pip install -q \\\n",
    "    k2==1.24.4.dev20251118+cuda${cuda_version}.torch2.9.1 -f https://k2-fsa.github.io/k2/cuda.html \\\n",
    "    soundfile==0.31.1 \\\n",
    "    librosa==0.11.0 \\\n",
    "    pyarabic==0.6.10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e482814",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87381bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import k2\n",
    "import soundfile as sf\n",
    "import librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e085600c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting environment information...\n",
      "\n",
      "k2 version: 1.24.4\n",
      "Build type: Release\n",
      "Git SHA1: 30c3039fbe89f245d5dba3c47e99abc3a638275f\n",
      "Git date: Tue Nov 18 07:41:31 2025\n",
      "Cuda used to build k2: 12.8\n",
      "cuDNN used to build k2: \n",
      "Python version used to build k2: 3.12\n",
      "OS used to build k2: AlmaLinux release 8.10 (Cerulean Leopard)\n",
      "CMake version: 4.1.2\n",
      "GCC version: 13.3.1\n",
      "CMAKE_CUDA_FLAGS: -Wno-deprecated-gpu-targets -lineinfo --expt-extended-lambda -use_fast_math -Xptxas=-w --expt-extended-lambda -gencode arch=compute_50,code=sm_50 -lineinfo --expt-extended-lambda -use_fast_math -Xptxas=-w --expt-extended-lambda -gencode arch=compute_60,code=sm_60 -lineinfo --expt-extended-lambda -use_fast_math -Xptxas=-w --expt-extended-lambda -gencode arch=compute_61,code=sm_61 -lineinfo --expt-extended-lambda -use_fast_math -Xptxas=-w --expt-extended-lambda -gencode arch=compute_70,code=sm_70 -lineinfo --expt-extended-lambda -use_fast_math -Xptxas=-w --expt-extended-lambda -gencode arch=compute_75,code=sm_75 -lineinfo --expt-extended-lambda -use_fast_math -Xptxas=-w --expt-extended-lambda -gencode arch=compute_80,code=sm_80 -lineinfo --expt-extended-lambda -use_fast_math -Xptxas=-w --expt-extended-lambda -gencode arch=compute_86,code=sm_86 -DONNX_NAMESPACE=onnx_c2 -gencode arch=compute_50,code=sm_50 -gencode arch=compute_80,code=sm_80 -gencode arch=compute_86,code=sm_86 -gencode arch=compute_89,code=sm_89 -gencode arch=compute_90,code=sm_90 -gencode arch=compute_90a,code=sm_90a -gencode arch=compute_100,code=sm_100 -gencode arch=compute_100a,code=sm_100a -gencode arch=compute_101a,code=sm_101a -gencode arch=compute_120,code=sm_120 -gencode arch=compute_120a,code=sm_120a -Xcudafe --diag_suppress=cc_clobber_ignored,--diag_suppress=field_without_dll_interface,--diag_suppress=base_class_has_different_dll_interface,--diag_suppress=dll_interface_conflict_none_assumed,--diag_suppress=dll_interface_conflict_dllexport_assumed,--diag_suppress=bad_friend_decl --expt-relaxed-constexpr --expt-extended-lambda  --compiler-options -Wall  --compiler-options -Wno-strict-overflow  --compiler-options -Wno-unknown-pragmas \n",
      "CMAKE_CXX_FLAGS:   -Wno-unused-variable  -Wno-strict-overflow \n",
      "PyTorch version used to build k2: 2.9.1+cu128\n",
      "PyTorch is using Cuda: 12.8\n",
      "NVTX enabled: True\n",
      "With CUDA: True\n",
      "Disable debug: True\n",
      "Sync kernels : False\n",
      "Disable checks: False\n",
      "Max cpu memory allocate: 214748364800 bytes (or 200.0 GB)\n",
      "k2 abort: False\n",
      "__file__: /home/rufael/Projects/diac-btc/.venv/lib/python3.12/site-packages/k2/version/version.py\n",
      "_k2.__file__: /home/rufael/Projects/diac-btc/.venv/lib/python3.12/site-packages/_k2.cpython-312-x86_64-linux-gnu.so\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "import k2.version\n",
    "k2.version.version.main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "049b8196",
   "metadata": {},
   "source": [
    "## English Exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "631706dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rufael/Projects/diac-btc/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded: facebook/wav2vec2-base-960h vocab size: 32\n"
     ]
    }
   ],
   "source": [
    "import torch, k2, soundfile as sf, numpy as np\n",
    "from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC, AutoModelForCTC\n",
    "\n",
    "\n",
    "model_name = \"facebook/wav2vec2-base-960h\"\n",
    "processor  = Wav2Vec2Processor.from_pretrained(model_name)\n",
    "model      = AutoModelForCTC.from_pretrained(model_name).cuda().eval()\n",
    "\n",
    "vocab = list(processor.tokenizer.get_vocab().keys())\n",
    "id2tok = {v: k for k, v in processor.tokenizer.get_vocab().items()}\n",
    "blank_id = processor.tokenizer.pad_token_id\n",
    "\n",
    "if blank_id is None:\n",
    "        blank_id = processor.tokenizer.word_delimiter_token_id\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "print(\"Model loaded:\", model_name, \"vocab size:\", vocab_size)\n",
    "\n",
    "def get_log_probs(path):\n",
    "    wav, sr = sf.read(path)\n",
    "    if sr != 16000:\n",
    "        import torchaudio\n",
    "        wav = torchaudio.functional.resample(torch.tensor(wav).float(), sr, 16000).numpy()\n",
    "        sr = 16000\n",
    "    inputs = processor(wav, sampling_rate=sr, return_tensors=\"pt\", padding=True)\n",
    "    with torch.no_grad():\n",
    "        logits = model(inputs.input_values.cuda()).logits[0]\n",
    "    log_probs = torch.log_softmax(logits, dim=-1).cpu()\n",
    "    return log_probs\n",
    "\n",
    "def get_logits(audio_path):\n",
    "    \"\"\"Load audio and get model logits.\"\"\"\n",
    "    wav, sr = sf.read(audio_path)\n",
    "    \n",
    "    # Resample if needed\n",
    "    if sr != 16000:\n",
    "        import librosa\n",
    "        wav = librosa.resample(wav, orig_sr=sr, target_sr=16000)\n",
    "        sr = 16000\n",
    "    \n",
    "    # Handle stereo\n",
    "    if len(wav.shape) > 1:\n",
    "        wav = wav[:, 0]\n",
    "    \n",
    "    # Get logits\n",
    "    inputs = processor(wav, sampling_rate=sr, return_tensors=\"pt\", padding=True)\n",
    "    with torch.no_grad():\n",
    "        logits = model(inputs.input_values.cuda()).logits[0].cpu().numpy()\n",
    "    \n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ac4922f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import torch, k2, soundfile as sf, numpy as np\n",
    "from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC, AutoModelForCTC\n",
    "\n",
    "\n",
    "model_name = \"facebook/wav2vec2-base-960h\"\n",
    "processor  = Wav2Vec2Processor.from_pretrained(model_name)\n",
    "model      = AutoModelForCTC.from_pretrained(model_name).cuda().eval()\n",
    "\n",
    "vocab = list(processor.tokenizer.get_vocab().keys())\n",
    "id2tok = {v: k for k, v in processor.tokenizer.get_vocab().items()}\n",
    "blank_id = processor.tokenizer.pad_token_id\n",
    "\n",
    "def build_pattern_fsa(pattern, token2id, wildcard_ids):\n",
    "    \"\"\"\n",
    "    pattern: list of characters, '.' for wildcard\n",
    "    token2id: dict mapping from char -> token id\n",
    "    wildcard_ids: allowed token ids for wildcard positions\n",
    "    \"\"\"\n",
    "    arcs = []\n",
    "    state = 0\n",
    "    for i, ch in enumerate(pattern):\n",
    "        if ch == '.':\n",
    "            for wid in wildcard_ids:\n",
    "                arcs.append(f\"{state} {state+1} {wid} {wid} 0.0\")\n",
    "        else:\n",
    "            if ch not in token2id:\n",
    "                continue\n",
    "            tid = token2id[ch]\n",
    "            arcs.append(f\"{state} {state+1} {tid} {tid} 0.0\")\n",
    "        state += 1\n",
    "    arcs.append(f\"{state} 0.0\")\n",
    "    txt = \"\\n\".join(arcs)\n",
    "    fsa = k2.Fsa.from_str(txt, acceptor=False, openfst=True)\n",
    "    return k2.arc_sort(fsa)\n",
    "\n",
    "def wildcard_decode_k2_old(log_probs, pattern, wildcard_set):\n",
    "    \"\"\"\n",
    "    log_probs: (T, V) log probabilities\n",
    "    pattern: list of characters (with '.')\n",
    "    wildcard_set: list of allowed tokens for '.'\n",
    "    \"\"\"\n",
    "    T, V = log_probs.shape\n",
    "    dense = k2.DenseFsaVec(log_probs.unsqueeze(0), torch.tensor([[0, 0, T]], dtype=torch.int32))\n",
    "    ctc_topo = k2.arc_sort(k2.ctc_topo(V-1))\n",
    "    pattern_fsa = build_pattern_fsa(pattern, processor.tokenizer.get_vocab(), wildcard_set)\n",
    "    decoding_graph = k2.arc_sort(k2.compose(ctc_topo, pattern_fsa))\n",
    "    lattice = k2.intersect_dense_pruned(\n",
    "       decoding_graph, dense,\n",
    "       search_beam=20.0, output_beam=8.0,\n",
    "       min_active_states=30, max_active_states=10000)\n",
    "    # lattice = k2.intersect_dense(\n",
    "    #     decoding_graph, dense, output_beam=10.0)\n",
    "    best_path = k2.shortest_path(lattice, use_double_scores=False)\n",
    "    aux = k2.get_aux_labels(best_path)[0]\n",
    "    hyp_ids = [x for x in aux if x >= 0]\n",
    "    return \"\".join(id2tok[i] for i in hyp_ids)\n",
    "\n",
    "def wildcard_decode_k2(logits, pattern, wildcard_set):\n",
    "    \"\"\"\n",
    "    logits: (T, V) raw logits from model\n",
    "    pattern: list of characters (with '.')\n",
    "    wildcard_set: list of allowed tokens for '.'\n",
    "    \"\"\"\n",
    "    # Convert to torch tensor if needed\n",
    "    if isinstance(logits, np.ndarray):\n",
    "        logits = torch.from_numpy(logits)\n",
    "    \n",
    "    # Convert logits to log probabilities\n",
    "    log_probs = torch.log_softmax(logits, dim=-1)\n",
    "    \n",
    "    T, V = log_probs.shape\n",
    "    dense = k2.DenseFsaVec(log_probs.unsqueeze(0), torch.tensor([[0, 0, T]], dtype=torch.int32))\n",
    "    ctc_topo = k2.arc_sort(k2.ctc_topo(V-1))\n",
    "    pattern_fsa = build_pattern_fsa(pattern, processor.tokenizer.get_vocab(), wildcard_set)\n",
    "    decoding_graph = k2.arc_sort(k2.compose(ctc_topo, pattern_fsa))\n",
    "    lattice = k2.intersect_dense_pruned(\n",
    "       decoding_graph, dense,\n",
    "       search_beam=20.0, output_beam=8.0,\n",
    "       min_active_states=30, max_active_states=10000)\n",
    "    \n",
    "    best_path = k2.shortest_path(lattice, use_double_scores=False)\n",
    "    aux = k2.get_aux_labels(best_path)[0]\n",
    "    hyp_ids = [x for x in aux if x >= 0]\n",
    "    \n",
    "    result = \"\".join(id2tok[i] for i in hyp_ids)\n",
    "    \n",
    "    # Replace word delimiter with space\n",
    "    word_delim = processor.tokenizer.word_delimiter_token\n",
    "    if word_delim:\n",
    "        result = result.replace(word_delim, \" \")\n",
    "    \n",
    "    return result.strip()\n",
    "\n",
    "#Regular CTC Decode:\n",
    "def ctc_decode_k2_old(log_probs, search_beam=20.0, output_beam=8.0):\n",
    "    \"\"\"\n",
    "    CTC decoding using k2 with same beam settings (no pattern constraints).\n",
    "    This is equivalent to your constrained decoding but without the pattern FSA.\n",
    "    \"\"\"\n",
    "    T, V = log_probs.shape\n",
    "    dense = k2.DenseFsaVec(log_probs.unsqueeze(0), torch.tensor([[0, 0, T]], dtype=torch.int32))\n",
    "    ctc_topo = k2.arc_sort(k2.ctc_topo(V-1))\n",
    "    \n",
    "    # No pattern FSA - just CTC topology\n",
    "    lattice = k2.intersect_dense_pruned(\n",
    "        ctc_topo, dense,\n",
    "        search_beam=search_beam, \n",
    "        output_beam=output_beam,\n",
    "        min_active_states=30, \n",
    "        max_active_states=10000\n",
    "    )\n",
    "    \n",
    "    best_path = k2.shortest_path(lattice, use_double_scores=False)\n",
    "    aux = k2.get_aux_labels(best_path)[0]\n",
    "    hyp_ids = [x for x in aux if x >= 0]\n",
    "    return \"\".join(id2tok[i] for i in hyp_ids)\n",
    "\n",
    "def ctc_decode_k2(logits, search_beam=20.0, output_beam=8.0):\n",
    "    \"\"\"\n",
    "    CTC decoding using k2 with beam settings (no pattern constraints).\n",
    "    Takes raw logits as input.\n",
    "    \"\"\"\n",
    "    # Convert to torch tensor if needed\n",
    "    if isinstance(logits, np.ndarray):\n",
    "        logits = torch.from_numpy(logits)\n",
    "    \n",
    "    # Convert logits to log probabilities\n",
    "    log_probs = torch.log_softmax(logits, dim=-1)\n",
    "    \n",
    "    T, V = log_probs.shape\n",
    "    dense = k2.DenseFsaVec(log_probs.unsqueeze(0), torch.tensor([[0, 0, T]], dtype=torch.int32))\n",
    "    ctc_topo = k2.arc_sort(k2.ctc_topo(V-1))\n",
    "    \n",
    "    # No pattern FSA - just CTC topology\n",
    "    lattice = k2.intersect_dense_pruned(\n",
    "        ctc_topo, dense,\n",
    "        search_beam=search_beam, \n",
    "        output_beam=output_beam,\n",
    "        min_active_states=30, \n",
    "        max_active_states=10000\n",
    "    )\n",
    "    \n",
    "    best_path = k2.shortest_path(lattice, use_double_scores=False)\n",
    "    aux = k2.get_aux_labels(best_path)[0]\n",
    "    hyp_ids = [x for x in aux if x >= 0]\n",
    "    \n",
    "    result = \"\".join(id2tok[i] for i in hyp_ids)\n",
    "    \n",
    "    # Replace word delimiter with space\n",
    "    word_delim = processor.tokenizer.word_delimiter_token\n",
    "    if word_delim:\n",
    "        result = result.replace(word_delim, \" \")\n",
    "    \n",
    "    return result.strip()\n",
    "\n",
    "def ctc_decode_greedy(logits):\n",
    "    \"\"\"\n",
    "    Greedy CTC decoding: argmax at each frame, then collapse repeats and remove blanks.\n",
    "    Takes raw logits as input (not log_probs).\n",
    "    \"\"\"\n",
    "    # Convert to torch tensor if needed\n",
    "    if isinstance(logits, np.ndarray):\n",
    "        logits = torch.from_numpy(logits)\n",
    "    \n",
    "    # Get the most probable token at each frame (argmax on logits)\n",
    "    greedy_ids = logits.argmax(dim=-1)  # Shape: (T,)\n",
    "    \n",
    "    # Collapse repeats and remove blanks\n",
    "    output = []\n",
    "    prev_id = None\n",
    "    \n",
    "    for token_id in greedy_ids.tolist():\n",
    "        if token_id == blank_id:\n",
    "            prev_id = None  # Reset on blank\n",
    "            continue\n",
    "        if token_id != prev_id:  # Only add if different from previous\n",
    "            output.append(token_id)\n",
    "            prev_id = token_id\n",
    "    \n",
    "    result = \"\".join(id2tok[i] for i in output)\n",
    "    \n",
    "    # Replace word delimiter with space\n",
    "    word_delim = processor.tokenizer.word_delimiter_token\n",
    "    if word_delim:\n",
    "        result = result.replace(word_delim, \" \")\n",
    "    \n",
    "    return result.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0455e5a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added space token '|' to wildcard set\n",
      "Reference : THE|BIRCH|CANOE|SLID|ON|THE|SMOOTH|PLANK|GLUE|THE|SHEET|TO|THE|DARK|BLUE|BACKGROUND|IT|IS|EASY|TO|TELL|THE|DEPTH|OF|THE|WELL|THESE|DAYS|A|CHICKEN|LEG|IS|A|RARE|DISH|RICE|IS|OFTEN|SERVED|IN|ROUND|BOWLS|THE|JUSE|OF|LEMON|MAKES|FINE|PUNCH|THE|BOX|WAS|TONL||BESIDE|THE|PARK|TRUNK|THE|HOX|ARE|SED|CHOPPED|CORN|AND|GARBAGE|FOUR|HOURS|A|STEADY|WORK|FACED|US|A|LARGE|SIDE|IN|STOCKINGS|IS|HARD|TO|SELL\n",
      "CTC prediction  THE BIRCH CANOE SLIT ON THE SMOOTH PLANK GLE THE HEE TO THE DARK BLUE BACKGROUND IT IS EASY TO TELL THE DEPTH OF THE WELL THESE DAYS A CICK A MEG IS A RARE DISH RICE IS OXEN SERVED IN ROUND BULL THE JUSE OF LONDONS MAKES FINE PUNCH THE BOX WAS TONL  BESIDE THE PARK TRUK THE HOX ARE SED CHOPPED CORN AND GARBAGE FOUR HOURS A STEADY WORK FACED US E LARGE SIDE AN STOCKINGS IS HARD TO SELL\n",
      "CTC greedy prediction  THE BIRCH CANOE SLIT ON THE SMOOTH PLANK GLE THE HEE TO THE DARK BLUE BACKGROUND IT IS EASY TO TELL THE DEPTH OF THE WELL THESE DAYS A CICK A MEG IS A RARE DISH RICE IS OXEN SERVED IN ROUND BULL THE JUSE OF LONDONS MAKES FINE PUNCH THE BOX WAS TONL  BESIDE THE PARK TRUK THE HOX ARE SED CHOPPED CORN AND GARBAGE FOUR HOURS A STEADY WORK FACED US E LARGE SIDE AN STOCKINGS IS HARD TO SELL\n",
      "Pattern (vowels masked)     : TH.|B.RCH|C.N..|SL.D|.N|TH.|SM..TH|PL.NK|GL..|TH.|SH..T|T.|TH.|D.RK|BL..|B.CKGR..ND|.T|.S|..SY|T.|T.LL|TH.|D.PTH|.F|TH.|W.LL|TH.S.|D.YS|.|CH.CK.N|L.G|.S|.|R.R.|D.SH|R.C.|.S|.FT.N|S.RV.D|.N|R..ND|B.WLS|TH.|J.S.|.F|L.M.N|M.K.S|F.N.|P.NCH|TH.|B.X|W.S|T.NL||B.S.D.|TH.|P.RK|TR.NK|TH.|H.X|.R.|S.D|CH.PP.D|C.RN|.ND|G.RB.G.|F..R|H..RS|.|ST..DY|W.RK|F.C.D|.S|.|L.RG.|S.D.|.N|ST.CK.NGS|.S|H.RD|T.|S.LL\n",
      "Prediction (vowels filled)  : THE BIRCH CANOE SLID ON THE SMOOTH PLANK GLEW THE SHEET TO THE DARK BLUE BACKGROUND IT IS EASY TO TELL THE DEPTH OF THE WELL THESE DAYS A CHICKAN LEG IS A RARE DISH RICE IS OFTEN SERVED IN ROUND BUWLS THE JUSE OF LOMON MAKES FINE PUNCH THE BOX WAS TONL  BESIDE THE PARK TRUNK THE HOX ARE SED CHOPPED CORN AND GARBAGE FOUR HOURS A STEADY WORK FACED US E LARGE SIDE AN STOCKINGS IS HARD TO SELL\n",
      "Pattern (every other masked): T.E.B.R.H.C.N.E.S.I.|.N.T.E.S.O.T.|.L.N.|.L.E.T.E.S.E.T.T.|.H.|.A.K.B.U.|.A.K.R.U.D.I.|.S.E.S.|.O.T.L.|.H.|.E.T.|.F.T.E.W.L.|.H.S.|.A.S.A.C.I.K.N.L.G.I.|.|.A.E.D.S.|.I.E.I.|.F.E.|.E.V.D.I.|.O.N.|.O.L.|.H.|.U.E.O.|.E.O.|.A.E.|.I.E.P.N.H.T.E.B.X.W.S.T.N.|.B.S.D.|.H.|.A.K.T.U.K.T.E.H.X.A.E.S.D.C.O.P.D.C.R.|.N.|.A.B.G.|.O.R.H.U.S.A.S.E.D.|.O.K.F.C.D.U.|.|.A.G.|.I.E.I.|.T.C.I.G.|.S.H.R.|.O.S.L.\n",
      "Prediction (filled)         : THE BIRCH CANOE SLIT ON THE SMOOTH PLANK GLUE THE SHEET TO THE DARK BLUE BACKGROUND IT IS EASY TO TELL THE DEPTH OF THE WELL THESE DAYS A CHICKAN LEG IS A RARE DISH RICE IS OF EN SERVED IN ROUND BOULL THE JUSE OF LENON MAKES FINE PUNCH THE BOX WAS TONL  BESIDE THE PARK TRUCK THE HOX ARE SED CHOPPED CORN AND GARBAGE FOUR HOURS A STEADY WORK FACED US E LARGE SIDE IN STOCKINGS IS HARD TO SEL\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#test\n",
    "audio_path = \"samples/test_english.wav\"\n",
    "reference  = \"THE BIRCH CANOE SLID ON THE SMOOTH PLANK GLUE THE SHEET TO THE DARK BLUE BACKGROUND IT IS EASY TO TELL THE DEPTH OF THE WELL THESE DAYS A CHICKEN LEG IS A RARE DISH RICE IS OFTEN SERVED IN ROUND BOWLS THE JUSE OF LEMON MAKES FINE PUNCH THE BOX WAS TONL  BESIDE THE PARK TRUNK THE HOX ARE SED CHOPPED CORN AND GARBAGE FOUR HOURS A STEADY WORK FACED US A LARGE SIDE IN STOCKINGS IS HARD TO SELL\" \n",
    "\n",
    "# Mask all vowels\n",
    "reference = reference.replace(' ', '|')\n",
    "pattern_vowels = [\".\" if ch in \"AEIOUaeiou\" else ch for ch in reference]\n",
    "# Every other char\n",
    "pattern_everyother = [ch if i % 2 == 0 else '.' for i, ch in enumerate(reference)]\n",
    "\n",
    "log_probs = get_log_probs(audio_path)\n",
    "logits = get_logits(audio_path)\n",
    "# wildcard set = all lowercase letters\n",
    "wildcard_ids = [processor.tokenizer.get_vocab()[ch] for ch in processor.tokenizer.get_vocab().keys()\n",
    "                if ch.isalpha() and len(ch) == 1]\n",
    "\n",
    "\n",
    "# Add the space token to wildcard_ids\n",
    "space_token = '|'\n",
    "if space_token in processor.tokenizer.get_vocab():\n",
    "    wildcard_ids.append(processor.tokenizer.get_vocab()[space_token])\n",
    "    print(f\"Added space token '{space_token}' to wildcard set\")\n",
    "\n",
    "\n",
    "    \n",
    "regular_output = ctc_decode_k2(logits, search_beam=20.0, output_beam=8.0)\n",
    "greedy_output = ctc_decode_greedy(logits)\n",
    "hyp_vowels = wildcard_decode_k2(logits, pattern_vowels, wildcard_ids)\n",
    "hyp_everyother = wildcard_decode_k2(logits, pattern_everyother, wildcard_ids)\n",
    "\n",
    "print(\"Reference :\", reference)\n",
    "print(\"CTC prediction \", regular_output)\n",
    "print(\"CTC greedy prediction \", greedy_output)\n",
    "print(\"Pattern (vowels masked)     :\", \"\".join(pattern_vowels))\n",
    "print(\"Prediction (vowels filled)  :\", hyp_vowels)\n",
    "print(\"Pattern (every other masked):\", \"\".join(pattern_everyother))\n",
    "print(\"Prediction (filled)         :\", hyp_everyother)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e215b84",
   "metadata": {},
   "source": [
    "## Diacritization exps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845e7806",
   "metadata": {},
   "source": [
    "### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "067f558e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, k2, soundfile as sf, numpy as np\n",
    "from jiwer import wer\n",
    "from pyarabic import araby\n",
    "\n",
    "diacritics = araby.DIACRITICS\n",
    "arabic_letters = [ch for ch in araby.LETTERS if ch not in araby.DIACRITICS]\n",
    "precomposed = ['\\u0627', '\\u064A', '\\u0648']\n",
    "wildcard_token = '.'\n",
    "\n",
    "\n",
    "def build_pattern_fsa(pattern, wildcard_ids, token2id):\n",
    "    \"\"\"\n",
    "    pattern: list of characters, '.' for wildcard\n",
    "    token2id: dict mapping from char -> token id\n",
    "    wildcard_ids: allowed token ids for wildcard positions\n",
    "    \"\"\"\n",
    "    arcs = []\n",
    "    state = 0\n",
    "    for i, ch in enumerate(pattern):\n",
    "        if ch == '.':\n",
    "            for wid in wildcard_ids:\n",
    "                arcs.append(f\"{state} {state+1} {wid} {wid} 0.0\")\n",
    "        else:\n",
    "            if ch not in token2id:\n",
    "                continue\n",
    "            tid = token2id[ch]\n",
    "            arcs.append(f\"{state} {state+1} {tid} {tid} 0.0\")\n",
    "        state += 1\n",
    "    arcs.append(f\"{state} 0.0\")\n",
    "    txt = \"\\n\".join(arcs)\n",
    "    fsa = k2.Fsa.from_str(txt, acceptor=False, openfst=True)\n",
    "    return k2.arc_sort(fsa)\n",
    "\n",
    "# WFS Decoding\n",
    "def wildcard_decode_k2(logits, pattern, wildcard_set, token2id, word_delimiter_token=\"|\"):\n",
    "    \"\"\"\n",
    "    logits: (T, V) raw logits from model\n",
    "    pattern: list of characters (with '.')\n",
    "    wildcard_set: list of allowed tokens for '.'\n",
    "    \"\"\"\n",
    "    # Convert to torch tensor if needed\n",
    "    if isinstance(logits, np.ndarray):\n",
    "        logits = torch.from_numpy(logits)\n",
    "    \n",
    "    # Convert logits to log probabilities\n",
    "    log_probs = torch.log_softmax(logits, dim=-1)\n",
    "    \n",
    "    T, V = log_probs.shape\n",
    "    dense = k2.DenseFsaVec(log_probs.unsqueeze(0), torch.tensor([[0, 0, T]], dtype=torch.int32))\n",
    "    ctc_topo = k2.arc_sort(k2.ctc_topo(V-1))\n",
    "    pattern_fsa = build_pattern_fsa(pattern, wildcard_set, token2id)\n",
    "    decoding_graph = k2.arc_sort(k2.compose(ctc_topo, pattern_fsa))\n",
    "    lattice = k2.intersect_dense_pruned(\n",
    "       decoding_graph, dense,\n",
    "       search_beam=20.0, output_beam=8.0,\n",
    "       min_active_states=30, max_active_states=10000)\n",
    "    \n",
    "    best_path = k2.shortest_path(lattice, use_double_scores=False)\n",
    "    aux = k2.get_aux_labels(best_path)[0]\n",
    "    hyp_ids = [x for x in aux if x >= 0]\n",
    "    \n",
    "    result = \"\".join(id2tok[i] for i in hyp_ids)\n",
    "    \n",
    "    # Replace word delimiter with space\n",
    "    if word_delimiter_token:\n",
    "        result = result.replace(word_delimiter_token, \" \")\n",
    "    \n",
    "    \n",
    "    return result.strip()\n",
    "\n",
    "# CTC Decoding\n",
    "def ctc_decode_k2(logits, search_beam=20.0, output_beam=8.0, word_delimiter_token=\"|\"):\n",
    "    \"\"\"\n",
    "    CTC decoding using k2 with beam settings (no pattern constraints).\n",
    "    Takes raw logits as input.\n",
    "    \"\"\"\n",
    "    # Convert to torch tensor if needed\n",
    "    if isinstance(logits, np.ndarray):\n",
    "        logits = torch.from_numpy(logits)\n",
    "    \n",
    "    # Convert logits to log probabilities\n",
    "    log_probs = torch.log_softmax(logits, dim=-1)\n",
    "    \n",
    "    T, V = log_probs.shape\n",
    "    dense = k2.DenseFsaVec(log_probs.unsqueeze(0), torch.tensor([[0, 0, T]], dtype=torch.int32))\n",
    "    ctc_topo = k2.arc_sort(k2.ctc_topo(V-1))\n",
    "    \n",
    "    # No pattern FSA - just CTC topology\n",
    "    lattice = k2.intersect_dense_pruned(\n",
    "        ctc_topo, dense,\n",
    "        search_beam=search_beam, \n",
    "        output_beam=output_beam,\n",
    "        min_active_states=30, \n",
    "        max_active_states=10000\n",
    "    )\n",
    "    \n",
    "    best_path = k2.shortest_path(lattice, use_double_scores=False)\n",
    "    aux = k2.get_aux_labels(best_path)[0]\n",
    "    hyp_ids = [x for x in aux if x >= 0]\n",
    "    \n",
    "    result = \"\".join(id2tok[i] for i in hyp_ids)\n",
    "    \n",
    "    # Replace word delimiter with space\n",
    "    if word_delimiter_token:\n",
    "        result = result.replace(word_delimiter_token, \" \")\n",
    "    \n",
    "    return result.strip()\n",
    "\n",
    "def ctc_decode_greedy(logits, word_delimiter_token=\"|\"):\n",
    "    \"\"\"\n",
    "    Greedy CTC decoding: argmax at each frame, then collapse repeats and remove blanks.\n",
    "    Takes raw logits as input (not log_probs).\n",
    "    \"\"\"\n",
    "    # Convert to torch tensor if needed\n",
    "    if isinstance(logits, np.ndarray):\n",
    "        logits = torch.from_numpy(logits)\n",
    "    \n",
    "    # Get the most probable token at each frame (argmax on logits)\n",
    "    greedy_ids = logits.argmax(dim=-1)  # Shape: (T,)\n",
    "    \n",
    "    # Collapse repeats and remove blanks\n",
    "    output = []\n",
    "    prev_id = None\n",
    "    \n",
    "    for token_id in greedy_ids.tolist():\n",
    "        if token_id == blank_id:\n",
    "            prev_id = None  # Reset on blank\n",
    "            continue\n",
    "        if token_id != prev_id:  # Only add if different from previous\n",
    "            output.append(token_id)\n",
    "            prev_id = token_id\n",
    "    \n",
    "    result = \"\".join(id2tok[i] for i in output)\n",
    "    \n",
    "    # Replace word delimiter with space\n",
    "    if word_delimiter_token:\n",
    "        result = result.replace(word_delimiter_token, \" \")\n",
    "    \n",
    "    return result.strip()\n",
    "\n",
    "def print_util(data):\n",
    "    col_width = max(len(row[0]) for row in data) + 2\n",
    "    for key, value in data:\n",
    "        print(f\"{key:<{col_width}}: {value}\")\n",
    "    print()  # extra newline for separation\n",
    "\n",
    "def clean_text(text, remove_diacritics=False):\n",
    "    if remove_diacritics:\n",
    "        text = araby.strip_diacritics(text)\n",
    "    # replace | and - with space\n",
    "    text = text.replace('|', ' ').replace('-', ' ')\n",
    "    # collapse extra spaces\n",
    "    text = text.replace('  ', ' ')\n",
    "    return text\n",
    "\n",
    "def calculate_wer(hyp, ref):\n",
    "    if isinstance(hyp, list):\n",
    "        hyp = [clean_text(h) for h in hyp]\n",
    "        ref = [clean_text(r) for r in ref]\n",
    "    else:\n",
    "        hyp = clean_text(hyp)\n",
    "        ref = clean_text(ref)\n",
    "    return wer(ref, hyp)\n",
    "\n",
    "def construct_pattern(text):\n",
    "    text = text.replace(' ', space_token)\n",
    "    pattern = []\n",
    "    for c in text:\n",
    "        if c not in precomposed:\n",
    "            pattern.append(c)\n",
    "        else:\n",
    "            pattern.append(wildcard_token)\n",
    "        if c in arabic_letters:\n",
    "            pattern.append(wildcard_token)\n",
    "    return pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3fc8faf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ا', 'ي', 'و']\n"
     ]
    }
   ],
   "source": [
    "print(precomposed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dd1ee1e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ص', 'ا', 'ئ', 'ر', ' ', 'خ', 'ب', 'ر', 'ا', ' ', 'ف', 'ك', 'ن', ' ', 'خ', 'ب', 'ر', 'ا', ' ', 'ي', 'ر', 'و', 'ق', ' ', 'ج', 'م', 'ي', 'ل', 'ا']\n",
      "ص 0\n",
      ". 1\n",
      "ا 2\n",
      ". 3\n",
      "ئ 4\n",
      ". 5\n",
      "ر 6\n",
      ". 7\n",
      "| 8\n",
      "خ 9\n",
      ". 10\n",
      "ب 11\n",
      ". 12\n",
      "ر 13\n",
      ". 14\n",
      "ا 15\n",
      ". 16\n",
      "| 17\n",
      "ف 18\n",
      ". 19\n",
      "ك 20\n",
      ". 21\n",
      "ن 22\n",
      ". 23\n",
      "| 24\n",
      "خ 25\n",
      ". 26\n",
      "ب 27\n",
      ". 28\n",
      "ر 29\n",
      ". 30\n",
      "ا 31\n",
      ". 32\n",
      "| 33\n",
      "ي 34\n",
      ". 35\n",
      "ر 36\n",
      ". 37\n",
      "و 38\n",
      ". 39\n",
      "ق 40\n",
      ". 41\n",
      "| 42\n",
      "ج 43\n",
      ". 44\n",
      "م 45\n",
      ". 46\n",
      "ي 47\n",
      ". 48\n",
      "ل 49\n",
      ". 50\n",
      "ا 51\n",
      ". 52\n"
     ]
    }
   ],
   "source": [
    "text = araby.strip_diacritics(\"صَائِرٌ خَبَرًا فَكُنْ خَبَرًا يَرُوقُ جَمِيلَا\")\n",
    "print(list(text))\n",
    "pattern = construct_pattern(text)\n",
    "for i,ch in enumerate(pattern):\n",
    "    print(f\"{ch} {i}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de177edd",
   "metadata": {},
   "source": [
    "### Wav2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "829611ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rufael/Projects/diac-btc/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/rufael/Projects/diac-btc/.venv/lib/python3.12/site-packages/transformers/configuration_utils.py:309: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "diacritics in vocab: ['ً', 'ٌ', 'ٍ', 'َ', 'ُ', 'ِ', 'ّ', 'ْ']\n",
      "vocab size: 51\n",
      "blank_id: 0\n",
      "token2id: {'<pad>': 0, '<s>': 1, '</s>': 2, '<unk>': 3, '|': 4, '-': 5, 'ء': 6, 'آ': 7, 'أ': 8, 'ؤ': 9, 'إ': 10, 'ئ': 11, 'ا': 12, 'ب': 13, 'ة': 14, 'ت': 15, 'ث': 16, 'ج': 17, 'ح': 18, 'خ': 19, 'د': 20, 'ذ': 21, 'ر': 22, 'ز': 23, 'س': 24, 'ش': 25, 'ص': 26, 'ض': 27, 'ط': 28, 'ظ': 29, 'ع': 30, 'غ': 31, 'ـ': 32, 'ف': 33, 'ق': 34, 'ك': 35, 'ل': 36, 'م': 37, 'ن': 38, 'ه': 39, 'و': 40, 'ى': 41, 'ي': 42, 'ً': 43, 'ٌ': 44, 'ٍ': 45, 'َ': 46, 'ُ': 47, 'ِ': 48, 'ّ': 49, 'ْ': 50}\n",
      "constrained_wildcard_ids: [43, 44, 45, 46, 47, 48, 49, 50, 4, 0]\n",
      "unconstrained_wildcard_ids: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50]\n"
     ]
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoProcessor, AutoModelForCTC\n",
    "wav2vec_processor = AutoProcessor.from_pretrained(\"jonatasgrosman/wav2vec2-large-xlsr-53-arabic\")\n",
    "wav2vec_model = AutoModelForCTC.from_pretrained(\"jonatasgrosman/wav2vec2-large-xlsr-53-arabic\")\n",
    "\n",
    "vocab = list(wav2vec_processor.tokenizer.get_vocab().keys())\n",
    "id2tok = {v: k for k, v in wav2vec_processor.tokenizer.get_vocab().items()}\n",
    "token2id = {v: k for k, v in enumerate(vocab)}\n",
    "blank_id = wav2vec_processor.tokenizer.pad_token_id\n",
    "space_token = wav2vec_processor.tokenizer.word_delimiter_token\n",
    "\n",
    "diac_in_vocab = [diac for diac in diacritics if diac in vocab]\n",
    "\n",
    "unconstrained_wildcard_ids = [i for i,ch in enumerate(vocab)]\n",
    "constrained_wildcard_ids = [i for i,ch in enumerate(vocab) if ch in diac_in_vocab]\n",
    "constrained_wildcard_ids.append(wav2vec_processor.tokenizer.word_delimiter_token_id)\n",
    "constrained_wildcard_ids.append(blank_id)\n",
    "\n",
    "print(f\"diacritics in vocab: {diac_in_vocab}\")\n",
    "print(f\"vocab size: {len(vocab)}\")\n",
    "print(f\"blank_id: {blank_id}\")\n",
    "print(f\"token2id: {token2id}\")\n",
    "print(f\"constrained_wildcard_ids: {constrained_wildcard_ids}\")\n",
    "print(f\"unconstrained_wildcard_ids: {unconstrained_wildcard_ids}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65a57c7",
   "metadata": {},
   "source": [
    "#### Test single sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "17a6107a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "arabic_audio_path = \"samples/female_ab_00000.wav\"\n",
    "arabic_reference = open('samples/female_ab_00000.txt', 'r', encoding='utf-8').read()\n",
    "arabic_reference_no_diac = araby.strip_diacritics(arabic_reference)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "70750c56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hyp: فَمِنَ الرَّامِ السُّوَيْديِّ - أُسْكُرْ سَواهِنْ - الَّذِي شَرَكَ فِي أُولَمْبِيَادِ أَلْفُتِسُعْمِئَةٍ وَعِشْرونَ - حِينَ كَانَ يَبْلُغُ اثْنَيْنِ وَسَبرينَ عَاماً - وَمِئتَيْنِ وَوَاحِدُ وثَمانينَ يَوْمًا - إِلَى الفارسَتَيْنِ أَنْْيُوزْلَندِيَّةِ - جيولِي بروغْهَامْ - والأَسْتْرَاليَّةِ مارِي هَنَا - الَّتَيْنِ تُشَارِكَانِ فِي أُولَمْبِيَادِرِيُو - وَهُمَا فِي الْوَاحِدِ وَالسِّتّينَ مِنَ الْعُمُرِ\n",
      "ref: فَمِنَ الرَّامِي السُّوِيدِيِّ أُوسْكَار سَوَاهِنْ الَّذِي شَارَكَ فِي أُولُمْبِيَادِ  أَلْفْ وَتِسْعُمِئَةٍ وَعِشْرُونْ حِينَ كَانَ يَبْلُغُ إَثْنَينْ وَ سَبْعينَ  عَامًا وَمِئَتَينْ وَوَاحِدْ وَثَمَانِينَ يَوْمًا إِلَى الْفَارِسَتَيْنِ النِّيُوزِيلَنْدِيَّةِ جُولِي برُوغْهَام وَالْأُسْتُرَالِيَّةِ مَارِي هَانَا اللَّتَيْنِ تُشَارِكَانِ فِي أُولُمْبِيَّادِ رِيُو وَهُمَا فِي الْوَاحِدِ وَالسِّتِينَ مِنَ الْعُمُر\n",
      "wer: 0.717948717948718\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def get_logits(audio_path):\n",
    "    wav, sr = sf.read(audio_path)\n",
    "\n",
    "    if sr != 16000:\n",
    "        import librosa\n",
    "        wav = librosa.resample(wav, orig_sr=sr, target_sr=16000)\n",
    "\n",
    "    wav = wav[None, :]\n",
    "\n",
    "    processed_input = wav2vec_processor(\n",
    "        audio=wav,\n",
    "        sampling_rate=16000,\n",
    "        return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "    output = wav2vec_model(processed_input.input_values)\n",
    "    logits = output.logits\n",
    "    predicted_ids = torch.argmax(logits, dim=-1)\n",
    "\n",
    "    transcription = wav2vec_processor.decode(predicted_ids[0])\n",
    "\n",
    "    return logits, transcription\n",
    "\n",
    "logits, transcription = get_logits(arabic_audio_path)\n",
    "wer_sample = calculate_wer(arabic_reference, transcription)\n",
    "\n",
    "print(f\"hyp: {transcription}\")\n",
    "print(f\"ref: {arabic_reference}\")\n",
    "print(f\"wer: {wer_sample}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e13f4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "greedy_output  : فَمِنَ الرَّامِ السُّوَيْديِّ  أُسْكُرْ سَواهِنْ  الَّذِي شَرَكَ فِي أُولَمْبِيَادِ أَلْفُتِسُعْمِئَةٍ وَعِشْرونَ  حِينَ كَانَ يَبْلُغُ اثْنَيْنِ وَسَبرينَ عَاماً  وَمِئتَيْنِ وَوَاحِدُ وثَمانينَ يَوْمًا  إِلَى الفارسَتَيْنِ أَنْْيُوزْلَندِيَّةِ  جيولِي بروغْهَامْ  والأَسْتْرَاليَّةِ مارِي هَنَا  الَّتَيْنِ تُشَارِكَانِ فِي أُولَمْبِيَادِرِيُو  وَهُمَا فِي الْوَاحِدِ وَالسِّتّينَ مِنَ الْعُمُرِ\n",
      "transcription  : فَمِنَ الرَّامِ السُّوَيْديِّ  أُسْكُرْ سَواهِنْ  الَّذِي شَرَكَ فِي أُولَمْبِيَادِ أَلْفُتِسُعْمِئَةٍ وَعِشْرونَ  حِينَ كَانَ يَبْلُغُ اثْنَيْنِ وَسَبرينَ عَاماً  وَمِئتَيْنِ وَوَاحِدُ وثَمانينَ يَوْمًا  إِلَى الفارسَتَيْنِ أَنْْيُوزْلَندِيَّةِ  جيولِي بروغْهَامْ  والأَسْتْرَاليَّةِ مارِي هَنَا  الَّتَيْنِ تُشَارِكَانِ فِي أُولَمْبِيَادِرِيُو  وَهُمَا فِي الْوَاحِدِ وَالسِّتّينَ مِنَ الْعُمُرِ\n",
      "reference      : فَمِنَ الرَّامِي السُّوِيدِيِّ أُوسْكَار سَوَاهِنْ الَّذِي شَارَكَ فِي أُولُمْبِيَادِ أَلْفْ وَتِسْعُمِئَةٍ وَعِشْرُونْ حِينَ كَانَ يَبْلُغُ إَثْنَينْ وَ سَبْعينَ عَامًا وَمِئَتَينْ وَوَاحِدْ وَثَمَانِينَ يَوْمًا إِلَى الْفَارِسَتَيْنِ النِّيُوزِيلَنْدِيَّةِ جُولِي برُوغْهَام وَالْأُسْتُرَالِيَّةِ مَارِي هَانَا اللَّتَيْنِ تُشَارِكَانِ فِي أُولُمْبِيَّادِ رِيُو وَهُمَا فِي الْوَاحِدِ وَالسِّتِينَ مِنَ الْعُمُر\n",
      "wer            : 0.6666666666666666\n",
      "\n",
      "regular_output  : فَمِنَ الرَّامِ السُّوَيْديِّ  أُسْكُرْ سَواهِنْ  الَّذِي شَرَكَ فِي أُولَمْبِيَادِ أَلْفُتِسُعْمِئَةٍ وَعِشْرونَ  حِينَ كَانَ يَبْلُغُ اثْنَيْنِ وَسَبرينَ عَاماً  وَمِئتَيْنِ وَوَاحِدُ وثَمانينَ يَوْمًا  إِلَى الفارسَتَيْنِ أَنْْيُوزْلَندِيَّةِ  جيولِي بروغْهَامْ  والأَسْتْرَاليَّةِ مارِي هَنَا  الَّتَيْنِ تُشَارِكَانِ فِي أُولَمْبِيَادِرِيُو  وَهُمَا فِي الْوَاحِدِ وَالسِّتّينَ مِنَ الْعُمُرِ\n",
      "transcription   : فَمِنَ الرَّامِ السُّوَيْديِّ  أُسْكُرْ سَواهِنْ  الَّذِي شَرَكَ فِي أُولَمْبِيَادِ أَلْفُتِسُعْمِئَةٍ وَعِشْرونَ  حِينَ كَانَ يَبْلُغُ اثْنَيْنِ وَسَبرينَ عَاماً  وَمِئتَيْنِ وَوَاحِدُ وثَمانينَ يَوْمًا  إِلَى الفارسَتَيْنِ أَنْْيُوزْلَندِيَّةِ  جيولِي بروغْهَامْ  والأَسْتْرَاليَّةِ مارِي هَنَا  الَّتَيْنِ تُشَارِكَانِ فِي أُولَمْبِيَادِرِيُو  وَهُمَا فِي الْوَاحِدِ وَالسِّتّينَ مِنَ الْعُمُرِ\n",
      "reference       : فَمِنَ الرَّامِي السُّوِيدِيِّ أُوسْكَار سَوَاهِنْ الَّذِي شَارَكَ فِي أُولُمْبِيَادِ أَلْفْ وَتِسْعُمِئَةٍ وَعِشْرُونْ حِينَ كَانَ يَبْلُغُ إَثْنَينْ وَ سَبْعينَ عَامًا وَمِئَتَينْ وَوَاحِدْ وَثَمَانِينَ يَوْمًا إِلَى الْفَارِسَتَيْنِ النِّيُوزِيلَنْدِيَّةِ جُولِي برُوغْهَام وَالْأُسْتُرَالِيَّةِ مَارِي هَانَا اللَّتَيْنِ تُشَارِكَانِ فِي أُولُمْبِيَّادِ رِيُو وَهُمَا فِي الْوَاحِدِ وَالسِّتِينَ مِنَ الْعُمُر\n",
      "wer             : 0.6666666666666666\n",
      "\n",
      "pattern                     : ف.م.ن.|..ل.ر...م...|..ل.س.....د...|أ...س.ك...ر.|س.....ه.ن.|..ل.ذ...|ش...ر.ك.|ف...|أ...ل.م.ب.....د.||أ.ل.ف.|..ت.س.ع.م.ئ.ة.|..ع.ش.ر...ن.|ح...ن.|ك...ن.|..ب.ل.غ.|إ.ث.ن...ن.|..|س.ب.ع...ن.||ع...م...|..م.ئ.ت...ن.|......ح.د.|..ث.م...ن...ن.|....م...|إ.ل.ى.|..ل.ف...ر.س.ت...ن.|..ل.ن.....ز...ل.ن.د...ة.|ج...ل...|ب.ر...غ.ه...م.|....ل.أ.س.ت.ر...ل...ة.|م...ر...|ه...ن...|..ل.ل.ت...ن.|ت.ش...ر.ك...ن.|ف...|أ...ل.م.ب.....د.|ر.....|..ه.م...|ف...|..ل.....ح.د.|....ل.س.ت...ن.|م.ن.|..ل.ع.م.ر.\n",
      "hyp_diacritics_constrained  : فَمِنَ الرَّامِ السّوَيْديِّ أُسْكُرْ سَواهِنْ الّذِي شَرَكَ فِي أُولَمْبِيَادِ أَلْفُ تِسُعْمِئَةٍ وَعِشْرونَ حِينَ كَانَ يَبْلُغُ إثْنَيْنِ وَ سَبعينَ عَاماً وَمِئتَيْنِ وَوَاحِدُ وثَمانينَ يَوْمًا إِلَى الفارسَتَيْنِ َلْنْيُوزْلَنديَّةِ جيولِي بروغْهَامْ والأَسْتْرَاليَّةِ مارِي هَنَا الّلَتَيْنِ تُشَارِكَانِ فِي أُولَمْبِيَادِ رِيُو وَهُمَا فِي الْوَاحِدِ وَالسّتّينَ مِنَ الْعُمُر\n",
      "reference                   : فَمِنَ الرَّامِي السُّوِيدِيِّ أُوسْكَار سَوَاهِنْ الَّذِي شَارَكَ فِي أُولُمْبِيَادِ أَلْفْ وَتِسْعُمِئَةٍ وَعِشْرُونْ حِينَ كَانَ يَبْلُغُ إَثْنَينْ وَ سَبْعينَ عَامًا وَمِئَتَينْ وَوَاحِدْ وَثَمَانِينَ يَوْمًا إِلَى الْفَارِسَتَيْنِ النِّيُوزِيلَنْدِيَّةِ جُولِي برُوغْهَام وَالْأُسْتُرَالِيَّةِ مَارِي هَانَا اللَّتَيْنِ تُشَارِكَانِ فِي أُولُمْبِيَّادِ رِيُو وَهُمَا فِي الْوَاحِدِ وَالسِّتِينَ مِنَ الْعُمُر\n",
      "wer                         : 0.6190476190476191\n",
      "\n",
      "pattern                       : ف.م.ن.|..ل.ر...م...|..ل.س.....د...|أ...س.ك...ر.|س.....ه.ن.|..ل.ذ...|ش...ر.ك.|ف...|أ...ل.م.ب.....د.||أ.ل.ف.|..ت.س.ع.م.ئ.ة.|..ع.ش.ر...ن.|ح...ن.|ك...ن.|..ب.ل.غ.|إ.ث.ن...ن.|..|س.ب.ع...ن.||ع...م...|..م.ئ.ت...ن.|......ح.د.|..ث.م...ن...ن.|....م...|إ.ل.ى.|..ل.ف...ر.س.ت...ن.|..ل.ن.....ز...ل.ن.د...ة.|ج...ل...|ب.ر...غ.ه...م.|....ل.أ.س.ت.ر...ل...ة.|م...ر...|ه...ن...|..ل.ل.ت...ن.|ت.ش...ر.ك...ن.|ف...|أ...ل.م.ب.....د.|ر.....|..ه.م...|ف...|..ل.....ح.د.|....ل.س.ت...ن.|م.ن.|..ل.ع.م.ر.\n",
      "hyp_diacritics_unconstrained  : فَمِنَ الرَّامِ السّوَيْدي  أُسْكُرْ سَواهِنْ الّذِي شَرَكَ فِي أُولَمْبِيَادِ أَلْفُ تِسُعْمِئَةٍ وَعِشْرونَ حِينَ كَانَ يَبْلُغُ إثْنَيْنِ وَ سَبعينَ عَاما  وَمِئتَيْنِ وَوَاحِدُ وثَمانينَ يَوْما  إِلَى الفارسَتَيْنِ أَلْنْيُوزْلَنديَّةِ جيولِي بروغْهَامْ  والأَسْتْرَاليَّةِ مارِي هَنا  الّلَتَيْنِ تُشَارِكَانِ فِي أُولَمْبِيَادِ رِيُو وَهُمَا فِي الْوَاحِدِ وَالسّتّينَ مِنَ الْعُمُر\n",
      "reference                     : فَمِنَ الرَّامِي السُّوِيدِيِّ أُوسْكَار سَوَاهِنْ الَّذِي شَارَكَ فِي أُولُمْبِيَادِ أَلْفْ وَتِسْعُمِئَةٍ وَعِشْرُونْ حِينَ كَانَ يَبْلُغُ إَثْنَينْ وَ سَبْعينَ عَامًا وَمِئَتَينْ وَوَاحِدْ وَثَمَانِينَ يَوْمًا إِلَى الْفَارِسَتَيْنِ النِّيُوزِيلَنْدِيَّةِ جُولِي برُوغْهَام وَالْأُسْتُرَالِيَّةِ مَارِي هَانَا اللَّتَيْنِ تُشَارِكَانِ فِي أُولُمْبِيَّادِ رِيُو وَهُمَا فِي الْوَاحِدِ وَالسِّتِينَ مِنَ الْعُمُر\n",
      "wer                           : 0.6428571428571429\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#################### Greedy CTC ####################\n",
    "greedy_output = ctc_decode_greedy(logits.squeeze(0))\n",
    "data = [\n",
    "    (\"greedy_output\", clean_text(greedy_output)),\n",
    "    (\"transcription\",  clean_text(transcription)),\n",
    "    (\"reference\",      clean_text(arabic_reference)),\n",
    "    (\"wer\", calculate_wer(greedy_output, arabic_reference))\n",
    "]\n",
    "print_util(data)\n",
    "############################################################\n",
    "\n",
    "#################### CTC K2 ########################\n",
    "regular_output = ctc_decode_k2(logits.squeeze(0), search_beam=20.0, output_beam=8.0)\n",
    "data = [\n",
    "    (\"regular_output\", clean_text(regular_output)),\n",
    "    (\"transcription\",  clean_text(transcription)),\n",
    "    (\"reference\",      clean_text(arabic_reference)),\n",
    "    (\"wer\", calculate_wer(regular_output, arabic_reference))\n",
    "]\n",
    "print_util(data)\n",
    "############################################################\n",
    "\n",
    "#################### WFST constrained #############################\n",
    "pattern_diacritics = construct_pattern(arabic_reference_no_diac)\n",
    "hyp_diacritics_constrained = wildcard_decode_k2(logits.squeeze(0), pattern_diacritics, constrained_wildcard_ids, token2id)\n",
    "data = [\n",
    "    (\"pattern\", \"\".join(pattern_diacritics)),\n",
    "    (\"hyp_diacritics_constrained\", clean_text(hyp_diacritics_constrained)),\n",
    "    # (\"transcription\",  clean_text(transcription)),\n",
    "    (\"reference\",      clean_text(arabic_reference)),\n",
    "    (\"wer\", calculate_wer(hyp_diacritics_constrained, arabic_reference))\n",
    "]\n",
    "print_util(data)\n",
    "############################################################\n",
    "\n",
    "#################### WFST unconstrained #############################\n",
    "hyp_diacritics_unconstrained = wildcard_decode_k2(logits.squeeze(0), pattern_diacritics, unconstrained_wildcard_ids, token2id)\n",
    "data = [\n",
    "    (\"pattern\", \"\".join(pattern_diacritics)),\n",
    "    (\"hyp_diacritics_unconstrained\", clean_text(hyp_diacritics_unconstrained)),\n",
    "    # (\"transcription\",  clean_text(transcription)),\n",
    "    (\"reference\",      clean_text(arabic_reference)),\n",
    "    (\"wer\", calculate_wer(hyp_diacritics_unconstrained, arabic_reference))\n",
    "]\n",
    "print_util(data)\n",
    "############################################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478b85a2",
   "metadata": {},
   "source": [
    "#### Evaluate on ArVoice, ClArTTS, NADI-2025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6116d984",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, load_from_disk\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def collate_fn(batch):\n",
    "    # For batch_size=1, return the single item directly (no extra dimension)\n",
    "    if len(batch) == 1:\n",
    "        wav2vec_logits = torch.from_numpy(np.array(batch[0]['wav2vec_logits'][0])).float()\n",
    "        transcription = batch[0]['transcription'][0]\n",
    "        text = batch[0]['text']\n",
    "    else:\n",
    "        wav2vec_logits = torch.from_numpy(np.array([item['wav2vec_logits'][0] for item in batch])).float()\n",
    "        transcription = [item['transcription'][0] for item in batch]\n",
    "        text = [item['text'][0] for item in batch]\n",
    "    return wav2vec_logits, transcription, text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d544396",
   "metadata": {},
   "source": [
    "##### ClArTTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ab9147ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'file', 'audio', 'sampling_rate', 'duration', 'transcription', 'wav2vec_logits'],\n",
       "    num_rows: 205\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load MBZUAI/ClArTTS\n",
    "clartts_dataset = load_from_disk(\"/home/rufael/Projects/diac-btc/data/clartts/wav2vec/test\")\n",
    "clartts_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "51974cc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'الْبِرُّ صِلَةٌ وَمَعْرُوفٌ وَالْبِرُّ نَوْعَانِ'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import soundfile as sf\n",
    "\n",
    "# audio = clartts_dataset[17]['audio']\n",
    "# sr = clartts_dataset[17]['sampling_rate']\n",
    "# sf.write('clartts_17.wav', np.array(audio), sr)\n",
    "clartts_dataset[17]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "38d4586c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'clartts_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mjson\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtqdm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m test_dataloader = DataLoader(\u001b[43mclartts_dataset\u001b[49m, batch_size=\u001b[32m1\u001b[39m, shuffle=\u001b[38;5;28;01mFalse\u001b[39;00m, collate_fn=collate_fn)\n\u001b[32m      7\u001b[39m results = {}\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i,batch \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28menumerate\u001b[39m(test_dataloader)):\n",
      "\u001b[31mNameError\u001b[39m: name 'clartts_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "test_dataloader = DataLoader(clartts_dataset, batch_size=1, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "results = {}\n",
    "for i,batch in tqdm(enumerate(test_dataloader)):\n",
    "    logits, transcription, text = batch\n",
    "    \n",
    "    ref_no_diac = araby.strip_diacritics(text)\n",
    "    pattern = construct_pattern(ref_no_diac)\n",
    "\n",
    "    hyp_diacritics_unconstrained = wildcard_decode_k2(logits.squeeze(0), pattern, unconstrained_wildcard_ids, token2id, word_delimiter_token=space_token)\n",
    "    hyp_diacritics_constrained = wildcard_decode_k2(logits.squeeze(0), pattern, constrained_wildcard_ids, token2id, word_delimiter_token=space_token)\n",
    "    \n",
    "    results[f'{i}']={\n",
    "        'hyp_constrained': hyp_diacritics_constrained,\n",
    "        'hyp_unconstrained': hyp_diacritics_unconstrained,\n",
    "        'text': text,\n",
    "        'wav2vec_regular_output': transcription,\n",
    "        'pattern': \"\".join(pattern),\n",
    "        'unconstrained_wer': calculate_wer(hyp_diacritics_unconstrained, text),\n",
    "        'constrained_wer': calculate_wer(hyp_diacritics_constrained, text),\n",
    "        'baseline_wer': calculate_wer(transcription, text)\n",
    "    }\n",
    "\n",
    "# collect and dump the text, hyp_constrained, hyp_unconstrained to separate txt files\n",
    "with open('results/wav2vec/clartts/text.txt', 'w', encoding='utf-8') as f:\n",
    "    for i in results:\n",
    "        f.write(results[i]['text'] + '\\n')\n",
    "\n",
    "with open('results/wav2vec/clartts/hyp_constrained.txt', 'w', encoding='utf-8') as f:\n",
    "    for i in results:\n",
    "        f.write(results[i]['hyp_constrained'] + '\\n')\n",
    "\n",
    "with open('results/wav2vec/clartts/hyp_unconstrained.txt', 'w', encoding='utf-8') as f:\n",
    "    for i in results:\n",
    "        f.write(results[i]['hyp_unconstrained'] + '\\n')\n",
    "\n",
    "with open('results/wav2vec/clartts/transcription.txt', 'w', encoding='utf-8') as f:\n",
    "    for i in results:\n",
    "        f.write(results[i]['wav2vec_regular_output'] + '\\n')\n",
    "\n",
    "# collect wer results\n",
    "results[\"wer_constrained\"] = sum([results[i]['constrained_wer'] for i in results])/len(results)\n",
    "results[\"wer_unconstrained\"] = sum([results[i]['unconstrained_wer'] for i in results])/len(results)\n",
    "results[\"wer_baseline\"] = sum([results[i]['baseline_wer'] for i in results])/len(results)\n",
    "\n",
    "# dump results to json\n",
    "with open('results/wav2vec/clartts/results.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(results, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"WER constrained: {results['wer_constrained']}\")\n",
    "print(f\"WER unconstrained: {results['wer_unconstrained']}\")\n",
    "print(f\"WER baseline: {results['wer_baseline']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea5e649a",
   "metadata": {},
   "source": [
    "##### ArVoice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b9c8cb64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['file_name', 'transcription', 'speaker_id', 'source', 'wav2vec_logits'],\n",
       "    num_rows: 248\n",
       "})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arvoice_dataset = load_from_disk(\"/home/rufael/Projects/diac-btc/data/arvoice/test\")\n",
    "arvoice_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a97b87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([255, 51])\n",
      "تحتل ضواحي مدينة أدرارا صدارة الأحداث في الرواية\n"
     ]
    }
   ],
   "source": [
    "test_dataloader = DataLoader(arvoice_dataset, batch_size=1, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "for batch in test_dataloader:\n",
    "    logits, transcription = batch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d50f7e5",
   "metadata": {},
   "source": [
    "##### NADI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f7cdb33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens:  51\n",
      "Unique tokens: 51\n",
      "\n",
      "Multi-character tokens:\n",
      "   <pad>  len=5  cps=[U+003C U+0070 U+0061 U+0064 U+003E]\n",
      "     <s>  len=3  cps=[U+003C U+0073 U+003E]\n",
      "    </s>  len=4  cps=[U+003C U+002F U+0073 U+003E]\n",
      "   <unk>  len=5  cps=[U+003C U+0075 U+006E U+006B U+003E]\n",
      "\n",
      "Combining-mark tokens (standalone diacritics):\n",
      "  'ً'  name=ARABIC FATHATAN  cps=[U+064B]\n",
      "  'ٌ'  name=ARABIC DAMMATAN  cps=[U+064C]\n",
      "  'ٍ'  name=ARABIC KASRATAN  cps=[U+064D]\n",
      "  'َ'  name=ARABIC FATHA  cps=[U+064E]\n",
      "  'ُ'  name=ARABIC DAMMA  cps=[U+064F]\n",
      "  'ِ'  name=ARABIC KASRA  cps=[U+0650]\n",
      "  'ّ'  name=ARABIC SHADDA  cps=[U+0651]\n",
      "  'ْ'  name=ARABIC SUKUN  cps=[U+0652]\n",
      "\n",
      "Merged/precomposed single-codepoint tokens (NFD expands):\n",
      "  آ (ARABIC LETTER ALEF WITH MADDA ABOVE) [U+0622]  ->  آ [U+0627 U+0653]\n",
      "  أ (ARABIC LETTER ALEF WITH HAMZA ABOVE) [U+0623]  ->  أ [U+0627 U+0654]\n",
      "  ؤ (ARABIC LETTER WAW WITH HAMZA ABOVE) [U+0624]  ->  ؤ [U+0648 U+0654]\n",
      "  إ (ARABIC LETTER ALEF WITH HAMZA BELOW) [U+0625]  ->  إ [U+0627 U+0655]\n",
      "  ئ (ARABIC LETTER YEH WITH HAMZA ABOVE) [U+0626]  ->  ئ [U+064A U+0654]\n",
      "\n",
      "Merged/precomposed single-codepoint tokens (NFKD expands):\n",
      "  آ (ARABIC LETTER ALEF WITH MADDA ABOVE) [U+0622]  ->  آ [U+0627 U+0653]\n",
      "  أ (ARABIC LETTER ALEF WITH HAMZA ABOVE) [U+0623]  ->  أ [U+0627 U+0654]\n",
      "  ؤ (ARABIC LETTER WAW WITH HAMZA ABOVE) [U+0624]  ->  ؤ [U+0648 U+0654]\n",
      "  إ (ARABIC LETTER ALEF WITH HAMZA BELOW) [U+0625]  ->  إ [U+0627 U+0655]\n",
      "  ئ (ARABIC LETTER YEH WITH HAMZA ABOVE) [U+0626]  ->  ئ [U+064A U+0654]\n"
     ]
    }
   ],
   "source": [
    "import unicodedata as ud\n",
    "\n",
    "vocab = ['<pad>','<s>','</s>','<unk>','|','-','ء','آ','أ','ؤ','إ','ئ','ا','ب','ة','ت','ث','ج','ح','خ','د','ذ','ر','ز','س','ش','ص','ض','ط','ظ','ع','غ','ـ','ف','ق','ك','ل','م','ن','ه','و','ى','ي','ً','ٌ','ٍ','َ','ُ','ِ','ّ','ْ']\n",
    "\n",
    "def codepoints(s: str) -> str:\n",
    "    return \" \".join(f\"U+{ord(ch):04X}\" for ch in s)\n",
    "\n",
    "def is_combining_only(s: str) -> bool:\n",
    "    return len(s) > 0 and all(ud.combining(ch) != 0 for ch in s)\n",
    "\n",
    "def decomposes(s: str, form: str) -> bool:\n",
    "    return ud.normalize(form, s) != s\n",
    "\n",
    "def audit_vocab(vocab):\n",
    "    # duplicates\n",
    "    dupes = sorted({t for t in vocab if vocab.count(t) > 1})\n",
    "\n",
    "    multi_char = [t for t in vocab if len(t) > 1]\n",
    "\n",
    "    combining_tokens = [t for t in vocab if is_combining_only(t)]\n",
    "\n",
    "    # precomposed / \"merged\" single-codepoint tokens that expand under NFD/NFKD\n",
    "    merged_nfd = []\n",
    "    merged_nfkd = []\n",
    "    for t in vocab:\n",
    "        if len(t) == 1:\n",
    "            nfd = ud.normalize(\"NFD\", t)\n",
    "            nfkd = ud.normalize(\"NFKD\", t)\n",
    "            if nfd != t:\n",
    "                merged_nfd.append((t, nfd))\n",
    "            if nfkd != t:\n",
    "                merged_nfkd.append((t, nfkd))\n",
    "\n",
    "    return {\n",
    "        \"total\": len(vocab),\n",
    "        \"unique\": len(set(vocab)),\n",
    "        \"duplicates\": dupes,\n",
    "        \"multi_char_tokens\": multi_char,\n",
    "        \"combining_tokens\": combining_tokens,\n",
    "        \"merged_singletons_nfd\": merged_nfd,\n",
    "        \"merged_singletons_nfkd\": merged_nfkd,\n",
    "    }\n",
    "\n",
    "report = audit_vocab(vocab)\n",
    "\n",
    "print(f\"Total tokens:  {report['total']}\")\n",
    "print(f\"Unique tokens: {report['unique']}\")\n",
    "print()\n",
    "\n",
    "if report[\"duplicates\"]:\n",
    "    print(\"Duplicates:\")\n",
    "    for t in report[\"duplicates\"]:\n",
    "        print(f\"  {repr(t)}\")\n",
    "    print()\n",
    "\n",
    "print(\"Multi-character tokens:\")\n",
    "for t in report[\"multi_char_tokens\"]:\n",
    "    print(f\"  {t:>6}  len={len(t)}  cps=[{codepoints(t)}]\")\n",
    "print()\n",
    "\n",
    "print(\"Combining-mark tokens (standalone diacritics):\")\n",
    "for t in report[\"combining_tokens\"]:\n",
    "    ch = t\n",
    "    print(f\"  {repr(ch)}  name={ud.name(ch)}  cps=[{codepoints(ch)}]\")\n",
    "print()\n",
    "\n",
    "print(\"Merged/precomposed single-codepoint tokens (NFD expands):\")\n",
    "for t, nfd in report[\"merged_singletons_nfd\"]:\n",
    "    print(f\"  {t} ({ud.name(t)}) [{codepoints(t)}]  ->  {nfd} [{codepoints(nfd)}]\")\n",
    "print()\n",
    "\n",
    "print(\"Merged/precomposed single-codepoint tokens (NFKD expands):\")\n",
    "for t, nfkd in report[\"merged_singletons_nfkd\"]:\n",
    "    print(f\"  {t} ({ud.name(t)}) [{codepoints(t)}]  ->  {nfkd} [{codepoints(nfkd)}]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9f4b2ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens:  51\n",
      "Unique tokens: 51\n",
      "\n",
      "Breakdown:\n",
      "  special_token            4\n",
      "  precomposed_singleton    5\n",
      "  combining_mark           8\n",
      "  arabic_base_letter      31\n",
      "  arabic_other             1\n",
      "  punct_or_symbol          2\n",
      "\n",
      "Details (tokens in each bucket):\n",
      "\n",
      "[special_token] (4)\n",
      "  <pad>  [U+003C U+0070 U+0061 U+0064 U+003E]\n",
      "  <s>  [U+003C U+0073 U+003E]\n",
      "  </s>  [U+003C U+002F U+0073 U+003E]\n",
      "  <unk>  [U+003C U+0075 U+006E U+006B U+003E]\n",
      "\n",
      "[precomposed_singleton] (5)\n",
      "  آ  ARABIC LETTER ALEF WITH MADDA ABOVE  [U+0622]  NFD-> آ [U+0627 U+0653]\n",
      "  أ  ARABIC LETTER ALEF WITH HAMZA ABOVE  [U+0623]  NFD-> أ [U+0627 U+0654]\n",
      "  ؤ  ARABIC LETTER WAW WITH HAMZA ABOVE  [U+0624]  NFD-> ؤ [U+0648 U+0654]\n",
      "  إ  ARABIC LETTER ALEF WITH HAMZA BELOW  [U+0625]  NFD-> إ [U+0627 U+0655]\n",
      "  ئ  ARABIC LETTER YEH WITH HAMZA ABOVE  [U+0626]  NFD-> ئ [U+064A U+0654]\n",
      "\n",
      "[combining_mark] (8)\n",
      "  ً  ARABIC FATHATAN  [U+064B]\n",
      "  ٌ  ARABIC DAMMATAN  [U+064C]\n",
      "  ٍ  ARABIC KASRATAN  [U+064D]\n",
      "  َ  ARABIC FATHA  [U+064E]\n",
      "  ُ  ARABIC DAMMA  [U+064F]\n",
      "  ِ  ARABIC KASRA  [U+0650]\n",
      "  ّ  ARABIC SHADDA  [U+0651]\n",
      "  ْ  ARABIC SUKUN  [U+0652]\n",
      "\n",
      "[arabic_base_letter] (31)\n",
      "  ء  ARABIC LETTER HAMZA  [U+0621]\n",
      "  ا  ARABIC LETTER ALEF  [U+0627]\n",
      "  ب  ARABIC LETTER BEH  [U+0628]\n",
      "  ة  ARABIC LETTER TEH MARBUTA  [U+0629]\n",
      "  ت  ARABIC LETTER TEH  [U+062A]\n",
      "  ث  ARABIC LETTER THEH  [U+062B]\n",
      "  ج  ARABIC LETTER JEEM  [U+062C]\n",
      "  ح  ARABIC LETTER HAH  [U+062D]\n",
      "  خ  ARABIC LETTER KHAH  [U+062E]\n",
      "  د  ARABIC LETTER DAL  [U+062F]\n",
      "  ذ  ARABIC LETTER THAL  [U+0630]\n",
      "  ر  ARABIC LETTER REH  [U+0631]\n",
      "  ز  ARABIC LETTER ZAIN  [U+0632]\n",
      "  س  ARABIC LETTER SEEN  [U+0633]\n",
      "  ش  ARABIC LETTER SHEEN  [U+0634]\n",
      "  ص  ARABIC LETTER SAD  [U+0635]\n",
      "  ض  ARABIC LETTER DAD  [U+0636]\n",
      "  ط  ARABIC LETTER TAH  [U+0637]\n",
      "  ظ  ARABIC LETTER ZAH  [U+0638]\n",
      "  ع  ARABIC LETTER AIN  [U+0639]\n",
      "  غ  ARABIC LETTER GHAIN  [U+063A]\n",
      "  ف  ARABIC LETTER FEH  [U+0641]\n",
      "  ق  ARABIC LETTER QAF  [U+0642]\n",
      "  ك  ARABIC LETTER KAF  [U+0643]\n",
      "  ل  ARABIC LETTER LAM  [U+0644]\n",
      "  م  ARABIC LETTER MEEM  [U+0645]\n",
      "  ن  ARABIC LETTER NOON  [U+0646]\n",
      "  ه  ARABIC LETTER HEH  [U+0647]\n",
      "  و  ARABIC LETTER WAW  [U+0648]\n",
      "  ى  ARABIC LETTER ALEF MAKSURA  [U+0649]\n",
      "  ي  ARABIC LETTER YEH  [U+064A]\n",
      "\n",
      "[arabic_other] (1)\n",
      "  ـ  ARABIC TATWEEL  [U+0640]\n",
      "\n",
      "[punct_or_symbol] (2)\n",
      "  |  VERTICAL LINE  [U+007C]\n",
      "  -  HYPHEN-MINUS  [U+002D]\n"
     ]
    }
   ],
   "source": [
    "import unicodedata as ud\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "# ---- paste your vocab here ----\n",
    "vocab = ['<pad>','<s>','</s>','<unk>','|','-','ء','آ','أ','ؤ','إ','ئ','ا','ب','ة','ت','ث','ج','ح','خ','د','ذ','ر','ز','س','ش','ص','ض','ط','ظ','ع','غ','ـ','ف','ق','ك','ل','م','ن','ه','و','ى','ي','ً','ٌ','ٍ','َ','ُ','ِ','ّ','ْ']\n",
    "\n",
    "# ---- helpers ----\n",
    "SPECIAL_EXACT = {\"<pad>\", \"<s>\", \"</s>\", \"<unk>\"}\n",
    "\n",
    "def cps(s: str) -> str:\n",
    "    return \" \".join(f\"U+{ord(ch):04X}\" for ch in s)\n",
    "\n",
    "def is_combining_only(token: str) -> bool:\n",
    "    return len(token) > 0 and all(ud.combining(ch) != 0 for ch in token)\n",
    "\n",
    "def is_arabic_char(ch: str) -> bool:\n",
    "    # \"ARABIC\" in name is a decent heuristic for letters/diacritics/tatweel etc.\n",
    "    try:\n",
    "        return \"ARABIC\" in ud.name(ch)\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "def is_precomposed_singleton(token: str, form: str = \"NFD\") -> bool:\n",
    "    # Single codepoint that expands under normalization => \"precomposed/merged\"\n",
    "    return len(token) == 1 and ud.normalize(form, token) != token\n",
    "\n",
    "def is_base_letter_arabic(ch: str) -> bool:\n",
    "    # Arabic letters: category \"Lo\" (Letter, other)\n",
    "    return ud.category(ch) == \"Lo\" and is_arabic_char(ch)\n",
    "\n",
    "def is_punctuation_or_symbol(ch: str) -> bool:\n",
    "    return ud.category(ch).startswith((\"P\", \"S\"))\n",
    "\n",
    "def is_format_char(ch: str) -> bool:\n",
    "    return ud.category(ch) == \"Cf\"\n",
    "\n",
    "def is_whitespace(ch: str) -> bool:\n",
    "    return ud.category(ch) == \"Zs\" or ch.isspace()\n",
    "\n",
    "# ---- classification ----\n",
    "def classify_token(token: str) -> str:\n",
    "    # 1) exact special tokens\n",
    "    if token in SPECIAL_EXACT:\n",
    "        return \"special_token\"\n",
    "\n",
    "    # 2) multi-character (non-special) tokens\n",
    "    if len(token) > 1:\n",
    "        # you can choose to treat these as \"special_token\" too; kept separate for visibility\n",
    "        return \"multi_char_token\"\n",
    "\n",
    "    ch = token  # single char\n",
    "    cat = ud.category(ch)\n",
    "\n",
    "    # 3) standalone combining mark (diacritic)\n",
    "    if ud.combining(ch) != 0:\n",
    "        return \"combining_mark\"\n",
    "\n",
    "    # 4) precomposed/merged singleton (expands in NFD)\n",
    "    if is_precomposed_singleton(token, \"NFD\"):\n",
    "        return \"precomposed_singleton\"\n",
    "\n",
    "    # 5) Arabic base letter\n",
    "    if is_base_letter_arabic(ch):\n",
    "        return \"arabic_base_letter\"\n",
    "\n",
    "    # 6) Arabic non-letter (tatweel, etc.)\n",
    "    if is_arabic_char(ch):\n",
    "        return \"arabic_other\"\n",
    "\n",
    "    # 7) whitespace\n",
    "    if is_whitespace(ch):\n",
    "        return \"whitespace\"\n",
    "\n",
    "    # 8) punctuation/symbols\n",
    "    if is_punctuation_or_symbol(ch):\n",
    "        return \"punct_or_symbol\"\n",
    "\n",
    "    # 9) format/control/other\n",
    "    if is_format_char(ch):\n",
    "        return \"format_char\"\n",
    "    if cat.startswith(\"C\"):\n",
    "        return \"control_or_other\"\n",
    "\n",
    "    return \"other\"\n",
    "\n",
    "# ---- reporting ----\n",
    "def breakdown(vocab):\n",
    "    counts = Counter()\n",
    "    buckets = defaultdict(list)\n",
    "\n",
    "    for t in vocab:\n",
    "        label = classify_token(t)\n",
    "        counts[label] += 1\n",
    "        buckets[label].append(t)\n",
    "\n",
    "    total = len(vocab)\n",
    "    unique = len(set(vocab))\n",
    "    dupes = [t for t, c in Counter(vocab).items() if c > 1]\n",
    "\n",
    "    print(f\"Total tokens:  {total}\")\n",
    "    print(f\"Unique tokens: {unique}\")\n",
    "    if dupes:\n",
    "        print(f\"Duplicates:   {len(dupes)} -> {dupes}\")\n",
    "    print()\n",
    "\n",
    "    # pretty order\n",
    "    order = [\n",
    "        \"special_token\",\n",
    "        \"multi_char_token\",\n",
    "        \"precomposed_singleton\",\n",
    "        \"combining_mark\",\n",
    "        \"arabic_base_letter\",\n",
    "        \"arabic_other\",\n",
    "        \"punct_or_symbol\",\n",
    "        \"whitespace\",\n",
    "        \"format_char\",\n",
    "        \"control_or_other\",\n",
    "        \"other\",\n",
    "    ]\n",
    "\n",
    "    print(\"Breakdown:\")\n",
    "    for k in order:\n",
    "        if k in counts:\n",
    "            print(f\"  {k:22s} {counts[k]:3d}\")\n",
    "    print()\n",
    "\n",
    "    # optional: show members of each bucket\n",
    "    print(\"Details (tokens in each bucket):\")\n",
    "    for k in order:\n",
    "        if k not in counts:\n",
    "            continue\n",
    "        toks = buckets[k]\n",
    "        print(f\"\\n[{k}] ({len(toks)})\")\n",
    "        for t in toks:\n",
    "            if len(t) == 1:\n",
    "                try:\n",
    "                    name = ud.name(t)\n",
    "                except ValueError:\n",
    "                    name = \"<no name>\"\n",
    "                nfd = ud.normalize(\"NFD\", t)\n",
    "                extra = \"\"\n",
    "                if len(t) == 1 and nfd != t:\n",
    "                    extra = f\"  NFD-> {nfd} [{cps(nfd)}]\"\n",
    "                print(f\"  {t}  {name}  [{cps(t)}]{extra}\")\n",
    "            else:\n",
    "                print(f\"  {t}  [{cps(t)}]\")\n",
    "\n",
    "breakdown(vocab)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e06461",
   "metadata": {},
   "source": [
    "### Nemo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6379f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nemo.collections.asr as nemo_asr\n",
    "from nemo.collections.common.data.utils import move_data_to_device\n",
    "from omegaconf import open_dict\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "asr_model = nemo_asr.models.EncDecHybridRNNTCTCBPEModel.from_pretrained(model_name=\"nvidia/stt_ar_fastconformer_hybrid_large_pcd_v1.0\")\n",
    "# Configure for greedy CTC\n",
    "with open_dict(asr_model.cfg.decoding):\n",
    "    asr_model.cfg.decoding.strategy = \"greedy\"\n",
    "    asr_model.cfg.decoding.compute_timestamps = False # Optional\n",
    "asr_model.change_decoding_strategy(decoder_type=\"ctc\")\n",
    "\n",
    "# test model\n",
    "output = asr_model.transcribe([arabic_audio_path])\n",
    "\n",
    "wer_val = calculate_wer(output[0].text, arabic_reference)\n",
    "print(output[0].text)\n",
    "print(arabic_reference)\n",
    "print(f\"WER: {wer_val}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0723f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get logits for nemo\n",
    "def get_logits(audio_path):\n",
    "    wav, sr = sf.read(audio_path)\n",
    "    \n",
    "    if sr != 16000:\n",
    "        import librosa\n",
    "        wav = librosa.resample(wav, orig_sr=sr, target_sr=16000)\n",
    "        sr = 16000\n",
    "\n",
    "        # get length of wav\n",
    "        length = len(wav)\n",
    "    else:\n",
    "        length = len(wav)\n",
    "\n",
    "    # convert to tensor\n",
    "    wav = torch.from_numpy(wav).unsqueeze(0)\n",
    "    length = torch.tensor([length])\n",
    "\n",
    "    wav = move_data_to_device(wav, device)\n",
    "    length = move_data_to_device(length, device)\n",
    "    # get logits from asr_model\n",
    "    encoded, encoded_length = asr_model.forward(input_signal=wav, input_signal_length=length)\n",
    "    logits = asr_model.ctc_decoder(encoder_output=encoded)\n",
    "\n",
    "    hypotheses = asr_model.ctc_decoding.ctc_decoder_predictions_tensor(\n",
    "            logits,\n",
    "            encoded_length,\n",
    "            return_hypotheses=True,\n",
    "        )\n",
    "\n",
    "    return hypotheses, logits.squeeze(0)\n",
    "\n",
    "asr_model.change_decoding_strategy(asr_model.cfg.decoding, decoder_type=\"ctc\")\n",
    "\n",
    "hypotheses, logits = get_logits(arabic_audio_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af81ea05",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rufael/Projects/diac-btc/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[NeMo W 2026-01-26 06:15:54 nemo_logging:405] Megatron num_microbatches_calculator not found, using Apex version.\n",
      "OneLogger: Setting error_handling_strategy to DISABLE_QUIETLY_AND_REPORT_METRIC_ERROR for rank (rank=0) with OneLogger disabled. To override: explicitly set error_handling_strategy parameter.\n",
      "No exporters were provided. This means that no telemetry data will be collected.\n",
      "[NeMo W 2026-01-26 06:16:07 nemo_logging:405] Skipped conversion for config/subconfig:\n",
      "    {'manifest_filepath': '???', 'sample_rate': 16000, 'batch_size': 32, 'shuffle': True, 'num_workers': 8, 'pin_memory': True, 'max_duration': 20, 'min_duration': 0.5, 'is_tarred': True, 'tarred_audio_filepaths': '???', 'shuffle_n': 2048, 'bucketing_strategy': 'fully_randomized', 'bucketing_batch_size': None}\n",
      "     Reason: Missing mandatory value: train_ds.manifest_filepath\n",
      "        full_key: train_ds.manifest_filepath\n",
      "        object_type=dict.\n",
      "[NeMo W 2026-01-26 06:16:07 nemo_logging:405] Skipped conversion for config/subconfig:\n",
      "    {'manifest_filepath': '???', 'sample_rate': 16000, 'batch_size': 16, 'shuffle': False, 'use_start_end_token': False, 'num_workers': 8, 'pin_memory': True}\n",
      "     Reason: Missing mandatory value: validation_ds.manifest_filepath\n",
      "        full_key: validation_ds.manifest_filepath\n",
      "        object_type=dict.\n",
      "[NeMo W 2026-01-26 06:16:07 nemo_logging:405] Skipped conversion for config/subconfig:\n",
      "    {'manifest_filepath': '???', 'sample_rate': 16000, 'batch_size': 16, 'shuffle': False, 'use_start_end_token': False, 'num_workers': 8, 'pin_memory': True}\n",
      "     Reason: Missing mandatory value: test_ds.manifest_filepath\n",
      "        full_key: test_ds.manifest_filepath\n",
      "        object_type=dict.\n",
      "[NeMo W 2026-01-26 06:16:07 nemo_logging:405] Skipped conversion for config/subconfig:\n",
      "    {'dir': '???', 'type': 'bpe', 'model_path': 'nemo:980b4bb50d294835b2b6a342de0d0270_tokenizer.model', 'vocab_path': 'nemo:f682c43e1c7b44b49dbd55bf06a50629_vocab.txt', 'spe_tokenizer_vocab': 'nemo:37b9452975c2486ea97b15946bdfa17c_tokenizer.vocab'}\n",
      "     Reason: Missing mandatory value: tokenizer.dir\n",
      "        full_key: tokenizer.dir\n",
      "        object_type=dict.\n",
      "[NeMo W 2026-01-26 06:16:07 nemo_logging:405] Skipped conversion for config/subconfig:\n",
      "    {'manifest_filepath': '???', 'sample_rate': 16000, 'batch_size': 32, 'shuffle': True, 'num_workers': 8, 'pin_memory': True, 'max_duration': 20, 'min_duration': 0.5, 'is_tarred': True, 'tarred_audio_filepaths': '???', 'shuffle_n': 2048, 'bucketing_strategy': 'fully_randomized', 'bucketing_batch_size': None}\n",
      "     Reason: Missing mandatory value: train_ds.manifest_filepath\n",
      "        full_key: train_ds.manifest_filepath\n",
      "        object_type=dict.\n",
      "[NeMo W 2026-01-26 06:16:07 nemo_logging:405] Skipped conversion for config/subconfig:\n",
      "    {'manifest_filepath': '???', 'sample_rate': 16000, 'batch_size': 16, 'shuffle': False, 'use_start_end_token': False, 'num_workers': 8, 'pin_memory': True}\n",
      "     Reason: Missing mandatory value: validation_ds.manifest_filepath\n",
      "        full_key: validation_ds.manifest_filepath\n",
      "        object_type=dict.\n",
      "[NeMo W 2026-01-26 06:16:07 nemo_logging:405] Skipped conversion for config/subconfig:\n",
      "    {'manifest_filepath': '???', 'sample_rate': 16000, 'batch_size': 16, 'shuffle': False, 'use_start_end_token': False, 'num_workers': 8, 'pin_memory': True}\n",
      "     Reason: Missing mandatory value: test_ds.manifest_filepath\n",
      "        full_key: test_ds.manifest_filepath\n",
      "        object_type=dict.\n",
      "[NeMo W 2026-01-26 06:16:07 nemo_logging:405] Skipped conversion for config/subconfig:\n",
      "    {'dir': '???', 'type': 'bpe', 'model_path': 'nemo:980b4bb50d294835b2b6a342de0d0270_tokenizer.model', 'vocab_path': 'nemo:f682c43e1c7b44b49dbd55bf06a50629_vocab.txt', 'spe_tokenizer_vocab': 'nemo:37b9452975c2486ea97b15946bdfa17c_tokenizer.vocab'}\n",
      "     Reason: Missing mandatory value: tokenizer.dir\n",
      "        full_key: tokenizer.dir\n",
      "        object_type=dict.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2026-01-26 06:16:07 nemo_logging:393] Tokenizer SentencePieceTokenizer initialized with 1024 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2026-01-26 06:16:07 nemo_logging:405] Skipped conversion for config/subconfig:\n",
      "    {'manifest_filepath': '???', 'sample_rate': 16000, 'batch_size': 32, 'shuffle': True, 'num_workers': 8, 'pin_memory': True, 'max_duration': 20, 'min_duration': 0.5, 'is_tarred': True, 'tarred_audio_filepaths': '???', 'shuffle_n': 2048, 'bucketing_strategy': 'fully_randomized', 'bucketing_batch_size': None}\n",
      "     Reason: Missing mandatory value: train_ds.manifest_filepath\n",
      "        full_key: train_ds.manifest_filepath\n",
      "        object_type=dict.\n",
      "[NeMo W 2026-01-26 06:16:07 nemo_logging:405] Skipped conversion for config/subconfig:\n",
      "    {'manifest_filepath': '???', 'sample_rate': 16000, 'batch_size': 16, 'shuffle': False, 'use_start_end_token': False, 'num_workers': 8, 'pin_memory': True}\n",
      "     Reason: Missing mandatory value: validation_ds.manifest_filepath\n",
      "        full_key: validation_ds.manifest_filepath\n",
      "        object_type=dict.\n",
      "[NeMo W 2026-01-26 06:16:07 nemo_logging:405] Skipped conversion for config/subconfig:\n",
      "    {'manifest_filepath': '???', 'sample_rate': 16000, 'batch_size': 16, 'shuffle': False, 'use_start_end_token': False, 'num_workers': 8, 'pin_memory': True}\n",
      "     Reason: Missing mandatory value: test_ds.manifest_filepath\n",
      "        full_key: test_ds.manifest_filepath\n",
      "        object_type=dict.\n",
      "[NeMo W 2026-01-26 06:16:07 nemo_logging:405] Skipped conversion for config/subconfig:\n",
      "    {'dir': '???', 'type': 'bpe', 'model_path': 'nemo:980b4bb50d294835b2b6a342de0d0270_tokenizer.model', 'vocab_path': 'nemo:f682c43e1c7b44b49dbd55bf06a50629_vocab.txt', 'spe_tokenizer_vocab': 'nemo:37b9452975c2486ea97b15946bdfa17c_tokenizer.vocab'}\n",
      "     Reason: Missing mandatory value: tokenizer.dir\n",
      "        full_key: tokenizer.dir\n",
      "        object_type=dict.\n",
      "[NeMo W 2026-01-26 06:16:07 nemo_logging:405] Skipped conversion for config/subconfig:\n",
      "    {'manifest_filepath': '???', 'sample_rate': 16000, 'batch_size': 32, 'shuffle': True, 'num_workers': 8, 'pin_memory': True, 'max_duration': 20, 'min_duration': 0.5, 'is_tarred': True, 'tarred_audio_filepaths': '???', 'shuffle_n': 2048, 'bucketing_strategy': 'fully_randomized', 'bucketing_batch_size': None}\n",
      "     Reason: Missing mandatory value: train_ds.manifest_filepath\n",
      "        full_key: train_ds.manifest_filepath\n",
      "        object_type=dict.\n",
      "[NeMo W 2026-01-26 06:16:07 nemo_logging:405] Skipped conversion for config/subconfig:\n",
      "    {'manifest_filepath': '???', 'sample_rate': 16000, 'batch_size': 16, 'shuffle': False, 'use_start_end_token': False, 'num_workers': 8, 'pin_memory': True}\n",
      "     Reason: Missing mandatory value: validation_ds.manifest_filepath\n",
      "        full_key: validation_ds.manifest_filepath\n",
      "        object_type=dict.\n",
      "[NeMo W 2026-01-26 06:16:07 nemo_logging:405] Skipped conversion for config/subconfig:\n",
      "    {'manifest_filepath': '???', 'sample_rate': 16000, 'batch_size': 16, 'shuffle': False, 'use_start_end_token': False, 'num_workers': 8, 'pin_memory': True}\n",
      "     Reason: Missing mandatory value: test_ds.manifest_filepath\n",
      "        full_key: test_ds.manifest_filepath\n",
      "        object_type=dict.\n",
      "[NeMo W 2026-01-26 06:16:07 nemo_logging:405] Skipped conversion for config/subconfig:\n",
      "    {'dir': '???', 'type': 'bpe', 'model_path': 'nemo:980b4bb50d294835b2b6a342de0d0270_tokenizer.model', 'vocab_path': 'nemo:f682c43e1c7b44b49dbd55bf06a50629_vocab.txt', 'spe_tokenizer_vocab': 'nemo:37b9452975c2486ea97b15946bdfa17c_tokenizer.vocab'}\n",
      "     Reason: Missing mandatory value: tokenizer.dir\n",
      "        full_key: tokenizer.dir\n",
      "        object_type=dict.\n",
      "[NeMo W 2026-01-26 06:16:08 nemo_logging:405] If you intend to do training or fine-tuning, please call the ModelPT.setup_training_data() method and provide a valid configuration file to setup the train data loader.\n",
      "    Train config : \n",
      "    manifest_filepath: ???\n",
      "    sample_rate: 16000\n",
      "    batch_size: 32\n",
      "    shuffle: true\n",
      "    num_workers: 8\n",
      "    pin_memory: true\n",
      "    max_duration: 20\n",
      "    min_duration: 0.5\n",
      "    is_tarred: true\n",
      "    tarred_audio_filepaths: ???\n",
      "    shuffle_n: 2048\n",
      "    bucketing_strategy: fully_randomized\n",
      "    bucketing_batch_size: null\n",
      "    \n",
      "[NeMo W 2026-01-26 06:16:08 nemo_logging:405] If you intend to do validation, please call the ModelPT.setup_validation_data() or ModelPT.setup_multiple_validation_data() method and provide a valid configuration file to setup the validation data loader(s). \n",
      "    Validation config : \n",
      "    manifest_filepath: ???\n",
      "    sample_rate: 16000\n",
      "    batch_size: 16\n",
      "    shuffle: false\n",
      "    use_start_end_token: false\n",
      "    num_workers: 8\n",
      "    pin_memory: true\n",
      "    \n",
      "[NeMo W 2026-01-26 06:16:08 nemo_logging:405] Please call the ModelPT.setup_test_data() or ModelPT.setup_multiple_test_data() method and provide a valid configuration file to setup the test data loader(s).\n",
      "    Test config : \n",
      "    manifest_filepath: ???\n",
      "    sample_rate: 16000\n",
      "    batch_size: 16\n",
      "    shuffle: false\n",
      "    use_start_end_token: false\n",
      "    num_workers: 8\n",
      "    pin_memory: true\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2026-01-26 06:16:08 nemo_logging:393] PADDING: 0\n",
      "[NeMo I 2026-01-26 06:16:08 nemo_logging:393] Using RNNT Loss : warprnnt_numba\n",
      "    Loss warprnnt_numba_kwargs: {'fastemit_lambda': 0.0, 'clamp': -1.0}\n",
      "[NeMo I 2026-01-26 06:16:08 nemo_logging:393] Using RNNT Loss : warprnnt_numba\n",
      "    Loss warprnnt_numba_kwargs: {'fastemit_lambda': 0.0, 'clamp': -1.0}\n",
      "[NeMo I 2026-01-26 06:16:08 nemo_logging:393] Using RNNT Loss : warprnnt_numba\n",
      "    Loss warprnnt_numba_kwargs: {'fastemit_lambda': 0.0, 'clamp': -1.0}\n",
      "[NeMo I 2026-01-26 06:16:09 nemo_logging:393] Model EncDecHybridRNNTCTCBPEModel was successfully restored from /home/rufael/.cache/huggingface/hub/models--nvidia--stt_ar_fastconformer_hybrid_large_pc_v1.0/snapshots/3e748b4d91935672f19fc4d6cba38fff9ef013c5/stt_ar_fastconformer_hybrid_large_pc_v1.0.nemo.\n"
     ]
    }
   ],
   "source": [
    "import nemo.collections.asr as nemo_asr\n",
    "asr_model = nemo_asr.models.EncDecHybridRNNTCTCBPEModel.from_pretrained(model_name=\"nvidia/stt_ar_fastconformer_hybrid_large_pc_v1.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1dc9183",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EncDecHybridRNNTCTCBPEModel(\n",
       "  (preprocessor): AudioToMelSpectrogramPreprocessor(\n",
       "    (featurizer): FilterbankFeatures()\n",
       "  )\n",
       "  (encoder): ConformerEncoder(\n",
       "    (pre_encode): ConvSubsampling(\n",
       "      (out): Linear(in_features=2560, out_features=512, bias=True)\n",
       "      (conv): MaskedConvSequential(\n",
       "        (0): Conv2d(1, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256)\n",
       "        (3): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (4): ReLU(inplace=True)\n",
       "        (5): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256)\n",
       "        (6): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (7): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (pos_enc): RelPositionalEncoding(\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (layers): ModuleList(\n",
       "      (0-16): 17 x ConformerLayer(\n",
       "        (norm_feed_forward1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (feed_forward1): ConformerFeedForward(\n",
       "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (activation): Swish()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (norm_conv): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (conv): ConformerConvolution(\n",
       "          (pointwise_conv1): Conv1d(512, 1024, kernel_size=(1,), stride=(1,))\n",
       "          (depthwise_conv): CausalConv1D(512, 512, kernel_size=(9,), stride=(1,), groups=512)\n",
       "          (batch_norm): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (activation): Swish()\n",
       "          (pointwise_conv2): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "        (norm_self_att): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attn): RelPositionMultiHeadAttention(\n",
       "          (linear_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (linear_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (linear_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (linear_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear_pos): Linear(in_features=512, out_features=512, bias=False)\n",
       "        )\n",
       "        (norm_feed_forward2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (feed_forward2): ConformerFeedForward(\n",
       "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (activation): Swish()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (norm_out): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): RNNTDecoder(\n",
       "    (prediction): ModuleDict(\n",
       "      (embed): Embedding(1025, 640, padding_idx=1024)\n",
       "      (dec_rnn): LSTMDropout(\n",
       "        (lstm): LSTM(640, 640, dropout=0.2)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (joint): RNNTJoint(\n",
       "    (pred): Linear(in_features=640, out_features=640, bias=True)\n",
       "    (enc): Linear(in_features=512, out_features=640, bias=True)\n",
       "    (joint_net): Sequential(\n",
       "      (0): ReLU(inplace=True)\n",
       "      (1): Dropout(p=0.2, inplace=False)\n",
       "      (2): Linear(in_features=640, out_features=1025, bias=True)\n",
       "    )\n",
       "    (_loss): RNNTLoss(\n",
       "      (_loss): RNNTLossNumba()\n",
       "    )\n",
       "    (_wer): WER()\n",
       "  )\n",
       "  (loss): RNNTLoss(\n",
       "    (_loss): RNNTLossNumba()\n",
       "  )\n",
       "  (spec_augmentation): SpectrogramAugmentation(\n",
       "    (spec_augment): SpecAugment()\n",
       "  )\n",
       "  (wer): WER()\n",
       "  (ctc_decoder): ConvASRDecoder(\n",
       "    (decoder_layers): Sequential(\n",
       "      (0): Conv1d(512, 1025, kernel_size=(1,), stride=(1,))\n",
       "    )\n",
       "  )\n",
       "  (ctc_loss): CTCLoss()\n",
       "  (ctc_wer): WER()\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "asr_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2fe88e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
