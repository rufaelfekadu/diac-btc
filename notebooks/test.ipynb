{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6afd852",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "87381bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import k2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e085600c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting environment information...\n",
      "\n",
      "k2 version: 1.24.4\n",
      "Build type: Release\n",
      "Git SHA1: 30c3039fbe89f245d5dba3c47e99abc3a638275f\n",
      "Git date: Tue Nov 18 07:41:31 2025\n",
      "Cuda used to build k2: 12.8\n",
      "cuDNN used to build k2: \n",
      "Python version used to build k2: 3.12\n",
      "OS used to build k2: AlmaLinux release 8.10 (Cerulean Leopard)\n",
      "CMake version: 4.1.2\n",
      "GCC version: 13.3.1\n",
      "CMAKE_CUDA_FLAGS: -Wno-deprecated-gpu-targets -lineinfo --expt-extended-lambda -use_fast_math -Xptxas=-w --expt-extended-lambda -gencode arch=compute_50,code=sm_50 -lineinfo --expt-extended-lambda -use_fast_math -Xptxas=-w --expt-extended-lambda -gencode arch=compute_60,code=sm_60 -lineinfo --expt-extended-lambda -use_fast_math -Xptxas=-w --expt-extended-lambda -gencode arch=compute_61,code=sm_61 -lineinfo --expt-extended-lambda -use_fast_math -Xptxas=-w --expt-extended-lambda -gencode arch=compute_70,code=sm_70 -lineinfo --expt-extended-lambda -use_fast_math -Xptxas=-w --expt-extended-lambda -gencode arch=compute_75,code=sm_75 -lineinfo --expt-extended-lambda -use_fast_math -Xptxas=-w --expt-extended-lambda -gencode arch=compute_80,code=sm_80 -lineinfo --expt-extended-lambda -use_fast_math -Xptxas=-w --expt-extended-lambda -gencode arch=compute_86,code=sm_86 -DONNX_NAMESPACE=onnx_c2 -gencode arch=compute_50,code=sm_50 -gencode arch=compute_80,code=sm_80 -gencode arch=compute_86,code=sm_86 -gencode arch=compute_89,code=sm_89 -gencode arch=compute_90,code=sm_90 -gencode arch=compute_90a,code=sm_90a -gencode arch=compute_100,code=sm_100 -gencode arch=compute_100a,code=sm_100a -gencode arch=compute_101a,code=sm_101a -gencode arch=compute_120,code=sm_120 -gencode arch=compute_120a,code=sm_120a -Xcudafe --diag_suppress=cc_clobber_ignored,--diag_suppress=field_without_dll_interface,--diag_suppress=base_class_has_different_dll_interface,--diag_suppress=dll_interface_conflict_none_assumed,--diag_suppress=dll_interface_conflict_dllexport_assumed,--diag_suppress=bad_friend_decl --expt-relaxed-constexpr --expt-extended-lambda  --compiler-options -Wall  --compiler-options -Wno-strict-overflow  --compiler-options -Wno-unknown-pragmas \n",
      "CMAKE_CXX_FLAGS:   -Wno-unused-variable  -Wno-strict-overflow \n",
      "PyTorch version used to build k2: 2.9.1+cu128\n",
      "PyTorch is using Cuda: 12.8\n",
      "NVTX enabled: True\n",
      "With CUDA: True\n",
      "Disable debug: True\n",
      "Sync kernels : False\n",
      "Disable checks: False\n",
      "Max cpu memory allocate: 214748364800 bytes (or 200.0 GB)\n",
      "k2 abort: False\n",
      "__file__: /home/rufael/Projects/diac-btc/.venv/lib/python3.12/site-packages/k2/version/version.py\n",
      "_k2.__file__: /home/rufael/Projects/diac-btc/.venv/lib/python3.12/site-packages/_k2.cpython-312-x86_64-linux-gnu.so\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "import k2.version\n",
    "k2.version.version.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6445ddd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.43.0 (0)\n",
       " -->\n",
       "<!-- Title: WFSA Pages: 1 -->\n",
       "<svg width=\"324pt\" height=\"80pt\"\n",
       " viewBox=\"0.00 0.00 324.00 80.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 76)\">\n",
       "<title>WFSA</title>\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-76 320,-76 320,4 -4,4\"/>\n",
       "<!-- 0 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>0</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" stroke-width=\"2\" cx=\"18\" cy=\"-45\" rx=\"18\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"18\" y=\"-41.3\" font-family=\"Times,serif\" font-size=\"14.00\">0</text>\n",
       "</g>\n",
       "<!-- 1 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>1</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" stroke-width=\"2\" cx=\"196\" cy=\"-45\" rx=\"18\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"196\" y=\"-41.3\" font-family=\"Times,serif\" font-size=\"14.00\">1</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;1 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>0&#45;&gt;1</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M35.46,-49.93C40.41,-51.19 45.88,-52.37 51,-53 100.4,-59.08 113.6,-59.08 163,-53 164.76,-52.78 166.56,-52.5 168.37,-52.18\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"169.52,-55.5 178.54,-49.93 168.02,-48.67 169.52,-55.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"107\" y=\"-60.8\" font-family=\"Times,serif\" font-size=\"14.00\">1/1</text>\n",
       "</g>\n",
       "<!-- 2 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>2</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" stroke-width=\"2\" cx=\"107\" cy=\"-18\" rx=\"18\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"107\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">2</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;2 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>0&#45;&gt;2</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M35.36,-39.92C47.88,-36.04 65.39,-30.6 79.84,-26.12\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"81.2,-29.36 89.71,-23.06 79.12,-22.68 81.2,-29.36\"/>\n",
       "<text text-anchor=\"middle\" x=\"62.5\" y=\"-37.8\" font-family=\"Times,serif\" font-size=\"14.00\">2/2</text>\n",
       "</g>\n",
       "<!-- 3 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>3</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" stroke-width=\"2\" cx=\"294\" cy=\"-45\" rx=\"18\" ry=\"18\"/>\n",
       "<ellipse fill=\"none\" stroke=\"black\" stroke-width=\"2\" cx=\"294\" cy=\"-45\" rx=\"22\" ry=\"22\"/>\n",
       "<text text-anchor=\"middle\" x=\"294\" y=\"-41.3\" font-family=\"Times,serif\" font-size=\"14.00\">3</text>\n",
       "</g>\n",
       "<!-- 1&#45;&gt;3 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>1&#45;&gt;3</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M214.16,-45C227.31,-45 245.81,-45 261.58,-45\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"261.97,-48.5 271.97,-45 261.97,-41.5 261.97,-48.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"243\" y=\"-48.8\" font-family=\"Times,serif\" font-size=\"14.00\">&#45;1/3</text>\n",
       "</g>\n",
       "<!-- 2&#45;&gt;1 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>2&#45;&gt;1</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M124.36,-23.08C136.88,-26.96 154.39,-32.4 168.84,-36.88\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"168.12,-40.32 178.71,-39.94 170.2,-33.64 168.12,-40.32\"/>\n",
       "<text text-anchor=\"middle\" x=\"151.5\" y=\"-37.8\" font-family=\"Times,serif\" font-size=\"14.00\">3/4</text>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x7f38052dc620>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = '''\n",
    "0 1 1 1\n",
    "0 2 2 2\n",
    "1 3 -1 3\n",
    "2 1 3 4\n",
    "3\n",
    "'''\n",
    "fsa = k2.Fsa.from_str(s)\n",
    "k2.to_dot(fsa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e3b7f105",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.43.0 (0)\n",
       " -->\n",
       "<!-- Title: WFSA Pages: 1 -->\n",
       "<svg width=\"324pt\" height=\"80pt\"\n",
       " viewBox=\"0.00 0.00 324.00 80.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 76)\">\n",
       "<title>WFSA</title>\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-76 320,-76 320,4 -4,4\"/>\n",
       "<!-- 0 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>0</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" stroke-width=\"2\" cx=\"18\" cy=\"-45\" rx=\"18\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"18\" y=\"-41.3\" font-family=\"Times,serif\" font-size=\"14.00\">0</text>\n",
       "</g>\n",
       "<!-- 1 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>1</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" stroke-width=\"2\" cx=\"196\" cy=\"-45\" rx=\"18\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"196\" y=\"-41.3\" font-family=\"Times,serif\" font-size=\"14.00\">1</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;1 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>0&#45;&gt;1</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M35.46,-49.93C40.41,-51.19 45.88,-52.37 51,-53 100.4,-59.08 113.6,-59.08 163,-53 164.76,-52.78 166.56,-52.5 168.37,-52.18\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"169.52,-55.5 178.54,-49.93 168.02,-48.67 169.52,-55.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"107\" y=\"-60.8\" font-family=\"Times,serif\" font-size=\"14.00\">1/1</text>\n",
       "</g>\n",
       "<!-- 2 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>2</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" stroke-width=\"2\" cx=\"107\" cy=\"-18\" rx=\"18\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"107\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">2</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;2 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>0&#45;&gt;2</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M35.36,-39.92C47.88,-36.04 65.39,-30.6 79.84,-26.12\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"81.2,-29.36 89.71,-23.06 79.12,-22.68 81.2,-29.36\"/>\n",
       "<text text-anchor=\"middle\" x=\"62.5\" y=\"-37.8\" font-family=\"Times,serif\" font-size=\"14.00\">2/2</text>\n",
       "</g>\n",
       "<!-- 3 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>3</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" stroke-width=\"2\" cx=\"294\" cy=\"-45\" rx=\"18\" ry=\"18\"/>\n",
       "<ellipse fill=\"none\" stroke=\"black\" stroke-width=\"2\" cx=\"294\" cy=\"-45\" rx=\"22\" ry=\"22\"/>\n",
       "<text text-anchor=\"middle\" x=\"294\" y=\"-41.3\" font-family=\"Times,serif\" font-size=\"14.00\">3</text>\n",
       "</g>\n",
       "<!-- 1&#45;&gt;3 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>1&#45;&gt;3</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M214.16,-45C227.31,-45 245.81,-45 261.58,-45\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"261.97,-48.5 271.97,-45 261.97,-41.5 261.97,-48.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"243\" y=\"-48.8\" font-family=\"Times,serif\" font-size=\"14.00\">&#45;1/3</text>\n",
       "</g>\n",
       "<!-- 2&#45;&gt;1 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>2&#45;&gt;1</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M124.36,-23.08C136.88,-26.96 154.39,-32.4 168.84,-36.88\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"168.12,-40.32 178.71,-39.94 170.2,-33.64 168.12,-40.32\"/>\n",
       "<text text-anchor=\"middle\" x=\"151.5\" y=\"-37.8\" font-family=\"Times,serif\" font-size=\"14.00\">3/4</text>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x7f38052e32c0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syms = '''\n",
    "<eps> 0\n",
    "a 1\n",
    "b 2\n",
    "c 3\n",
    "'''\n",
    "fsa.symbols = k2.SymbolTable.from_str(syms)\n",
    "k2.to_dot(fsa)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "049b8196",
   "metadata": {},
   "source": [
    "### English Exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "631706dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rufael/Projects/diac-btc/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded: facebook/wav2vec2-base-960h vocab size: 32\n"
     ]
    }
   ],
   "source": [
    "import torch, k2, soundfile as sf, numpy as np\n",
    "from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC, AutoModelForCTC\n",
    "\n",
    "\n",
    "model_name = \"facebook/wav2vec2-base-960h\"\n",
    "processor  = Wav2Vec2Processor.from_pretrained(model_name)\n",
    "model      = AutoModelForCTC.from_pretrained(model_name).cuda().eval()\n",
    "\n",
    "vocab = list(processor.tokenizer.get_vocab().keys())\n",
    "id2tok = {v: k for k, v in processor.tokenizer.get_vocab().items()}\n",
    "blank_id = processor.tokenizer.pad_token_id\n",
    "\n",
    "if blank_id is None:\n",
    "        blank_id = processor.tokenizer.word_delimiter_token_id\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "print(\"Model loaded:\", model_name, \"vocab size:\", vocab_size)\n",
    "\n",
    "def get_log_probs(path):\n",
    "    wav, sr = sf.read(path)\n",
    "    if sr != 16000:\n",
    "        import torchaudio\n",
    "        wav = torchaudio.functional.resample(torch.tensor(wav).float(), sr, 16000).numpy()\n",
    "        sr = 16000\n",
    "    inputs = processor(wav, sampling_rate=sr, return_tensors=\"pt\", padding=True)\n",
    "    with torch.no_grad():\n",
    "        logits = model(inputs.input_values.cuda()).logits[0]\n",
    "    log_probs = torch.log_softmax(logits, dim=-1).cpu()\n",
    "    return log_probs\n",
    "\n",
    "def get_logits(audio_path):\n",
    "    \"\"\"Load audio and get model logits.\"\"\"\n",
    "    wav, sr = sf.read(audio_path)\n",
    "    \n",
    "    # Resample if needed\n",
    "    if sr != 16000:\n",
    "        import librosa\n",
    "        wav = librosa.resample(wav, orig_sr=sr, target_sr=16000)\n",
    "        sr = 16000\n",
    "    \n",
    "    # Handle stereo\n",
    "    if len(wav.shape) > 1:\n",
    "        wav = wav[:, 0]\n",
    "    \n",
    "    # Get logits\n",
    "    inputs = processor(wav, sampling_rate=sr, return_tensors=\"pt\", padding=True)\n",
    "    with torch.no_grad():\n",
    "        logits = model(inputs.input_values.cuda()).logits[0].cpu().numpy()\n",
    "    \n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ac4922f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import torch, k2, soundfile as sf, numpy as np\n",
    "from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC, AutoModelForCTC\n",
    "\n",
    "\n",
    "model_name = \"facebook/wav2vec2-base-960h\"\n",
    "processor  = Wav2Vec2Processor.from_pretrained(model_name)\n",
    "model      = AutoModelForCTC.from_pretrained(model_name).cuda().eval()\n",
    "\n",
    "vocab = list(processor.tokenizer.get_vocab().keys())\n",
    "id2tok = {v: k for k, v in processor.tokenizer.get_vocab().items()}\n",
    "blank_id = processor.tokenizer.pad_token_id\n",
    "\n",
    "def build_pattern_fsa(pattern, token2id, wildcard_ids):\n",
    "    \"\"\"\n",
    "    pattern: list of characters, '.' for wildcard\n",
    "    token2id: dict mapping from char -> token id\n",
    "    wildcard_ids: allowed token ids for wildcard positions\n",
    "    \"\"\"\n",
    "    arcs = []\n",
    "    state = 0\n",
    "    for i, ch in enumerate(pattern):\n",
    "        if ch == '.':\n",
    "            for wid in wildcard_ids:\n",
    "                arcs.append(f\"{state} {state+1} {wid} {wid} 0.0\")\n",
    "        else:\n",
    "            if ch not in token2id:\n",
    "                continue\n",
    "            tid = token2id[ch]\n",
    "            arcs.append(f\"{state} {state+1} {tid} {tid} 0.0\")\n",
    "        state += 1\n",
    "    arcs.append(f\"{state} 0.0\")\n",
    "    txt = \"\\n\".join(arcs)\n",
    "    fsa = k2.Fsa.from_str(txt, acceptor=False, openfst=True)\n",
    "    return k2.arc_sort(fsa)\n",
    "\n",
    "def wildcard_decode_k2_old(log_probs, pattern, wildcard_set):\n",
    "    \"\"\"\n",
    "    log_probs: (T, V) log probabilities\n",
    "    pattern: list of characters (with '.')\n",
    "    wildcard_set: list of allowed tokens for '.'\n",
    "    \"\"\"\n",
    "    T, V = log_probs.shape\n",
    "    dense = k2.DenseFsaVec(log_probs.unsqueeze(0), torch.tensor([[0, 0, T]], dtype=torch.int32))\n",
    "    ctc_topo = k2.arc_sort(k2.ctc_topo(V-1))\n",
    "    pattern_fsa = build_pattern_fsa(pattern, processor.tokenizer.get_vocab(), wildcard_set)\n",
    "    decoding_graph = k2.arc_sort(k2.compose(ctc_topo, pattern_fsa))\n",
    "    lattice = k2.intersect_dense_pruned(\n",
    "       decoding_graph, dense,\n",
    "       search_beam=20.0, output_beam=8.0,\n",
    "       min_active_states=30, max_active_states=10000)\n",
    "    # lattice = k2.intersect_dense(\n",
    "    #     decoding_graph, dense, output_beam=10.0)\n",
    "    best_path = k2.shortest_path(lattice, use_double_scores=False)\n",
    "    aux = k2.get_aux_labels(best_path)[0]\n",
    "    hyp_ids = [x for x in aux if x >= 0]\n",
    "    return \"\".join(id2tok[i] for i in hyp_ids)\n",
    "\n",
    "def wildcard_decode_k2(logits, pattern, wildcard_set):\n",
    "    \"\"\"\n",
    "    logits: (T, V) raw logits from model\n",
    "    pattern: list of characters (with '.')\n",
    "    wildcard_set: list of allowed tokens for '.'\n",
    "    \"\"\"\n",
    "    # Convert to torch tensor if needed\n",
    "    if isinstance(logits, np.ndarray):\n",
    "        logits = torch.from_numpy(logits)\n",
    "    \n",
    "    # Convert logits to log probabilities\n",
    "    log_probs = torch.log_softmax(logits, dim=-1)\n",
    "    \n",
    "    T, V = log_probs.shape\n",
    "    dense = k2.DenseFsaVec(log_probs.unsqueeze(0), torch.tensor([[0, 0, T]], dtype=torch.int32))\n",
    "    ctc_topo = k2.arc_sort(k2.ctc_topo(V-1))\n",
    "    pattern_fsa = build_pattern_fsa(pattern, processor.tokenizer.get_vocab(), wildcard_set)\n",
    "    decoding_graph = k2.arc_sort(k2.compose(ctc_topo, pattern_fsa))\n",
    "    lattice = k2.intersect_dense_pruned(\n",
    "       decoding_graph, dense,\n",
    "       search_beam=20.0, output_beam=8.0,\n",
    "       min_active_states=30, max_active_states=10000)\n",
    "    \n",
    "    best_path = k2.shortest_path(lattice, use_double_scores=False)\n",
    "    aux = k2.get_aux_labels(best_path)[0]\n",
    "    hyp_ids = [x for x in aux if x >= 0]\n",
    "    \n",
    "    result = \"\".join(id2tok[i] for i in hyp_ids)\n",
    "    \n",
    "    # Replace word delimiter with space\n",
    "    word_delim = processor.tokenizer.word_delimiter_token\n",
    "    if word_delim:\n",
    "        result = result.replace(word_delim, \" \")\n",
    "    \n",
    "    return result.strip()\n",
    "\n",
    "#Regular CTC Decode:\n",
    "def ctc_decode_k2_old(log_probs, search_beam=20.0, output_beam=8.0):\n",
    "    \"\"\"\n",
    "    CTC decoding using k2 with same beam settings (no pattern constraints).\n",
    "    This is equivalent to your constrained decoding but without the pattern FSA.\n",
    "    \"\"\"\n",
    "    T, V = log_probs.shape\n",
    "    dense = k2.DenseFsaVec(log_probs.unsqueeze(0), torch.tensor([[0, 0, T]], dtype=torch.int32))\n",
    "    ctc_topo = k2.arc_sort(k2.ctc_topo(V-1))\n",
    "    \n",
    "    # No pattern FSA - just CTC topology\n",
    "    lattice = k2.intersect_dense_pruned(\n",
    "        ctc_topo, dense,\n",
    "        search_beam=search_beam, \n",
    "        output_beam=output_beam,\n",
    "        min_active_states=30, \n",
    "        max_active_states=10000\n",
    "    )\n",
    "    \n",
    "    best_path = k2.shortest_path(lattice, use_double_scores=False)\n",
    "    aux = k2.get_aux_labels(best_path)[0]\n",
    "    hyp_ids = [x for x in aux if x >= 0]\n",
    "    return \"\".join(id2tok[i] for i in hyp_ids)\n",
    "\n",
    "def ctc_decode_k2(logits, search_beam=20.0, output_beam=8.0):\n",
    "    \"\"\"\n",
    "    CTC decoding using k2 with beam settings (no pattern constraints).\n",
    "    Takes raw logits as input.\n",
    "    \"\"\"\n",
    "    # Convert to torch tensor if needed\n",
    "    if isinstance(logits, np.ndarray):\n",
    "        logits = torch.from_numpy(logits)\n",
    "    \n",
    "    # Convert logits to log probabilities\n",
    "    log_probs = torch.log_softmax(logits, dim=-1)\n",
    "    \n",
    "    T, V = log_probs.shape\n",
    "    dense = k2.DenseFsaVec(log_probs.unsqueeze(0), torch.tensor([[0, 0, T]], dtype=torch.int32))\n",
    "    ctc_topo = k2.arc_sort(k2.ctc_topo(V-1))\n",
    "    \n",
    "    # No pattern FSA - just CTC topology\n",
    "    lattice = k2.intersect_dense_pruned(\n",
    "        ctc_topo, dense,\n",
    "        search_beam=search_beam, \n",
    "        output_beam=output_beam,\n",
    "        min_active_states=30, \n",
    "        max_active_states=10000\n",
    "    )\n",
    "    \n",
    "    best_path = k2.shortest_path(lattice, use_double_scores=False)\n",
    "    aux = k2.get_aux_labels(best_path)[0]\n",
    "    hyp_ids = [x for x in aux if x >= 0]\n",
    "    \n",
    "    result = \"\".join(id2tok[i] for i in hyp_ids)\n",
    "    \n",
    "    # Replace word delimiter with space\n",
    "    word_delim = processor.tokenizer.word_delimiter_token\n",
    "    if word_delim:\n",
    "        result = result.replace(word_delim, \" \")\n",
    "    \n",
    "    return result.strip()\n",
    "\n",
    "def ctc_decode_greedy(logits):\n",
    "    \"\"\"\n",
    "    Greedy CTC decoding: argmax at each frame, then collapse repeats and remove blanks.\n",
    "    Takes raw logits as input (not log_probs).\n",
    "    \"\"\"\n",
    "    # Convert to torch tensor if needed\n",
    "    if isinstance(logits, np.ndarray):\n",
    "        logits = torch.from_numpy(logits)\n",
    "    \n",
    "    # Get the most probable token at each frame (argmax on logits)\n",
    "    greedy_ids = logits.argmax(dim=-1)  # Shape: (T,)\n",
    "    \n",
    "    # Collapse repeats and remove blanks\n",
    "    output = []\n",
    "    prev_id = None\n",
    "    \n",
    "    for token_id in greedy_ids.tolist():\n",
    "        if token_id == blank_id:\n",
    "            prev_id = None  # Reset on blank\n",
    "            continue\n",
    "        if token_id != prev_id:  # Only add if different from previous\n",
    "            output.append(token_id)\n",
    "            prev_id = token_id\n",
    "    \n",
    "    result = \"\".join(id2tok[i] for i in output)\n",
    "    \n",
    "    # Replace word delimiter with space\n",
    "    word_delim = processor.tokenizer.word_delimiter_token\n",
    "    if word_delim:\n",
    "        result = result.replace(word_delim, \" \")\n",
    "    \n",
    "    return result.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0455e5a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added space token '|' to wildcard set\n",
      "Reference : THE|BIRCH|CANOE|SLID|ON|THE|SMOOTH|PLANK|GLUE|THE|SHEET|TO|THE|DARK|BLUE|BACKGROUND|IT|IS|EASY|TO|TELL|THE|DEPTH|OF|THE|WELL|THESE|DAYS|A|CHICKEN|LEG|IS|A|RARE|DISH|RICE|IS|OFTEN|SERVED|IN|ROUND|BOWLS|THE|JUSE|OF|LEMON|MAKES|FINE|PUNCH|THE|BOX|WAS|TONL||BESIDE|THE|PARK|TRUNK|THE|HOX|ARE|SED|CHOPPED|CORN|AND|GARBAGE|FOUR|HOURS|A|STEADY|WORK|FACED|US|A|LARGE|SIDE|IN|STOCKINGS|IS|HARD|TO|SELL\n",
      "CTC prediction  THE BIRCH CANOE SLIT ON THE SMOOTH PLANK GLE THE HEE TO THE DARK BLUE BACKGROUND IT IS EASY TO TELL THE DEPTH OF THE WELL THESE DAYS A CICK A MEG IS A RARE DISH RICE IS OXEN SERVED IN ROUND BULL THE JUSE OF LONDONS MAKES FINE PUNCH THE BOX WAS TONL  BESIDE THE PARK TRUK THE HOX ARE SED CHOPPED CORN AND GARBAGE FOUR HOURS A STEADY WORK FACED US E LARGE SIDE AN STOCKINGS IS HARD TO SELL\n",
      "CTC greedy prediction  THE BIRCH CANOE SLIT ON THE SMOOTH PLANK GLE THE HEE TO THE DARK BLUE BACKGROUND IT IS EASY TO TELL THE DEPTH OF THE WELL THESE DAYS A CICK A MEG IS A RARE DISH RICE IS OXEN SERVED IN ROUND BULL THE JUSE OF LONDONS MAKES FINE PUNCH THE BOX WAS TONL  BESIDE THE PARK TRUK THE HOX ARE SED CHOPPED CORN AND GARBAGE FOUR HOURS A STEADY WORK FACED US E LARGE SIDE AN STOCKINGS IS HARD TO SELL\n",
      "Pattern (vowels masked)     : TH.|B.RCH|C.N..|SL.D|.N|TH.|SM..TH|PL.NK|GL..|TH.|SH..T|T.|TH.|D.RK|BL..|B.CKGR..ND|.T|.S|..SY|T.|T.LL|TH.|D.PTH|.F|TH.|W.LL|TH.S.|D.YS|.|CH.CK.N|L.G|.S|.|R.R.|D.SH|R.C.|.S|.FT.N|S.RV.D|.N|R..ND|B.WLS|TH.|J.S.|.F|L.M.N|M.K.S|F.N.|P.NCH|TH.|B.X|W.S|T.NL||B.S.D.|TH.|P.RK|TR.NK|TH.|H.X|.R.|S.D|CH.PP.D|C.RN|.ND|G.RB.G.|F..R|H..RS|.|ST..DY|W.RK|F.C.D|.S|.|L.RG.|S.D.|.N|ST.CK.NGS|.S|H.RD|T.|S.LL\n",
      "Prediction (vowels filled)  : THE BIRCH CANOE SLID ON THE SMOOTH PLANK GLEW THE SHEET TO THE DARK BLUE BACKGROUND IT IS EASY TO TELL THE DEPTH OF THE WELL THESE DAYS A CHICKAN LEG IS A RARE DISH RICE IS OFTEN SERVED IN ROUND BUWLS THE JUSE OF LOMON MAKES FINE PUNCH THE BOX WAS TONL  BESIDE THE PARK TRUNK THE HOX ARE SED CHOPPED CORN AND GARBAGE FOUR HOURS A STEADY WORK FACED US E LARGE SIDE AN STOCKINGS IS HARD TO SELL\n",
      "Pattern (every other masked): T.E.B.R.H.C.N.E.S.I.|.N.T.E.S.O.T.|.L.N.|.L.E.T.E.S.E.T.T.|.H.|.A.K.B.U.|.A.K.R.U.D.I.|.S.E.S.|.O.T.L.|.H.|.E.T.|.F.T.E.W.L.|.H.S.|.A.S.A.C.I.K.N.L.G.I.|.|.A.E.D.S.|.I.E.I.|.F.E.|.E.V.D.I.|.O.N.|.O.L.|.H.|.U.E.O.|.E.O.|.A.E.|.I.E.P.N.H.T.E.B.X.W.S.T.N.|.B.S.D.|.H.|.A.K.T.U.K.T.E.H.X.A.E.S.D.C.O.P.D.C.R.|.N.|.A.B.G.|.O.R.H.U.S.A.S.E.D.|.O.K.F.C.D.U.|.|.A.G.|.I.E.I.|.T.C.I.G.|.S.H.R.|.O.S.L.\n",
      "Prediction (filled)         : THE BIRCH CANOE SLIT ON THE SMOOTH PLANK GLUE THE SHEET TO THE DARK BLUE BACKGROUND IT IS EASY TO TELL THE DEPTH OF THE WELL THESE DAYS A CHICKAN LEG IS A RARE DISH RICE IS OF EN SERVED IN ROUND BOULL THE JUSE OF LENON MAKES FINE PUNCH THE BOX WAS TONL  BESIDE THE PARK TRUCK THE HOX ARE SED CHOPPED CORN AND GARBAGE FOUR HOURS A STEADY WORK FACED US E LARGE SIDE IN STOCKINGS IS HARD TO SEL\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#test\n",
    "audio_path = \"samples/test_english.wav\"\n",
    "reference  = \"THE BIRCH CANOE SLID ON THE SMOOTH PLANK GLUE THE SHEET TO THE DARK BLUE BACKGROUND IT IS EASY TO TELL THE DEPTH OF THE WELL THESE DAYS A CHICKEN LEG IS A RARE DISH RICE IS OFTEN SERVED IN ROUND BOWLS THE JUSE OF LEMON MAKES FINE PUNCH THE BOX WAS TONL  BESIDE THE PARK TRUNK THE HOX ARE SED CHOPPED CORN AND GARBAGE FOUR HOURS A STEADY WORK FACED US A LARGE SIDE IN STOCKINGS IS HARD TO SELL\" \n",
    "\n",
    "# Mask all vowels\n",
    "reference = reference.replace(' ', '|')\n",
    "pattern_vowels = [\".\" if ch in \"AEIOUaeiou\" else ch for ch in reference]\n",
    "# Every other char\n",
    "pattern_everyother = [ch if i % 2 == 0 else '.' for i, ch in enumerate(reference)]\n",
    "\n",
    "log_probs = get_log_probs(audio_path)\n",
    "logits = get_logits(audio_path)\n",
    "# wildcard set = all lowercase letters\n",
    "wildcard_ids = [processor.tokenizer.get_vocab()[ch] for ch in processor.tokenizer.get_vocab().keys()\n",
    "                if ch.isalpha() and len(ch) == 1]\n",
    "\n",
    "\n",
    "# Add the space token to wildcard_ids\n",
    "space_token = '|'\n",
    "if space_token in processor.tokenizer.get_vocab():\n",
    "    wildcard_ids.append(processor.tokenizer.get_vocab()[space_token])\n",
    "    print(f\"Added space token '{space_token}' to wildcard set\")\n",
    "\n",
    "\n",
    "    \n",
    "regular_output = ctc_decode_k2(logits, search_beam=20.0, output_beam=8.0)\n",
    "greedy_output = ctc_decode_greedy(logits)\n",
    "hyp_vowels = wildcard_decode_k2(logits, pattern_vowels, wildcard_ids)\n",
    "hyp_everyother = wildcard_decode_k2(logits, pattern_everyother, wildcard_ids)\n",
    "\n",
    "print(\"Reference :\", reference)\n",
    "print(\"CTC prediction \", regular_output)\n",
    "print(\"CTC greedy prediction \", greedy_output)\n",
    "print(\"Pattern (vowels masked)     :\", \"\".join(pattern_vowels))\n",
    "print(\"Prediction (vowels filled)  :\", hyp_vowels)\n",
    "print(\"Pattern (every other masked):\", \"\".join(pattern_everyother))\n",
    "print(\"Prediction (filled)         :\", hyp_everyother)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e215b84",
   "metadata": {},
   "source": [
    "### Diacritization exps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "17a6107a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyarabic import araby\n",
    "\n",
    "diacritics = araby.DIACRITICS\n",
    "arabic_letters = [ch for ch in araby.LETTERS if ch not in araby.DIACRITICS]\n",
    "\n",
    "arabic_audio_path = \"samples/female_ab_00000.wav\"\n",
    "arabic_reference = open('samples/female_ab_00000.txt', 'r', encoding='utf-8').read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e946035",
   "metadata": {},
   "source": [
    "#### Nemo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a6379f13",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2026-01-19 15:29:33 nemo_logging:405] Skipped conversion for config/subconfig:\n",
      "    {'manifest_filepath': '???', 'sample_rate': 16000, 'batch_size': 16, 'shuffle': True, 'num_workers': 8, 'pin_memory': True, 'max_duration': 20, 'min_duration': 0.5, 'is_tarred': True, 'tarred_audio_filepaths': '???', 'shuffle_n': 2048, 'bucketing_strategy': 'fully_randomized', 'bucketing_batch_size': None}\n",
      "     Reason: Missing mandatory value: train_ds.manifest_filepath\n",
      "        full_key: train_ds.manifest_filepath\n",
      "        object_type=dict.\n",
      "[NeMo W 2026-01-19 15:29:33 nemo_logging:405] Skipped conversion for config/subconfig:\n",
      "    {'manifest_filepath': '???', 'sample_rate': 16000, 'batch_size': 16, 'shuffle': False, 'use_start_end_token': False, 'num_workers': 8, 'pin_memory': True}\n",
      "     Reason: Missing mandatory value: validation_ds.manifest_filepath\n",
      "        full_key: validation_ds.manifest_filepath\n",
      "        object_type=dict.\n",
      "[NeMo W 2026-01-19 15:29:33 nemo_logging:405] Skipped conversion for config/subconfig:\n",
      "    {'manifest_filepath': '???', 'sample_rate': 16000, 'batch_size': 16, 'shuffle': False, 'use_start_end_token': False, 'num_workers': 8, 'pin_memory': True}\n",
      "     Reason: Missing mandatory value: test_ds.manifest_filepath\n",
      "        full_key: test_ds.manifest_filepath\n",
      "        object_type=dict.\n",
      "[NeMo W 2026-01-19 15:29:33 nemo_logging:405] Skipped conversion for config/subconfig:\n",
      "    {'dir': '???', 'type': 'bpe', 'model_path': 'nemo:43c84e71237048ddab3bb273bdc00fa0_tokenizer.model', 'vocab_path': 'nemo:1e7bbe36b91a472896c99283bda08bf3_vocab.txt', 'spe_tokenizer_vocab': 'nemo:e7a019581cd54cce88ac2acf8e4c54a3_tokenizer.vocab'}\n",
      "     Reason: Missing mandatory value: tokenizer.dir\n",
      "        full_key: tokenizer.dir\n",
      "        object_type=dict.\n",
      "[NeMo W 2026-01-19 15:29:33 nemo_logging:405] Skipped conversion for config/subconfig:\n",
      "    {'manifest_filepath': '???', 'sample_rate': 16000, 'batch_size': 16, 'shuffle': True, 'num_workers': 8, 'pin_memory': True, 'max_duration': 20, 'min_duration': 0.5, 'is_tarred': True, 'tarred_audio_filepaths': '???', 'shuffle_n': 2048, 'bucketing_strategy': 'fully_randomized', 'bucketing_batch_size': None}\n",
      "     Reason: Missing mandatory value: train_ds.manifest_filepath\n",
      "        full_key: train_ds.manifest_filepath\n",
      "        object_type=dict.\n",
      "[NeMo W 2026-01-19 15:29:33 nemo_logging:405] Skipped conversion for config/subconfig:\n",
      "    {'manifest_filepath': '???', 'sample_rate': 16000, 'batch_size': 16, 'shuffle': False, 'use_start_end_token': False, 'num_workers': 8, 'pin_memory': True}\n",
      "     Reason: Missing mandatory value: validation_ds.manifest_filepath\n",
      "        full_key: validation_ds.manifest_filepath\n",
      "        object_type=dict.\n",
      "[NeMo W 2026-01-19 15:29:33 nemo_logging:405] Skipped conversion for config/subconfig:\n",
      "    {'manifest_filepath': '???', 'sample_rate': 16000, 'batch_size': 16, 'shuffle': False, 'use_start_end_token': False, 'num_workers': 8, 'pin_memory': True}\n",
      "     Reason: Missing mandatory value: test_ds.manifest_filepath\n",
      "        full_key: test_ds.manifest_filepath\n",
      "        object_type=dict.\n",
      "[NeMo W 2026-01-19 15:29:33 nemo_logging:405] Skipped conversion for config/subconfig:\n",
      "    {'dir': '???', 'type': 'bpe', 'model_path': 'nemo:43c84e71237048ddab3bb273bdc00fa0_tokenizer.model', 'vocab_path': 'nemo:1e7bbe36b91a472896c99283bda08bf3_vocab.txt', 'spe_tokenizer_vocab': 'nemo:e7a019581cd54cce88ac2acf8e4c54a3_tokenizer.vocab'}\n",
      "     Reason: Missing mandatory value: tokenizer.dir\n",
      "        full_key: tokenizer.dir\n",
      "        object_type=dict.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2026-01-19 15:29:33 nemo_logging:393] Tokenizer SentencePieceTokenizer initialized with 1024 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2026-01-19 15:29:33 nemo_logging:405] Skipped conversion for config/subconfig:\n",
      "    {'manifest_filepath': '???', 'sample_rate': 16000, 'batch_size': 16, 'shuffle': True, 'num_workers': 8, 'pin_memory': True, 'max_duration': 20, 'min_duration': 0.5, 'is_tarred': True, 'tarred_audio_filepaths': '???', 'shuffle_n': 2048, 'bucketing_strategy': 'fully_randomized', 'bucketing_batch_size': None}\n",
      "     Reason: Missing mandatory value: train_ds.manifest_filepath\n",
      "        full_key: train_ds.manifest_filepath\n",
      "        object_type=dict.\n",
      "[NeMo W 2026-01-19 15:29:33 nemo_logging:405] Skipped conversion for config/subconfig:\n",
      "    {'manifest_filepath': '???', 'sample_rate': 16000, 'batch_size': 16, 'shuffle': False, 'use_start_end_token': False, 'num_workers': 8, 'pin_memory': True}\n",
      "     Reason: Missing mandatory value: validation_ds.manifest_filepath\n",
      "        full_key: validation_ds.manifest_filepath\n",
      "        object_type=dict.\n",
      "[NeMo W 2026-01-19 15:29:33 nemo_logging:405] Skipped conversion for config/subconfig:\n",
      "    {'manifest_filepath': '???', 'sample_rate': 16000, 'batch_size': 16, 'shuffle': False, 'use_start_end_token': False, 'num_workers': 8, 'pin_memory': True}\n",
      "     Reason: Missing mandatory value: test_ds.manifest_filepath\n",
      "        full_key: test_ds.manifest_filepath\n",
      "        object_type=dict.\n",
      "[NeMo W 2026-01-19 15:29:33 nemo_logging:405] Skipped conversion for config/subconfig:\n",
      "    {'dir': '???', 'type': 'bpe', 'model_path': 'nemo:43c84e71237048ddab3bb273bdc00fa0_tokenizer.model', 'vocab_path': 'nemo:1e7bbe36b91a472896c99283bda08bf3_vocab.txt', 'spe_tokenizer_vocab': 'nemo:e7a019581cd54cce88ac2acf8e4c54a3_tokenizer.vocab'}\n",
      "     Reason: Missing mandatory value: tokenizer.dir\n",
      "        full_key: tokenizer.dir\n",
      "        object_type=dict.\n",
      "[NeMo W 2026-01-19 15:29:33 nemo_logging:405] Skipped conversion for config/subconfig:\n",
      "    {'manifest_filepath': '???', 'sample_rate': 16000, 'batch_size': 16, 'shuffle': True, 'num_workers': 8, 'pin_memory': True, 'max_duration': 20, 'min_duration': 0.5, 'is_tarred': True, 'tarred_audio_filepaths': '???', 'shuffle_n': 2048, 'bucketing_strategy': 'fully_randomized', 'bucketing_batch_size': None}\n",
      "     Reason: Missing mandatory value: train_ds.manifest_filepath\n",
      "        full_key: train_ds.manifest_filepath\n",
      "        object_type=dict.\n",
      "[NeMo W 2026-01-19 15:29:33 nemo_logging:405] Skipped conversion for config/subconfig:\n",
      "    {'manifest_filepath': '???', 'sample_rate': 16000, 'batch_size': 16, 'shuffle': False, 'use_start_end_token': False, 'num_workers': 8, 'pin_memory': True}\n",
      "     Reason: Missing mandatory value: validation_ds.manifest_filepath\n",
      "        full_key: validation_ds.manifest_filepath\n",
      "        object_type=dict.\n",
      "[NeMo W 2026-01-19 15:29:33 nemo_logging:405] Skipped conversion for config/subconfig:\n",
      "    {'manifest_filepath': '???', 'sample_rate': 16000, 'batch_size': 16, 'shuffle': False, 'use_start_end_token': False, 'num_workers': 8, 'pin_memory': True}\n",
      "     Reason: Missing mandatory value: test_ds.manifest_filepath\n",
      "        full_key: test_ds.manifest_filepath\n",
      "        object_type=dict.\n",
      "[NeMo W 2026-01-19 15:29:33 nemo_logging:405] Skipped conversion for config/subconfig:\n",
      "    {'dir': '???', 'type': 'bpe', 'model_path': 'nemo:43c84e71237048ddab3bb273bdc00fa0_tokenizer.model', 'vocab_path': 'nemo:1e7bbe36b91a472896c99283bda08bf3_vocab.txt', 'spe_tokenizer_vocab': 'nemo:e7a019581cd54cce88ac2acf8e4c54a3_tokenizer.vocab'}\n",
      "     Reason: Missing mandatory value: tokenizer.dir\n",
      "        full_key: tokenizer.dir\n",
      "        object_type=dict.\n",
      "[NeMo W 2026-01-19 15:29:33 nemo_logging:405] If you intend to do training or fine-tuning, please call the ModelPT.setup_training_data() method and provide a valid configuration file to setup the train data loader.\n",
      "    Train config : \n",
      "    manifest_filepath: ???\n",
      "    sample_rate: 16000\n",
      "    batch_size: 16\n",
      "    shuffle: true\n",
      "    num_workers: 8\n",
      "    pin_memory: true\n",
      "    max_duration: 20\n",
      "    min_duration: 0.5\n",
      "    is_tarred: true\n",
      "    tarred_audio_filepaths: ???\n",
      "    shuffle_n: 2048\n",
      "    bucketing_strategy: fully_randomized\n",
      "    bucketing_batch_size: null\n",
      "    \n",
      "[NeMo W 2026-01-19 15:29:33 nemo_logging:405] If you intend to do validation, please call the ModelPT.setup_validation_data() or ModelPT.setup_multiple_validation_data() method and provide a valid configuration file to setup the validation data loader(s). \n",
      "    Validation config : \n",
      "    manifest_filepath: ???\n",
      "    sample_rate: 16000\n",
      "    batch_size: 16\n",
      "    shuffle: false\n",
      "    use_start_end_token: false\n",
      "    num_workers: 8\n",
      "    pin_memory: true\n",
      "    \n",
      "[NeMo W 2026-01-19 15:29:33 nemo_logging:405] Please call the ModelPT.setup_test_data() or ModelPT.setup_multiple_test_data() method and provide a valid configuration file to setup the test data loader(s).\n",
      "    Test config : \n",
      "    manifest_filepath: ???\n",
      "    sample_rate: 16000\n",
      "    batch_size: 16\n",
      "    shuffle: false\n",
      "    use_start_end_token: false\n",
      "    num_workers: 8\n",
      "    pin_memory: true\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2026-01-19 15:29:33 nemo_logging:393] PADDING: 0\n",
      "[NeMo I 2026-01-19 15:29:34 nemo_logging:393] Using RNNT Loss : warprnnt_numba\n",
      "    Loss warprnnt_numba_kwargs: {'fastemit_lambda': 0.0, 'clamp': -1.0}\n",
      "[NeMo I 2026-01-19 15:29:34 nemo_logging:393] Using RNNT Loss : warprnnt_numba\n",
      "    Loss warprnnt_numba_kwargs: {'fastemit_lambda': 0.0, 'clamp': -1.0}\n",
      "[NeMo I 2026-01-19 15:29:34 nemo_logging:393] Using RNNT Loss : warprnnt_numba\n",
      "    Loss warprnnt_numba_kwargs: {'fastemit_lambda': 0.0, 'clamp': -1.0}\n",
      "[NeMo I 2026-01-19 15:29:34 nemo_logging:393] Model EncDecHybridRNNTCTCBPEModel was successfully restored from /home/rufael/.cache/huggingface/hub/models--nvidia--stt_ar_fastconformer_hybrid_large_pcd_v1.0/snapshots/7f32349d952f42a28dce979ba73270aa2bbdfa89/stt_ar_fastconformer_hybrid_large_pcd_v1.0.nemo.\n",
      "[NeMo I 2026-01-19 15:29:34 nemo_logging:393] No `decoding_cfg` passed when changing decoding strategy, using internal config\n",
      "[NeMo I 2026-01-19 15:29:34 nemo_logging:393] Changed decoding strategy of the CTC decoder to \n",
      "    strategy: greedy\n",
      "    preserve_alignments: null\n",
      "    compute_timestamps: null\n",
      "    word_seperator: ' '\n",
      "    segment_seperators:\n",
      "    - .\n",
      "    - '!'\n",
      "    - '?'\n",
      "    segment_gap_threshold: null\n",
      "    ctc_timestamp_type: all\n",
      "    batch_dim_index: 0\n",
      "    greedy:\n",
      "      preserve_alignments: false\n",
      "      compute_timestamps: false\n",
      "      preserve_frame_confidence: false\n",
      "      confidence_method_cfg:\n",
      "        name: entropy\n",
      "        entropy_type: tsallis\n",
      "        alpha: 0.33\n",
      "        entropy_norm: exp\n",
      "        temperature: DEPRECATED\n",
      "      ngram_lm_model: null\n",
      "      ngram_lm_alpha: 0.0\n",
      "      boosting_tree:\n",
      "        model_path: null\n",
      "        key_phrases_file: null\n",
      "        key_phrases_list: null\n",
      "        context_score: 1.0\n",
      "        depth_scaling: 2.0\n",
      "        unk_score: 0.0\n",
      "        final_eos_score: 1.0\n",
      "        score_per_phrase: 0.0\n",
      "        source_lang: en\n",
      "        use_triton: true\n",
      "        uniform_weights: false\n",
      "        use_bpe_dropout: false\n",
      "        num_of_transcriptions: 5\n",
      "        bpe_alpha: 0.3\n",
      "      boosting_tree_alpha: 0.0\n",
      "      allow_cuda_graphs: true\n",
      "    beam:\n",
      "      beam_size: 4\n",
      "      search_type: default\n",
      "      preserve_alignments: false\n",
      "      compute_timestamps: false\n",
      "      return_best_hypothesis: true\n",
      "      allow_cuda_graphs: true\n",
      "      beam_alpha: null\n",
      "      beam_beta: 1.0\n",
      "      beam_threshold: 20.0\n",
      "      kenlm_path: null\n",
      "      ngram_lm_alpha: 1.0\n",
      "      ngram_lm_model: null\n",
      "      boosting_tree:\n",
      "        model_path: null\n",
      "        key_phrases_file: null\n",
      "        key_phrases_list: null\n",
      "        context_score: 1.0\n",
      "        depth_scaling: 2.0\n",
      "        unk_score: 0.0\n",
      "        final_eos_score: 1.0\n",
      "        score_per_phrase: 0.0\n",
      "        source_lang: en\n",
      "        use_triton: true\n",
      "        uniform_weights: false\n",
      "        use_bpe_dropout: false\n",
      "        num_of_transcriptions: 5\n",
      "        bpe_alpha: 0.3\n",
      "      boosting_tree_alpha: 0.0\n",
      "      flashlight_cfg:\n",
      "        lexicon_path: null\n",
      "        boost_path: null\n",
      "        beam_size_token: 16\n",
      "        beam_threshold: 20.0\n",
      "        unk_weight: -.inf\n",
      "        sil_weight: 0.0\n",
      "      pyctcdecode_cfg:\n",
      "        beam_prune_logp: -10.0\n",
      "        token_min_logp: -5.0\n",
      "        prune_history: false\n",
      "        hotwords: null\n",
      "        hotword_weight: 10.0\n",
      "    wfst:\n",
      "      beam_size: 4\n",
      "      search_type: riva\n",
      "      return_best_hypothesis: true\n",
      "      preserve_alignments: false\n",
      "      compute_timestamps: false\n",
      "      decoding_mode: nbest\n",
      "      open_vocabulary_decoding: false\n",
      "      beam_width: 10.0\n",
      "      lm_weight: 1.0\n",
      "      device: cuda\n",
      "      arpa_lm_path: null\n",
      "      wfst_lm_path: null\n",
      "      riva_decoding_cfg: {}\n",
      "      k2_decoding_cfg:\n",
      "        search_beam: 20.0\n",
      "        output_beam: 10.0\n",
      "        min_active_states: 30\n",
      "        max_active_states: 10000\n",
      "    confidence_cfg:\n",
      "      preserve_frame_confidence: false\n",
      "      preserve_token_confidence: false\n",
      "      preserve_word_confidence: false\n",
      "      exclude_blank: true\n",
      "      aggregation: min\n",
      "      tdt_include_duration: false\n",
      "      method_cfg:\n",
      "        name: entropy\n",
      "        entropy_type: tsallis\n",
      "        alpha: 0.33\n",
      "        entropy_norm: exp\n",
      "        temperature: DEPRECATED\n",
      "    temperature: 1.0\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2026-01-19 15:29:34 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2026-01-19 15:29:34 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "Transcribing: 1it [00:00, 45.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "فمن الرامي السويدي أوسكار سواهن الذي شارك في أولمبياد ألف وتسع مئة وعشرون حين كان يبلغ اثنان وسبعون عامًا و مئتان وواحد وثمانون يومًا. إلى الفارستين النيوزيلندية جولي بروغهام والأستترالية ماري هانا اللتين تشاركان في أولمبياد ريو وهما في واحد وستون من العمر\n",
      "فَمِنَ الرَّامِي السُّوِيدِيِّ أُوسْكَار سَوَاهِنْ الَّذِي شَارَكَ فِي أُولُمْبِيَادِ  أَلْفْ وَتِسْعُمِئَةٍ وَعِشْرُونْ حِينَ كَانَ يَبْلُغُ إَثْنَينْ وَ سَبْعينَ  عَامًا وَمِئَتَينْ وَوَاحِدْ وَثَمَانِينَ يَوْمًا إِلَى الْفَارِسَتَيْنِ النِّيُوزِيلَنْدِيَّةِ جُولِي برُوغْهَام وَالْأُسْتُرَالِيَّةِ مَارِي هَانَا اللَّتَيْنِ تُشَارِكَانِ فِي أُولُمْبِيَّادِ رِيُو وَهُمَا فِي الْوَاحِدِ وَالسِّتِينَ مِنَ الْعُمُر\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import nemo.collections.asr as nemo_asr\n",
    "from nemo.collections.common.data.utils import move_data_to_device\n",
    "from omegaconf import open_dict\n",
    "import soundfile as sf\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "asr_model = nemo_asr.models.EncDecHybridRNNTCTCBPEModel.from_pretrained(model_name=\"nvidia/stt_ar_fastconformer_hybrid_large_pcd_v1.0\")\n",
    "# Configure for greedy CTC\n",
    "with open_dict(asr_model.cfg.decoding):\n",
    "    asr_model.cfg.decoding.strategy = \"greedy\"\n",
    "    asr_model.cfg.decoding.compute_timestamps = False # Optional\n",
    "asr_model.change_decoding_strategy(decoder_type=\"ctc\")\n",
    "\n",
    "# test model\n",
    "output = asr_model.transcribe([arabic_audio_path])\n",
    "\n",
    "print(output[0].text)\n",
    "print(arabic_reference)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "88225f64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WER: 1.0\n",
      "CER: 0.6640625\n"
     ]
    }
   ],
   "source": [
    "from jiwer import wer,cer\n",
    "wer_val = wer(output[0].text, arabic_reference)\n",
    "cer_val = cer(output[0].text, arabic_reference)\n",
    "print(f\"WER: {wer_val}\")\n",
    "print(f\"CER: {cer_val}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9e0723f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2026-01-19 15:29:50 nemo_logging:393] Changed decoding strategy of the CTC decoder to \n",
      "    strategy: greedy\n",
      "    preserve_alignments: null\n",
      "    compute_timestamps: false\n",
      "    word_seperator: ' '\n",
      "    segment_seperators:\n",
      "    - .\n",
      "    - '!'\n",
      "    - '?'\n",
      "    segment_gap_threshold: null\n",
      "    ctc_timestamp_type: all\n",
      "    batch_dim_index: 0\n",
      "    greedy:\n",
      "      preserve_alignments: false\n",
      "      compute_timestamps: false\n",
      "      preserve_frame_confidence: false\n",
      "      confidence_method_cfg:\n",
      "        name: entropy\n",
      "        entropy_type: tsallis\n",
      "        alpha: 0.33\n",
      "        entropy_norm: exp\n",
      "        temperature: DEPRECATED\n",
      "      ngram_lm_model: null\n",
      "      ngram_lm_alpha: 0.0\n",
      "      boosting_tree:\n",
      "        model_path: null\n",
      "        key_phrases_file: null\n",
      "        key_phrases_list: null\n",
      "        context_score: 1.0\n",
      "        depth_scaling: 2.0\n",
      "        unk_score: 0.0\n",
      "        final_eos_score: 1.0\n",
      "        score_per_phrase: 0.0\n",
      "        source_lang: en\n",
      "        use_triton: true\n",
      "        uniform_weights: false\n",
      "        use_bpe_dropout: false\n",
      "        num_of_transcriptions: 5\n",
      "        bpe_alpha: 0.3\n",
      "      boosting_tree_alpha: 0.0\n",
      "      allow_cuda_graphs: true\n",
      "      max_symbols: 10\n",
      "    beam:\n",
      "      beam_size: 2\n",
      "      search_type: default\n",
      "      preserve_alignments: false\n",
      "      compute_timestamps: false\n",
      "      return_best_hypothesis: false\n",
      "      allow_cuda_graphs: true\n",
      "      beam_alpha: null\n",
      "      beam_beta: 1.0\n",
      "      beam_threshold: 20.0\n",
      "      kenlm_path: null\n",
      "      ngram_lm_alpha: 1.0\n",
      "      ngram_lm_model: null\n",
      "      boosting_tree:\n",
      "        model_path: null\n",
      "        key_phrases_file: null\n",
      "        key_phrases_list: null\n",
      "        context_score: 1.0\n",
      "        depth_scaling: 2.0\n",
      "        unk_score: 0.0\n",
      "        final_eos_score: 1.0\n",
      "        score_per_phrase: 0.0\n",
      "        source_lang: en\n",
      "        use_triton: true\n",
      "        uniform_weights: false\n",
      "        use_bpe_dropout: false\n",
      "        num_of_transcriptions: 5\n",
      "        bpe_alpha: 0.3\n",
      "      boosting_tree_alpha: 0.0\n",
      "      flashlight_cfg:\n",
      "        lexicon_path: null\n",
      "        boost_path: null\n",
      "        beam_size_token: 16\n",
      "        beam_threshold: 20.0\n",
      "        unk_weight: -.inf\n",
      "        sil_weight: 0.0\n",
      "      pyctcdecode_cfg:\n",
      "        beam_prune_logp: -10.0\n",
      "        token_min_logp: -5.0\n",
      "        prune_history: false\n",
      "        hotwords: null\n",
      "        hotword_weight: 10.0\n",
      "      score_norm: true\n",
      "      tsd_max_sym_exp: 50\n",
      "      alsd_max_target_len: 2.0\n",
      "    wfst:\n",
      "      beam_size: 4\n",
      "      search_type: riva\n",
      "      return_best_hypothesis: true\n",
      "      preserve_alignments: false\n",
      "      compute_timestamps: false\n",
      "      decoding_mode: nbest\n",
      "      open_vocabulary_decoding: false\n",
      "      beam_width: 10.0\n",
      "      lm_weight: 1.0\n",
      "      device: cuda\n",
      "      arpa_lm_path: null\n",
      "      wfst_lm_path: null\n",
      "      riva_decoding_cfg: {}\n",
      "      k2_decoding_cfg:\n",
      "        search_beam: 20.0\n",
      "        output_beam: 10.0\n",
      "        min_active_states: 30\n",
      "        max_active_states: 10000\n",
      "    confidence_cfg:\n",
      "      preserve_frame_confidence: false\n",
      "      preserve_token_confidence: false\n",
      "      preserve_word_confidence: false\n",
      "      exclude_blank: true\n",
      "      aggregation: min\n",
      "      tdt_include_duration: false\n",
      "      method_cfg:\n",
      "        name: entropy\n",
      "        entropy_type: tsallis\n",
      "        alpha: 0.33\n",
      "        entropy_norm: exp\n",
      "        temperature: DEPRECATED\n",
      "    temperature: 1.0\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2026-01-19 15:29:50 nemo_logging:405] AudioPreprocessor received an input signal of dtype torch.float64, rather than torch.float32. In sweeps across multiple datasets, we have found that the preprocessor is not robust to low precision  mathematics. As such, it runs in float32. Your input will be cast to float32, but this is not necessarily enough to recovery full accuracy. For example, simply casting input_signal from torch.float32 to torch.bfloat16, then back to torch.float32 before running AudioPreprocessor causes drops in absolute WER of up to 0.1%. torch.bfloat16 simply does not have enough mantissa bits to represent enough values in the range [-1.0,+1.0] correctly.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Get logits for nemo\n",
    "def get_logits(audio_path):\n",
    "    wav, sr = sf.read(audio_path)\n",
    "    \n",
    "    if sr != 16000:\n",
    "        import librosa\n",
    "        wav = librosa.resample(wav, orig_sr=sr, target_sr=16000)\n",
    "        sr = 16000\n",
    "\n",
    "        # get length of wav\n",
    "        length = len(wav)\n",
    "    else:\n",
    "        length = len(wav)\n",
    "\n",
    "    # convert to tensor\n",
    "    wav = torch.from_numpy(wav).unsqueeze(0)\n",
    "    length = torch.tensor([length])\n",
    "\n",
    "    wav = move_data_to_device(wav, device)\n",
    "    length = move_data_to_device(length, device)\n",
    "    # get logits from asr_model\n",
    "    encoded, encoded_length = asr_model.forward(input_signal=wav, input_signal_length=length)\n",
    "    logits = asr_model.ctc_decoder(encoder_output=encoded)\n",
    "\n",
    "    hypotheses = asr_model.ctc_decoding.ctc_decoder_predictions_tensor(\n",
    "            logits,\n",
    "            encoded_length,\n",
    "            return_hypotheses=True,\n",
    "        )\n",
    "\n",
    "    return hypotheses, logits.squeeze(0)\n",
    "\n",
    "asr_model.change_decoding_strategy(asr_model.cfg.decoding, decoder_type=\"ctc\")\n",
    "\n",
    "hypotheses, logits = get_logits(arabic_audio_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "14d9e4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, k2, soundfile as sf, numpy as np\n",
    "\n",
    "vocab = asr_model.ctc_decoder.vocabulary\n",
    "diac_in_vocab = [diac for diac in diacritics if diac in vocab]\n",
    "token2id = {v: k for k, v in enumerate(vocab)}\n",
    "id2tok = {v: k for k, v in token2id.items()}\n",
    "blank_id = asr_model.ctc_decoding.blank_id\n",
    "\n",
    "def build_pattern_fsa(pattern, wildcard_ids, token2id=token2id):\n",
    "    \"\"\"\n",
    "    pattern: list of characters, '.' for wildcard\n",
    "    token2id: dict mapping from char -> token id\n",
    "    wildcard_ids: allowed token ids for wildcard positions\n",
    "    \"\"\"\n",
    "    arcs = []\n",
    "    state = 0\n",
    "    for i, ch in enumerate(pattern):\n",
    "        if ch == '.':\n",
    "            for wid in wildcard_ids:\n",
    "                arcs.append(f\"{state} {state+1} {wid} {wid} 0.0\")\n",
    "        else:\n",
    "            if ch not in token2id:\n",
    "                continue\n",
    "            tid = token2id[ch]\n",
    "            arcs.append(f\"{state} {state+1} {tid} {tid} 0.0\")\n",
    "        state += 1\n",
    "    arcs.append(f\"{state} 0.0\")\n",
    "    txt = \"\\n\".join(arcs)\n",
    "    fsa = k2.Fsa.from_str(txt, acceptor=False, openfst=True)\n",
    "    return k2.arc_sort(fsa)\n",
    "\n",
    "# WFS Decoding\n",
    "def wildcard_decode_k2(logits, pattern, wildcard_set):\n",
    "    \"\"\"\n",
    "    logits: (T, V) raw logits from model\n",
    "    pattern: list of characters (with '.')\n",
    "    wildcard_set: list of allowed tokens for '.'\n",
    "    \"\"\"\n",
    "    # Convert to torch tensor if needed\n",
    "    if isinstance(logits, np.ndarray):\n",
    "        logits = torch.from_numpy(logits)\n",
    "    \n",
    "    # Convert logits to log probabilities\n",
    "    log_probs = torch.log_softmax(logits, dim=-1)\n",
    "    \n",
    "    T, V = log_probs.shape\n",
    "    dense = k2.DenseFsaVec(log_probs.unsqueeze(0), torch.tensor([[0, 0, T]], dtype=torch.int32))\n",
    "    ctc_topo = k2.arc_sort(k2.ctc_topo(V-1))\n",
    "    pattern_fsa = build_pattern_fsa(pattern, wildcard_set)\n",
    "    decoding_graph = k2.arc_sort(k2.compose(ctc_topo, pattern_fsa))\n",
    "    lattice = k2.intersect_dense_pruned(\n",
    "       decoding_graph, dense,\n",
    "       search_beam=20.0, output_beam=8.0,\n",
    "       min_active_states=30, max_active_states=10000)\n",
    "    \n",
    "    best_path = k2.shortest_path(lattice, use_double_scores=False)\n",
    "    aux = k2.get_aux_labels(best_path)[0]\n",
    "    hyp_ids = [x for x in aux if x >= 0]\n",
    "    \n",
    "    result = \"\".join(id2tok[i] for i in hyp_ids)\n",
    "    \n",
    "    # Replace word delimiter with space\n",
    "    word_delim = processor.tokenizer.word_delimiter_token\n",
    "    if word_delim:\n",
    "        result = result.replace(word_delim, \" \")\n",
    "    \n",
    "    return result.strip()\n",
    "\n",
    "# CTC Decoding\n",
    "def ctc_decode_k2(logits, search_beam=20.0, output_beam=8.0):\n",
    "    \"\"\"\n",
    "    CTC decoding using k2 with beam settings (no pattern constraints).\n",
    "    Takes raw logits as input.\n",
    "    \"\"\"\n",
    "    # Convert to torch tensor if needed\n",
    "    if isinstance(logits, np.ndarray):\n",
    "        logits = torch.from_numpy(logits)\n",
    "    \n",
    "    # Convert logits to log probabilities\n",
    "    log_probs = torch.log_softmax(logits, dim=-1)\n",
    "    \n",
    "    T, V = log_probs.shape\n",
    "    dense = k2.DenseFsaVec(log_probs.unsqueeze(0), torch.tensor([[0, 0, T]], dtype=torch.int32))\n",
    "    ctc_topo = k2.arc_sort(k2.ctc_topo(V-1))\n",
    "    \n",
    "    # No pattern FSA - just CTC topology\n",
    "    lattice = k2.intersect_dense_pruned(\n",
    "        ctc_topo, dense,\n",
    "        search_beam=search_beam, \n",
    "        output_beam=output_beam,\n",
    "        min_active_states=30, \n",
    "        max_active_states=10000\n",
    "    )\n",
    "    \n",
    "    best_path = k2.shortest_path(lattice, use_double_scores=False)\n",
    "    aux = k2.get_aux_labels(best_path)[0]\n",
    "    hyp_ids = [x for x in aux if x >= 0]\n",
    "    \n",
    "    result = \"\".join(id2tok[i] for i in hyp_ids)\n",
    "    \n",
    "    # Replace word delimiter with space\n",
    "    word_delim = processor.tokenizer.word_delimiter_token\n",
    "    if word_delim:\n",
    "        result = result.replace(word_delim, \" \")\n",
    "    \n",
    "    return result.strip()\n",
    "\n",
    "def ctc_decode_greedy(logits):\n",
    "    \"\"\"\n",
    "    Greedy CTC decoding: argmax at each frame, then collapse repeats and remove blanks.\n",
    "    Takes raw logits as input (not log_probs).\n",
    "    \"\"\"\n",
    "    # Convert to torch tensor if needed\n",
    "    if isinstance(logits, np.ndarray):\n",
    "        logits = torch.from_numpy(logits)\n",
    "    \n",
    "    # Get the most probable token at each frame (argmax on logits)\n",
    "    greedy_ids = logits.argmax(dim=-1)  # Shape: (T,)\n",
    "    \n",
    "    # Collapse repeats and remove blanks\n",
    "    output = []\n",
    "    prev_id = None\n",
    "    \n",
    "    for token_id in greedy_ids.tolist():\n",
    "        if token_id == blank_id:\n",
    "            prev_id = None  # Reset on blank\n",
    "            continue\n",
    "        if token_id != prev_id:  # Only add if different from previous\n",
    "            output.append(token_id)\n",
    "            prev_id = token_id\n",
    "    \n",
    "    result = \"\".join(id2tok[i] for i in output)\n",
    "    \n",
    "    # Replace word delimiter with space\n",
    "    word_delim = processor.tokenizer.word_delimiter_token\n",
    "    if word_delim:\n",
    "        result = result.replace(word_delim, \" \")\n",
    "    \n",
    "    return result.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "73f223b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[F] /var/www/k2/csrc/intersect_dense_pruned.cu:163:void k2::MultiGraphDenseIntersectPruned::Intersect(k2::DenseFsaVec*) Check failed: c_->IsCompatible(*b_fsas->Context()) \n",
      "\n",
      "\n",
      "[ Stack-Trace: ]\n",
      "/home/rufael/Projects/diac-btc/.venv/lib/python3.12/site-packages/k2/lib64/libk2_log.so(k2::internal::GetStackTrace[abi:cxx11]()+0x3c) [0x7f38bcee766c]\n",
      "/home/rufael/Projects/diac-btc/.venv/lib/python3.12/site-packages/k2/lib64/libk2context.so(k2::internal::Logger::~Logger()+0x35) [0x7f38bd0603b5]\n",
      "/home/rufael/Projects/diac-btc/.venv/lib/python3.12/site-packages/k2/lib64/libk2context.so(k2::MultiGraphDenseIntersectPruned::Intersect(k2::DenseFsaVec*)+0x7a) [0x7f38bd20057a]\n",
      "/home/rufael/Projects/diac-btc/.venv/lib/python3.12/site-packages/k2/lib64/libk2context.so(k2::IntersectDensePruned(k2::Ragged<k2::Arc>&, k2::DenseFsaVec&, float, float, int, int, bool, k2::Ragged<k2::Arc>*, k2::Array1<int>*, k2::Array1<int>*)+0xbf) [0x7f38bd1d616f]\n",
      "/home/rufael/Projects/diac-btc/.venv/lib/python3.12/site-packages/_k2.cpython-312-x86_64-linux-gnu.so(+0xac675) [0x7f38c6f7c675]\n",
      "/home/rufael/Projects/diac-btc/.venv/lib/python3.12/site-packages/_k2.cpython-312-x86_64-linux-gnu.so(+0x609a5) [0x7f38c6f309a5]\n",
      "/home/rufael/Projects/diac-btc/.venv/bin/python(+0x2334a1) [0x55abc41c24a1]\n",
      "/home/rufael/Projects/diac-btc/.venv/bin/python(_PyObject_MakeTpCall+0x2eb) [0x55abc419f9ab]\n",
      "/home/rufael/Projects/diac-btc/.venv/bin/python(+0x114a7e) [0x55abc40a3a7e]\n",
      "/home/rufael/Projects/diac-btc/.venv/lib/python3.12/site-packages/torch/lib/libtorch_python.so(+0x7a588d) [0x7f3a1a86b88d]\n",
      "/home/rufael/Projects/diac-btc/.venv/bin/python(+0x2334ca) [0x55abc41c24ca]\n",
      "/home/rufael/Projects/diac-btc/.venv/bin/python(_PyObject_Call+0xb5) [0x55abc41d54a5]\n",
      "/home/rufael/Projects/diac-btc/.venv/bin/python(+0x113688) [0x55abc40a2688]\n",
      "/home/rufael/Projects/diac-btc/.venv/bin/python(PyEval_EvalCode+0xae) [0x55abc426acae]\n",
      "/home/rufael/Projects/diac-btc/.venv/bin/python(+0x2f643d) [0x55abc428543d]\n",
      "/home/rufael/Projects/diac-btc/.venv/bin/python(+0x11833e) [0x55abc40a733e]\n",
      "/home/rufael/Projects/diac-btc/.venv/bin/python(+0x2f1002) [0x55abc4280002]\n",
      "/home/rufael/Projects/diac-btc/.venv/bin/python(+0x2f2267) [0x55abc4281267]\n",
      "/home/rufael/Projects/diac-btc/.venv/bin/python(+0x118bcd) [0x55abc40a7bcd]\n",
      "/home/rufael/Projects/diac-btc/.venv/bin/python(+0x26503c) [0x55abc41f403c]\n",
      "/home/rufael/Projects/diac-btc/.venv/bin/python(+0x264bad) [0x55abc41f3bad]\n",
      "/home/rufael/Projects/diac-btc/.venv/bin/python(_PyObject_Call+0x123) [0x55abc41d5513]\n",
      "/home/rufael/Projects/diac-btc/.venv/bin/python(+0x113688) [0x55abc40a2688]\n",
      "/home/rufael/Projects/diac-btc/.venv/bin/python(+0x2f1002) [0x55abc4280002]\n",
      "/home/hawau/miniconda3/lib/python3.12/lib-dynload/_asyncio.cpython-312-x86_64-linux-gnu.so(+0x81e6) [0x7f3a43be51e6]\n",
      "/home/hawau/miniconda3/lib/python3.12/lib-dynload/_asyncio.cpython-312-x86_64-linux-gnu.so(+0x899a) [0x7f3a43be599a]\n",
      "/home/rufael/Projects/diac-btc/.venv/bin/python(_PyObject_MakeTpCall+0x2eb) [0x55abc419f9ab]\n",
      "/home/rufael/Projects/diac-btc/.venv/bin/python(+0x1cf916) [0x55abc415e916]\n",
      "/home/rufael/Projects/diac-btc/.venv/bin/python(+0x2280a6) [0x55abc41b70a6]\n",
      "/home/rufael/Projects/diac-btc/.venv/bin/python(+0x113688) [0x55abc40a2688]\n",
      "/home/rufael/Projects/diac-btc/.venv/bin/python(PyEval_EvalCode+0xae) [0x55abc426acae]\n",
      "/home/rufael/Projects/diac-btc/.venv/bin/python(+0x2f643d) [0x55abc428543d]\n",
      "/home/rufael/Projects/diac-btc/.venv/bin/python(+0x2280a6) [0x55abc41b70a6]\n",
      "/home/rufael/Projects/diac-btc/.venv/bin/python(PyObject_Vectorcall+0x4f) [0x55abc41b6dff]\n",
      "/home/rufael/Projects/diac-btc/.venv/bin/python(+0x114a7e) [0x55abc40a3a7e]\n",
      "/home/rufael/Projects/diac-btc/.venv/bin/python(+0x30aca8) [0x55abc4299ca8]\n",
      "/home/rufael/Projects/diac-btc/.venv/bin/python(Py_RunMain+0x3c7) [0x55abc4299787]\n",
      "/home/rufael/Projects/diac-btc/.venv/bin/python(Py_BytesMain+0x37) [0x55abc4255217]\n",
      "/lib/x86_64-linux-gnu/libc.so.6(__libc_start_main+0xf3) [0x7f3a45516083]\n",
      "/home/rufael/Projects/diac-btc/.venv/bin/python(+0x2c6095) [0x55abc4255095]\n",
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "\n    Some bad things happened. Please read the above error messages and stack\n    trace. If you are using Python, the following command may be helpful:\n\n      gdb --args python /path/to/your/code.py\n\n    (You can use `gdb` to debug the code. Please consider compiling\n    a debug version of k2.).\n\n    If you are unable to fix it, please open an issue at:\n\n      https://github.com/k2-fsa/k2/issues/new\n    ",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m      8\u001b[39m logits_diacritics = logits[:, wildcard_ids]\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# regular_output = ctc_decode_k2(logits.squeeze(0), search_beam=20.0, output_beam=8.0)\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m#################### Greedy CTC Decoding ####################\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# greedy_output = ctc_decode_greedy(logits.squeeze(0))\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     15\u001b[39m \n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m#################### WFS Decoding ############################\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m hyp_diacritics = \u001b[43mwildcard_decode_k2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits_diacritics\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpattern_diacritics\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwildcard_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[38;5;28mprint\u001b[39m(hyp_diacritics)\n\u001b[32m     19\u001b[39m \u001b[38;5;28mprint\u001b[39m(arabic_reference)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 51\u001b[39m, in \u001b[36mwildcard_decode_k2\u001b[39m\u001b[34m(logits, pattern, wildcard_set)\u001b[39m\n\u001b[32m     49\u001b[39m pattern_fsa = build_pattern_fsa(pattern, wildcard_set)\n\u001b[32m     50\u001b[39m decoding_graph = k2.arc_sort(k2.compose(ctc_topo, pattern_fsa))\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m lattice = \u001b[43mk2\u001b[49m\u001b[43m.\u001b[49m\u001b[43mintersect_dense_pruned\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     52\u001b[39m \u001b[43m   \u001b[49m\u001b[43mdecoding_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdense\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     53\u001b[39m \u001b[43m   \u001b[49m\u001b[43msearch_beam\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m20.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_beam\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m8.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[43m   \u001b[49m\u001b[43mmin_active_states\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m30\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_active_states\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     56\u001b[39m best_path = k2.shortest_path(lattice, use_double_scores=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m     57\u001b[39m aux = k2.get_aux_labels(best_path)[\u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/diac-btc/.venv/lib/python3.12/site-packages/k2/autograd.py:742\u001b[39m, in \u001b[36mintersect_dense_pruned\u001b[39m\u001b[34m(a_fsas, b_fsas, search_beam, output_beam, min_active_states, max_active_states, seqframe_idx_name, frame_idx_name, allow_partial)\u001b[39m\n\u001b[32m    738\u001b[39m out_fsa = [\u001b[32m0\u001b[39m]\n\u001b[32m    740\u001b[39m \u001b[38;5;66;03m# the following return value is discarded since it is already contained\u001b[39;00m\n\u001b[32m    741\u001b[39m \u001b[38;5;66;03m# in `out_fsa[0].scores`\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m742\u001b[39m \u001b[43m_IntersectDensePrunedFunction\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma_fsas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb_fsas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_fsa\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msearch_beam\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    743\u001b[39m \u001b[43m                                    \u001b[49m\u001b[43moutput_beam\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_active_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    744\u001b[39m \u001b[43m                                    \u001b[49m\u001b[43mmax_active_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_partial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    745\u001b[39m \u001b[43m                                    \u001b[49m\u001b[43ma_fsas\u001b[49m\u001b[43m.\u001b[49m\u001b[43mscores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    746\u001b[39m \u001b[43m                                    \u001b[49m\u001b[43mb_fsas\u001b[49m\u001b[43m.\u001b[49m\u001b[43mscores\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseqframe_idx_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    747\u001b[39m \u001b[43m                                    \u001b[49m\u001b[43mframe_idx_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    748\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m out_fsa[\u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/diac-btc/.venv/lib/python3.12/site-packages/torch/autograd/function.py:581\u001b[39m, in \u001b[36mFunction.apply\u001b[39m\u001b[34m(cls, *args, **kwargs)\u001b[39m\n\u001b[32m    578\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch._C._are_functorch_transforms_active():\n\u001b[32m    579\u001b[39m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[32m    580\u001b[39m     args = _functorch.utils.unwrap_dead_wrappers(args)\n\u001b[32m--> \u001b[39m\u001b[32m581\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m    583\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_setup_ctx_defined:\n\u001b[32m    584\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    585\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    586\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    587\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mstaticmethod. For more details, please see \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    588\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mhttps://pytorch.org/docs/main/notes/extending.func.html\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    589\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/diac-btc/.venv/lib/python3.12/site-packages/k2/autograd.py:421\u001b[39m, in \u001b[36m_IntersectDensePrunedFunction.forward\u001b[39m\u001b[34m(ctx, a_fsas, b_fsas, out_fsa, search_beam, output_beam, min_active_states, max_active_states, allow_partial, unused_scores_a, unused_scores_b, seqframe_idx_name, frame_idx_name)\u001b[39m\n\u001b[32m    366\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m'''Intersect array of FSAs on CPU/GPU.\u001b[39;00m\n\u001b[32m    367\u001b[39m \n\u001b[32m    368\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    417\u001b[39m \u001b[33;03m   Return `out_fsa[0].scores`.\u001b[39;00m\n\u001b[32m    418\u001b[39m \u001b[33;03m'''\u001b[39;00m\n\u001b[32m    419\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out_fsa) == \u001b[32m1\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m421\u001b[39m ragged_arc, arc_map_a, arc_map_b = \u001b[43m_k2\u001b[49m\u001b[43m.\u001b[49m\u001b[43mintersect_dense_pruned\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    422\u001b[39m \u001b[43m    \u001b[49m\u001b[43ma_fsas\u001b[49m\u001b[43m=\u001b[49m\u001b[43ma_fsas\u001b[49m\u001b[43m.\u001b[49m\u001b[43marcs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    423\u001b[39m \u001b[43m    \u001b[49m\u001b[43mb_fsas\u001b[49m\u001b[43m=\u001b[49m\u001b[43mb_fsas\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdense_fsa_vec\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    424\u001b[39m \u001b[43m    \u001b[49m\u001b[43msearch_beam\u001b[49m\u001b[43m=\u001b[49m\u001b[43msearch_beam\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    425\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_beam\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_beam\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    426\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmin_active_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmin_active_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    427\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_active_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_active_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    428\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_partial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mallow_partial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    430\u001b[39m out_fsa[\u001b[32m0\u001b[39m] = Fsa(ragged_arc)\n\u001b[32m    432\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m name, a_value \u001b[38;5;129;01min\u001b[39;00m a_fsas.named_tensor_attr(include_scores=\u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "\u001b[31mRuntimeError\u001b[39m: \n    Some bad things happened. Please read the above error messages and stack\n    trace. If you are using Python, the following command may be helpful:\n\n      gdb --args python /path/to/your/code.py\n\n    (You can use `gdb` to debug the code. Please consider compiling\n    a debug version of k2.).\n\n    If you are unable to fix it, please open an issue at:\n\n      https://github.com/k2-fsa/k2/issues/new\n    "
     ]
    }
   ],
   "source": [
    "pattern_diacritics = [ch if ch not in diac_in_vocab else '.' for ch in arabic_reference]\n",
    "\n",
    "hyp, logits = get_logits(arabic_audio_path)\n",
    "wildcard_ids = [ i for i,ch in enumerate(vocab)\n",
    "                if ch in diac_in_vocab]\n",
    "\n",
    "# select logits for the diacritics only\n",
    "logits_diacritics = logits[:, wildcard_ids]\n",
    "# regular_output = ctc_decode_k2(logits.squeeze(0), search_beam=20.0, output_beam=8.0)\n",
    "#################### Greedy CTC Decoding ####################\n",
    "# greedy_output = ctc_decode_greedy(logits.squeeze(0))\n",
    "# print(greedy_output)\n",
    "# print(arabic_reference)\n",
    "############################################################\n",
    "\n",
    "#################### WFS Decoding ############################\n",
    "hyp_diacritics = wildcard_decode_k2(logits_diacritics, pattern_diacritics, wildcard_ids)\n",
    "print(hyp_diacritics)\n",
    "print(arabic_reference)\n",
    "############################################################\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65a57c7",
   "metadata": {},
   "source": [
    "#### Wav2Vec2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "70750c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoProcessor, AutoModelForCTC\n",
    "from jiwer import wer\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"jonatasgrosman/wav2vec2-large-xlsr-53-arabic\")\n",
    "model = AutoModelForCTC.from_pretrained(\"jonatasgrosman/wav2vec2-large-xlsr-53-arabic\")\n",
    "\n",
    "def get_logits(audio_path):\n",
    "    wav, sr = sf.read(audio_path)\n",
    "\n",
    "    if sr != 16000:\n",
    "        import librosa\n",
    "        wav = librosa.resample(wav, orig_sr=sr, target_sr=16000)\n",
    "\n",
    "    wav = wav[None, :]\n",
    "\n",
    "    processed_input = processor(\n",
    "        audio=wav,\n",
    "        sampling_rate=16000,\n",
    "        return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "    output = model(processed_input.input_values)\n",
    "    logits = output.logits\n",
    "    predicted_ids = torch.argmax(logits, dim=-1)\n",
    "\n",
    "    transcription = processor.decode(predicted_ids[0])\n",
    "\n",
    "    return logits, transcription\n",
    "\n",
    "logits, transcription = get_logits(arabic_audio_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d2629faf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "فَمِنَ الرَّامِ السُّوَيْديِّ - أُسْكُرْ سَواهِنْ - الَّذِي شَرَكَ فِي أُولَمْبِيَادِ أَلْفُتِسُعْمِئَةٍ وَعِشْرونَ - حِينَ كَانَ يَبْلُغُ اثْنَيْنِ وَسَبرينَ عَاماً - وَمِئتَيْنِ وَوَاحِدُ وثَمانينَ يَوْمًا - إِلَى الفارسَتَيْنِ أَنْْيُوزْلَندِيَّةِ - جيولِي بروغْهَامْ - والأَسْتْرَاليَّةِ مارِي هَنَا - الَّتَيْنِ تُشَارِكَانِ فِي أُولَمْبِيَادِرِيُو - وَهُمَا فِي الْوَاحِدِ وَالسِّتّينَ مِنَ الْعُمُرِ\n",
      "فَمِنَ الرَّامِي السُّوِيدِيِّ أُوسْكَار سَوَاهِنْ الَّذِي شَارَكَ فِي أُولُمْبِيَادِ  أَلْفْ وَتِسْعُمِئَةٍ وَعِشْرُونْ حِينَ كَانَ يَبْلُغُ إَثْنَينْ وَ سَبْعينَ  عَامًا وَمِئَتَينْ وَوَاحِدْ وَثَمَانِينَ يَوْمًا إِلَى الْفَارِسَتَيْنِ النِّيُوزِيلَنْدِيَّةِ جُولِي برُوغْهَام وَالْأُسْتُرَالِيَّةِ مَارِي هَانَا اللَّتَيْنِ تُشَارِكَانِ فِي أُولُمْبِيَّادِ رِيُو وَهُمَا فِي الْوَاحِدِ وَالسِّتِينَ مِنَ الْعُمُر\n",
      "0.8095238095238095\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(transcription)\n",
    "print(arabic_reference)\n",
    "\n",
    "# calculate wer\n",
    "wer = wer(arabic_reference, transcription)\n",
    "print(wer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "15501c83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "diacritics in vocab: ['ً', 'ٌ', 'ٍ', 'َ', 'ُ', 'ِ', 'ّ', 'ْ']\n",
      "vocab size: 51\n",
      "blank_id: 0\n",
      "token2id: {'<pad>': 0, '<s>': 1, '</s>': 2, '<unk>': 3, '|': 4, '-': 5, 'ء': 6, 'آ': 7, 'أ': 8, 'ؤ': 9, 'إ': 10, 'ئ': 11, 'ا': 12, 'ب': 13, 'ة': 14, 'ت': 15, 'ث': 16, 'ج': 17, 'ح': 18, 'خ': 19, 'د': 20, 'ذ': 21, 'ر': 22, 'ز': 23, 'س': 24, 'ش': 25, 'ص': 26, 'ض': 27, 'ط': 28, 'ظ': 29, 'ع': 30, 'غ': 31, 'ـ': 32, 'ف': 33, 'ق': 34, 'ك': 35, 'ل': 36, 'م': 37, 'ن': 38, 'ه': 39, 'و': 40, 'ى': 41, 'ي': 42, 'ً': 43, 'ٌ': 44, 'ٍ': 45, 'َ': 46, 'ُ': 47, 'ِ': 48, 'ّ': 49, 'ْ': 50}\n"
     ]
    }
   ],
   "source": [
    "vocab = list(processor.tokenizer.get_vocab().keys())\n",
    "id2tok = {v: k for k, v in processor.tokenizer.get_vocab().items()}\n",
    "token2id = {v: k for k, v in enumerate(vocab)}\n",
    "blank_id = processor.tokenizer.pad_token_id\n",
    "\n",
    "diac_in_vocab = [diac for diac in diacritics if diac in vocab]\n",
    "\n",
    "print(f\"diacritics in vocab: {diac_in_vocab}\")\n",
    "print(f\"vocab size: {len(vocab)}\")\n",
    "print(f\"blank_id: {blank_id}\")\n",
    "print(f\"token2id: {token2id}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "067f558e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, k2, soundfile as sf, numpy as np\n",
    "\n",
    "def build_pattern_fsa(pattern, wildcard_ids, token2id=token2id):\n",
    "    \"\"\"\n",
    "    pattern: list of characters, '.' for wildcard\n",
    "    token2id: dict mapping from char -> token id\n",
    "    wildcard_ids: allowed token ids for wildcard positions\n",
    "    \"\"\"\n",
    "    arcs = []\n",
    "    state = 0\n",
    "    for i, ch in enumerate(pattern):\n",
    "        if ch == '.':\n",
    "            for wid in wildcard_ids:\n",
    "                arcs.append(f\"{state} {state+1} {wid} {wid} 0.0\")\n",
    "        else:\n",
    "            if ch not in token2id:\n",
    "                continue\n",
    "            tid = token2id[ch]\n",
    "            arcs.append(f\"{state} {state+1} {tid} {tid} 0.0\")\n",
    "        state += 1\n",
    "    arcs.append(f\"{state} 0.0\")\n",
    "    txt = \"\\n\".join(arcs)\n",
    "    fsa = k2.Fsa.from_str(txt, acceptor=False, openfst=True)\n",
    "    return k2.arc_sort(fsa)\n",
    "\n",
    "# WFS Decoding\n",
    "def wildcard_decode_k2(logits, pattern, wildcard_set):\n",
    "    \"\"\"\n",
    "    logits: (T, V) raw logits from model\n",
    "    pattern: list of characters (with '.')\n",
    "    wildcard_set: list of allowed tokens for '.'\n",
    "    \"\"\"\n",
    "    # Convert to torch tensor if needed\n",
    "    if isinstance(logits, np.ndarray):\n",
    "        logits = torch.from_numpy(logits)\n",
    "    \n",
    "    # Convert logits to log probabilities\n",
    "    log_probs = torch.log_softmax(logits, dim=-1)\n",
    "    \n",
    "    T, V = log_probs.shape\n",
    "    dense = k2.DenseFsaVec(log_probs.unsqueeze(0), torch.tensor([[0, 0, T]], dtype=torch.int32))\n",
    "    ctc_topo = k2.arc_sort(k2.ctc_topo(V-1))\n",
    "    pattern_fsa = build_pattern_fsa(pattern, wildcard_set)\n",
    "    decoding_graph = k2.arc_sort(k2.compose(ctc_topo, pattern_fsa))\n",
    "    lattice = k2.intersect_dense_pruned(\n",
    "       decoding_graph, dense,\n",
    "       search_beam=20.0, output_beam=8.0,\n",
    "       min_active_states=30, max_active_states=10000)\n",
    "    \n",
    "    best_path = k2.shortest_path(lattice, use_double_scores=False)\n",
    "    aux = k2.get_aux_labels(best_path)[0]\n",
    "    hyp_ids = [x for x in aux if x >= 0]\n",
    "    \n",
    "    result = \"\".join(id2tok[i] for i in hyp_ids)\n",
    "    \n",
    "    # Replace word delimiter with space\n",
    "    word_delim = processor.tokenizer.word_delimiter_token\n",
    "    if word_delim:\n",
    "        result = result.replace(word_delim, \" \")\n",
    "    \n",
    "    return result.strip()\n",
    "\n",
    "# CTC Decoding\n",
    "def ctc_decode_k2(logits, search_beam=20.0, output_beam=8.0):\n",
    "    \"\"\"\n",
    "    CTC decoding using k2 with beam settings (no pattern constraints).\n",
    "    Takes raw logits as input.\n",
    "    \"\"\"\n",
    "    # Convert to torch tensor if needed\n",
    "    if isinstance(logits, np.ndarray):\n",
    "        logits = torch.from_numpy(logits)\n",
    "    \n",
    "    # Convert logits to log probabilities\n",
    "    log_probs = torch.log_softmax(logits, dim=-1)\n",
    "    \n",
    "    T, V = log_probs.shape\n",
    "    dense = k2.DenseFsaVec(log_probs.unsqueeze(0), torch.tensor([[0, 0, T]], dtype=torch.int32))\n",
    "    ctc_topo = k2.arc_sort(k2.ctc_topo(V-1))\n",
    "    \n",
    "    # No pattern FSA - just CTC topology\n",
    "    lattice = k2.intersect_dense_pruned(\n",
    "        ctc_topo, dense,\n",
    "        search_beam=search_beam, \n",
    "        output_beam=output_beam,\n",
    "        min_active_states=30, \n",
    "        max_active_states=10000\n",
    "    )\n",
    "    \n",
    "    best_path = k2.shortest_path(lattice, use_double_scores=False)\n",
    "    aux = k2.get_aux_labels(best_path)[0]\n",
    "    hyp_ids = [x for x in aux if x >= 0]\n",
    "    \n",
    "    result = \"\".join(id2tok[i] for i in hyp_ids)\n",
    "    \n",
    "    # Replace word delimiter with space\n",
    "    word_delim = processor.tokenizer.word_delimiter_token\n",
    "    if word_delim:\n",
    "        result = result.replace(word_delim, \" \")\n",
    "    \n",
    "    return result.strip()\n",
    "\n",
    "def ctc_decode_greedy(logits):\n",
    "    \"\"\"\n",
    "    Greedy CTC decoding: argmax at each frame, then collapse repeats and remove blanks.\n",
    "    Takes raw logits as input (not log_probs).\n",
    "    \"\"\"\n",
    "    # Convert to torch tensor if needed\n",
    "    if isinstance(logits, np.ndarray):\n",
    "        logits = torch.from_numpy(logits)\n",
    "    \n",
    "    # Get the most probable token at each frame (argmax on logits)\n",
    "    greedy_ids = logits.argmax(dim=-1)  # Shape: (T,)\n",
    "    \n",
    "    # Collapse repeats and remove blanks\n",
    "    output = []\n",
    "    prev_id = None\n",
    "    \n",
    "    for token_id in greedy_ids.tolist():\n",
    "        if token_id == blank_id:\n",
    "            prev_id = None  # Reset on blank\n",
    "            continue\n",
    "        if token_id != prev_id:  # Only add if different from previous\n",
    "            output.append(token_id)\n",
    "            prev_id = token_id\n",
    "    \n",
    "    result = \"\".join(id2tok[i] for i in output)\n",
    "    \n",
    "    # Replace word delimiter with space\n",
    "    word_delim = processor.tokenizer.word_delimiter_token\n",
    "    if word_delim:\n",
    "        result = result.replace(word_delim, \" \")\n",
    "    \n",
    "    return result.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e13f4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "greedy_output: فَمِنَ الرَّامِ السُّوَيْديِّ - أُسْكُرْ سَواهِنْ - الَّذِي شَرَكَ فِي أُولَمْبِيَادِ أَلْفُتِسُعْمِئَةٍ وَعِشْرونَ - حِينَ كَانَ يَبْلُغُ اثْنَيْنِ وَسَبرينَ عَاماً - وَمِئتَيْنِ وَوَاحِدُ وثَمانينَ يَوْمًا - إِلَى الفارسَتَيْنِ أَنْْيُوزْلَندِيَّةِ - جيولِي بروغْهَامْ - والأَسْتْرَاليَّةِ مارِي هَنَا - الَّتَيْنِ تُشَارِكَانِ فِي أُولَمْبِيَادِرِيُو - وَهُمَا فِي الْوَاحِدِ وَالسِّتّينَ مِنَ الْعُمُرِ\n",
      "transcription: فَمِنَ الرَّامِ السُّوَيْديِّ - أُسْكُرْ سَواهِنْ - الَّذِي شَرَكَ فِي أُولَمْبِيَادِ أَلْفُتِسُعْمِئَةٍ وَعِشْرونَ - حِينَ كَانَ يَبْلُغُ اثْنَيْنِ وَسَبرينَ عَاماً - وَمِئتَيْنِ وَوَاحِدُ وثَمانينَ يَوْمًا - إِلَى الفارسَتَيْنِ أَنْْيُوزْلَندِيَّةِ - جيولِي بروغْهَامْ - والأَسْتْرَاليَّةِ مارِي هَنَا - الَّتَيْنِ تُشَارِكَانِ فِي أُولَمْبِيَادِرِيُو - وَهُمَا فِي الْوَاحِدِ وَالسِّتّينَ مِنَ الْعُمُرِ\n",
      "reference: فَمِنَ|الرَّامِي|السُّوِيدِيِّ|أُوسْكَار|سَوَاهِنْ|الَّذِي|شَارَكَ|فِي|أُولُمْبِيَادِ||أَلْفْ|وَتِسْعُمِئَةٍ|وَعِشْرُونْ|حِينَ|كَانَ|يَبْلُغُ|إَثْنَينْ|وَ|سَبْعينَ||عَامًا|وَمِئَتَينْ|وَوَاحِدْ|وَثَمَانِينَ|يَوْمًا|إِلَى|الْفَارِسَتَيْنِ|النِّيُوزِيلَنْدِيَّةِ|جُولِي|برُوغْهَام|وَالْأُسْتُرَالِيَّةِ|مَارِي|هَانَا|اللَّتَيْنِ|تُشَارِكَانِ|فِي|أُولُمْبِيَّادِ|رِيُو|وَهُمَا|فِي|الْوَاحِدِ|وَالسِّتِينَ|مِنَ|الْعُمُر\n",
      "\n",
      "\n",
      "k2_ctc_output: فَمِنَ الرَّامِ السُّوَيْديِّ - أُسْكُرْ سَواهِنْ - الَّذِي شَرَكَ فِي أُولَمْبِيَادِ أَلْفُتِسُعْمِئَةٍ وَعِشْرونَ - حِينَ كَانَ يَبْلُغُ اثْنَيْنِ وَسَبرينَ عَاماً - وَمِئتَيْنِ وَوَاحِدُ وثَمانينَ يَوْمًا - إِلَى الفارسَتَيْنِ أَنْْيُوزْلَندِيَّةِ - جيولِي بروغْهَامْ - والأَسْتْرَاليَّةِ مارِي هَنَا - الَّتَيْنِ تُشَارِكَانِ فِي أُولَمْبِيَادِرِيُو - وَهُمَا فِي الْوَاحِدِ وَالسِّتّينَ مِنَ الْعُمُرِ\n",
      "transcription: فَمِنَ الرَّامِ السُّوَيْديِّ - أُسْكُرْ سَواهِنْ - الَّذِي شَرَكَ فِي أُولَمْبِيَادِ أَلْفُتِسُعْمِئَةٍ وَعِشْرونَ - حِينَ كَانَ يَبْلُغُ اثْنَيْنِ وَسَبرينَ عَاماً - وَمِئتَيْنِ وَوَاحِدُ وثَمانينَ يَوْمًا - إِلَى الفارسَتَيْنِ أَنْْيُوزْلَندِيَّةِ - جيولِي بروغْهَامْ - والأَسْتْرَاليَّةِ مارِي هَنَا - الَّتَيْنِ تُشَارِكَانِ فِي أُولَمْبِيَادِرِيُو - وَهُمَا فِي الْوَاحِدِ وَالسِّتّينَ مِنَ الْعُمُرِ\n",
      "reference: فَمِنَ|الرَّامِي|السُّوِيدِيِّ|أُوسْكَار|سَوَاهِنْ|الَّذِي|شَارَكَ|فِي|أُولُمْبِيَادِ||أَلْفْ|وَتِسْعُمِئَةٍ|وَعِشْرُونْ|حِينَ|كَانَ|يَبْلُغُ|إَثْنَينْ|وَ|سَبْعينَ||عَامًا|وَمِئَتَينْ|وَوَاحِدْ|وَثَمَانِينَ|يَوْمًا|إِلَى|الْفَارِسَتَيْنِ|النِّيُوزِيلَنْدِيَّةِ|جُولِي|برُوغْهَام|وَالْأُسْتُرَالِيَّةِ|مَارِي|هَانَا|اللَّتَيْنِ|تُشَارِكَانِ|فِي|أُولُمْبِيَّادِ|رِيُو|وَهُمَا|فِي|الْوَاحِدِ|وَالسِّتِينَ|مِنَ|الْعُمُر\n",
      "\n",
      "\n",
      "hyp_diacritics: فَمِنَ الرَّامِي السُّوَيدِيِّ أُوسْكُار سَوَاهِنْ الَّذِي شَارَكَ فِي أُولَمْبِيَادِ  أَلْفُ وْتِسُعْمِئَةٍ وَعِشْرُونَ حِينَ كَانَ يَبْلُغُ إَثْنَينِ وَ سَبْعينَ  عَامًا وَمِئَتَينِ وَوَاحِدُ وَثَمَانِينَ يَوْمًا إِلَى الْفَارِسَتَيْنِ النْْيُوزْيلَنْدِيَّةِ جُولِي برُوغْهَام وَالْأَسْتْرَالِيَّةِ مَارِي هَانَا اللَّتَيْنِ تُشَارِكَانِ فِي أُولَمْبِيَّادِ رِيُو وَهُمَا فِي الْوَاحِدِ وَالسِّتّينَ مِنَ الْعُمُر\n",
      "transcription: فَمِنَ الرَّامِ السُّوَيْديِّ - أُسْكُرْ سَواهِنْ - الَّذِي شَرَكَ فِي أُولَمْبِيَادِ أَلْفُتِسُعْمِئَةٍ وَعِشْرونَ - حِينَ كَانَ يَبْلُغُ اثْنَيْنِ وَسَبرينَ عَاماً - وَمِئتَيْنِ وَوَاحِدُ وثَمانينَ يَوْمًا - إِلَى الفارسَتَيْنِ أَنْْيُوزْلَندِيَّةِ - جيولِي بروغْهَامْ - والأَسْتْرَاليَّةِ مارِي هَنَا - الَّتَيْنِ تُشَارِكَانِ فِي أُولَمْبِيَادِرِيُو - وَهُمَا فِي الْوَاحِدِ وَالسِّتّينَ مِنَ الْعُمُرِ\n",
      "reference: فَمِنَ|الرَّامِي|السُّوِيدِيِّ|أُوسْكَار|سَوَاهِنْ|الَّذِي|شَارَكَ|فِي|أُولُمْبِيَادِ||أَلْفْ|وَتِسْعُمِئَةٍ|وَعِشْرُونْ|حِينَ|كَانَ|يَبْلُغُ|إَثْنَينْ|وَ|سَبْعينَ||عَامًا|وَمِئَتَينْ|وَوَاحِدْ|وَثَمَانِينَ|يَوْمًا|إِلَى|الْفَارِسَتَيْنِ|النِّيُوزِيلَنْدِيَّةِ|جُولِي|برُوغْهَام|وَالْأُسْتُرَالِيَّةِ|مَارِي|هَانَا|اللَّتَيْنِ|تُشَارِكَانِ|فِي|أُولُمْبِيَّادِ|رِيُو|وَهُمَا|فِي|الْوَاحِدِ|وَالسِّتِينَ|مِنَ|الْعُمُر\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "arabic_reference = arabic_reference.replace(' ', '|')\n",
    "pattern_diacritics = [ch if ch not in diac_in_vocab else '.' for ch in arabic_reference]\n",
    "\n",
    "logits, transcription  = get_logits(arabic_audio_path)\n",
    "wildcard_ids = [ i for i,ch in enumerate(vocab)\n",
    "                if ch in diac_in_vocab]\n",
    "\n",
    "# select logits for the diacritics only\n",
    "# logits_diacritics = logits[:, wildcard_ids]\n",
    "# regular_output = ctc_decode_k2(logits.squeeze(0), search_beam=20.0, output_beam=8.0)\n",
    "#################### Greedy CTC ####################\n",
    "greedy_output = ctc_decode_greedy(logits.squeeze(0))\n",
    "print(f\"greedy_output: {greedy_output}\")\n",
    "print(f\"transcription: {transcription}\")\n",
    "print(f\"reference: {arabic_reference}\\n\\n\")\n",
    "############################################################\n",
    "\n",
    "#################### CTC K2 ########################\n",
    "regular_output = ctc_decode_k2(logits.squeeze(0), search_beam=20.0, output_beam=8.0)\n",
    "print(f\"k2_ctc_output: {regular_output}\")\n",
    "print(f\"transcription: {transcription}\")\n",
    "print(f\"reference: {arabic_reference}\\n\\n\")\n",
    "############################################################\n",
    "\n",
    "#################### CTC+WFS ############################\n",
    "hyp_diacritics = wildcard_decode_k2(logits.squeeze(0), pattern_diacritics, wildcard_ids)\n",
    "print(f\"hyp_diacritics: {hyp_diacritics}\")\n",
    "print(f\"transcription: {transcription}\")\n",
    "print(f\"reference: {arabic_reference}\\n\\n\")\n",
    "############################################################\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "eb83aae5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wsft WER: 0.3333333333333333\n",
      "greedy WER: 0.8095238095238095\n",
      "k2_ctc WER: 0.8095238095238095\n"
     ]
    }
   ],
   "source": [
    "from jiwer import wer\n",
    "\n",
    "\n",
    "\n",
    "def clean_text(text, remove_diacritics=False):\n",
    "    if remove_diacritics:\n",
    "        text = araby.strip_diacritics(text)\n",
    "    return text.replace('|', ' ')\n",
    "\n",
    "def calculate_wer(hyp, ref):\n",
    "    hyp = clean_text(hyp)\n",
    "    ref = clean_text(ref)\n",
    "    return wer(ref, hyp)\n",
    "\n",
    "wer_wsft = wer(arabic_reference, hyp_diacritics)\n",
    "\n",
    "print(f\"wsft WER: {calculate_wer(hyp_diacritics, arabic_reference)}\")\n",
    "print(f\"greedy WER: {calculate_wer(greedy_output, arabic_reference)}\")\n",
    "print(f\"k2_ctc WER: {calculate_wer(regular_output, arabic_reference)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a1f08a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
