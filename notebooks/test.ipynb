{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6afd852",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125933bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# colab only setup uncomment to install dependencies\n",
    "%%bash\n",
    "pip install -q \\\n",
    "    torch>=2.9.1 \\\n",
    "    torchaudio>=2.9.1 \\\n",
    "    transformers>=4.57.0 \\\n",
    "    k2==1.24.4.dev20251118+cuda12.8.torch2.9.1 \\\n",
    "    soundfile==0.31.1 \\\n",
    "    librosa==0.11.0 \\\n",
    "    pyarabic==0.6.10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87381bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import k2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e085600c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting environment information...\n",
      "\n",
      "k2 version: 1.24.4\n",
      "Build type: Release\n",
      "Git SHA1: 30c3039fbe89f245d5dba3c47e99abc3a638275f\n",
      "Git date: Tue Nov 18 07:41:31 2025\n",
      "Cuda used to build k2: 12.8\n",
      "cuDNN used to build k2: \n",
      "Python version used to build k2: 3.12\n",
      "OS used to build k2: AlmaLinux release 8.10 (Cerulean Leopard)\n",
      "CMake version: 4.1.2\n",
      "GCC version: 13.3.1\n",
      "CMAKE_CUDA_FLAGS: -Wno-deprecated-gpu-targets -lineinfo --expt-extended-lambda -use_fast_math -Xptxas=-w --expt-extended-lambda -gencode arch=compute_50,code=sm_50 -lineinfo --expt-extended-lambda -use_fast_math -Xptxas=-w --expt-extended-lambda -gencode arch=compute_60,code=sm_60 -lineinfo --expt-extended-lambda -use_fast_math -Xptxas=-w --expt-extended-lambda -gencode arch=compute_61,code=sm_61 -lineinfo --expt-extended-lambda -use_fast_math -Xptxas=-w --expt-extended-lambda -gencode arch=compute_70,code=sm_70 -lineinfo --expt-extended-lambda -use_fast_math -Xptxas=-w --expt-extended-lambda -gencode arch=compute_75,code=sm_75 -lineinfo --expt-extended-lambda -use_fast_math -Xptxas=-w --expt-extended-lambda -gencode arch=compute_80,code=sm_80 -lineinfo --expt-extended-lambda -use_fast_math -Xptxas=-w --expt-extended-lambda -gencode arch=compute_86,code=sm_86 -DONNX_NAMESPACE=onnx_c2 -gencode arch=compute_50,code=sm_50 -gencode arch=compute_80,code=sm_80 -gencode arch=compute_86,code=sm_86 -gencode arch=compute_89,code=sm_89 -gencode arch=compute_90,code=sm_90 -gencode arch=compute_90a,code=sm_90a -gencode arch=compute_100,code=sm_100 -gencode arch=compute_100a,code=sm_100a -gencode arch=compute_101a,code=sm_101a -gencode arch=compute_120,code=sm_120 -gencode arch=compute_120a,code=sm_120a -Xcudafe --diag_suppress=cc_clobber_ignored,--diag_suppress=field_without_dll_interface,--diag_suppress=base_class_has_different_dll_interface,--diag_suppress=dll_interface_conflict_none_assumed,--diag_suppress=dll_interface_conflict_dllexport_assumed,--diag_suppress=bad_friend_decl --expt-relaxed-constexpr --expt-extended-lambda  --compiler-options -Wall  --compiler-options -Wno-strict-overflow  --compiler-options -Wno-unknown-pragmas \n",
      "CMAKE_CXX_FLAGS:   -Wno-unused-variable  -Wno-strict-overflow \n",
      "PyTorch version used to build k2: 2.9.1+cu128\n",
      "PyTorch is using Cuda: 12.8\n",
      "NVTX enabled: True\n",
      "With CUDA: True\n",
      "Disable debug: True\n",
      "Sync kernels : False\n",
      "Disable checks: False\n",
      "Max cpu memory allocate: 214748364800 bytes (or 200.0 GB)\n",
      "k2 abort: False\n",
      "__file__: /home/rufael/Projects/diac-btc/.venv/lib/python3.12/site-packages/k2/version/version.py\n",
      "_k2.__file__: /home/rufael/Projects/diac-btc/.venv/lib/python3.12/site-packages/_k2.cpython-312-x86_64-linux-gnu.so\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "import k2.version\n",
    "k2.version.version.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6445ddd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.43.0 (0)\n",
       " -->\n",
       "<!-- Title: WFSA Pages: 1 -->\n",
       "<svg width=\"324pt\" height=\"80pt\"\n",
       " viewBox=\"0.00 0.00 324.00 80.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 76)\">\n",
       "<title>WFSA</title>\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-76 320,-76 320,4 -4,4\"/>\n",
       "<!-- 0 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>0</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" stroke-width=\"2\" cx=\"18\" cy=\"-45\" rx=\"18\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"18\" y=\"-41.3\" font-family=\"Times,serif\" font-size=\"14.00\">0</text>\n",
       "</g>\n",
       "<!-- 1 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>1</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" stroke-width=\"2\" cx=\"196\" cy=\"-45\" rx=\"18\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"196\" y=\"-41.3\" font-family=\"Times,serif\" font-size=\"14.00\">1</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;1 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>0&#45;&gt;1</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M35.46,-49.93C40.41,-51.19 45.88,-52.37 51,-53 100.4,-59.08 113.6,-59.08 163,-53 164.76,-52.78 166.56,-52.5 168.37,-52.18\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"169.52,-55.5 178.54,-49.93 168.02,-48.67 169.52,-55.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"107\" y=\"-60.8\" font-family=\"Times,serif\" font-size=\"14.00\">1/1</text>\n",
       "</g>\n",
       "<!-- 2 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>2</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" stroke-width=\"2\" cx=\"107\" cy=\"-18\" rx=\"18\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"107\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">2</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;2 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>0&#45;&gt;2</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M35.36,-39.92C47.88,-36.04 65.39,-30.6 79.84,-26.12\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"81.2,-29.36 89.71,-23.06 79.12,-22.68 81.2,-29.36\"/>\n",
       "<text text-anchor=\"middle\" x=\"62.5\" y=\"-37.8\" font-family=\"Times,serif\" font-size=\"14.00\">2/2</text>\n",
       "</g>\n",
       "<!-- 3 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>3</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" stroke-width=\"2\" cx=\"294\" cy=\"-45\" rx=\"18\" ry=\"18\"/>\n",
       "<ellipse fill=\"none\" stroke=\"black\" stroke-width=\"2\" cx=\"294\" cy=\"-45\" rx=\"22\" ry=\"22\"/>\n",
       "<text text-anchor=\"middle\" x=\"294\" y=\"-41.3\" font-family=\"Times,serif\" font-size=\"14.00\">3</text>\n",
       "</g>\n",
       "<!-- 1&#45;&gt;3 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>1&#45;&gt;3</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M214.16,-45C227.31,-45 245.81,-45 261.58,-45\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"261.97,-48.5 271.97,-45 261.97,-41.5 261.97,-48.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"243\" y=\"-48.8\" font-family=\"Times,serif\" font-size=\"14.00\">&#45;1/3</text>\n",
       "</g>\n",
       "<!-- 2&#45;&gt;1 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>2&#45;&gt;1</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M124.36,-23.08C136.88,-26.96 154.39,-32.4 168.84,-36.88\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"168.12,-40.32 178.71,-39.94 170.2,-33.64 168.12,-40.32\"/>\n",
       "<text text-anchor=\"middle\" x=\"151.5\" y=\"-37.8\" font-family=\"Times,serif\" font-size=\"14.00\">3/4</text>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x7f3a531df350>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = '''\n",
    "0 1 1 1\n",
    "0 2 2 2\n",
    "1 3 -1 3\n",
    "2 1 3 4\n",
    "3\n",
    "'''\n",
    "fsa = k2.Fsa.from_str(s)\n",
    "k2.to_dot(fsa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e3b7f105",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.43.0 (0)\n",
       " -->\n",
       "<!-- Title: WFSA Pages: 1 -->\n",
       "<svg width=\"324pt\" height=\"80pt\"\n",
       " viewBox=\"0.00 0.00 324.00 80.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 76)\">\n",
       "<title>WFSA</title>\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-76 320,-76 320,4 -4,4\"/>\n",
       "<!-- 0 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>0</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" stroke-width=\"2\" cx=\"18\" cy=\"-45\" rx=\"18\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"18\" y=\"-41.3\" font-family=\"Times,serif\" font-size=\"14.00\">0</text>\n",
       "</g>\n",
       "<!-- 1 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>1</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" stroke-width=\"2\" cx=\"196\" cy=\"-45\" rx=\"18\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"196\" y=\"-41.3\" font-family=\"Times,serif\" font-size=\"14.00\">1</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;1 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>0&#45;&gt;1</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M35.46,-49.93C40.41,-51.19 45.88,-52.37 51,-53 100.4,-59.08 113.6,-59.08 163,-53 164.76,-52.78 166.56,-52.5 168.37,-52.18\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"169.52,-55.5 178.54,-49.93 168.02,-48.67 169.52,-55.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"107\" y=\"-60.8\" font-family=\"Times,serif\" font-size=\"14.00\">1/1</text>\n",
       "</g>\n",
       "<!-- 2 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>2</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" stroke-width=\"2\" cx=\"107\" cy=\"-18\" rx=\"18\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"107\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">2</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;2 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>0&#45;&gt;2</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M35.36,-39.92C47.88,-36.04 65.39,-30.6 79.84,-26.12\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"81.2,-29.36 89.71,-23.06 79.12,-22.68 81.2,-29.36\"/>\n",
       "<text text-anchor=\"middle\" x=\"62.5\" y=\"-37.8\" font-family=\"Times,serif\" font-size=\"14.00\">2/2</text>\n",
       "</g>\n",
       "<!-- 3 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>3</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" stroke-width=\"2\" cx=\"294\" cy=\"-45\" rx=\"18\" ry=\"18\"/>\n",
       "<ellipse fill=\"none\" stroke=\"black\" stroke-width=\"2\" cx=\"294\" cy=\"-45\" rx=\"22\" ry=\"22\"/>\n",
       "<text text-anchor=\"middle\" x=\"294\" y=\"-41.3\" font-family=\"Times,serif\" font-size=\"14.00\">3</text>\n",
       "</g>\n",
       "<!-- 1&#45;&gt;3 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>1&#45;&gt;3</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M214.16,-45C227.31,-45 245.81,-45 261.58,-45\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"261.97,-48.5 271.97,-45 261.97,-41.5 261.97,-48.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"243\" y=\"-48.8\" font-family=\"Times,serif\" font-size=\"14.00\">&#45;1/3</text>\n",
       "</g>\n",
       "<!-- 2&#45;&gt;1 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>2&#45;&gt;1</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M124.36,-23.08C136.88,-26.96 154.39,-32.4 168.84,-36.88\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"168.12,-40.32 178.71,-39.94 170.2,-33.64 168.12,-40.32\"/>\n",
       "<text text-anchor=\"middle\" x=\"151.5\" y=\"-37.8\" font-family=\"Times,serif\" font-size=\"14.00\">3/4</text>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x7f3a43b4cd70>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syms = '''\n",
    "<eps> 0\n",
    "a 1\n",
    "b 2\n",
    "c 3\n",
    "'''\n",
    "fsa.symbols = k2.SymbolTable.from_str(syms)\n",
    "k2.to_dot(fsa)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "049b8196",
   "metadata": {},
   "source": [
    "### English Exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "631706dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rufael/Projects/diac-btc/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded: facebook/wav2vec2-base-960h vocab size: 32\n"
     ]
    }
   ],
   "source": [
    "import torch, k2, soundfile as sf, numpy as np\n",
    "from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC, AutoModelForCTC\n",
    "\n",
    "\n",
    "model_name = \"facebook/wav2vec2-base-960h\"\n",
    "processor  = Wav2Vec2Processor.from_pretrained(model_name)\n",
    "model      = AutoModelForCTC.from_pretrained(model_name).cuda().eval()\n",
    "\n",
    "vocab = list(processor.tokenizer.get_vocab().keys())\n",
    "id2tok = {v: k for k, v in processor.tokenizer.get_vocab().items()}\n",
    "blank_id = processor.tokenizer.pad_token_id\n",
    "\n",
    "if blank_id is None:\n",
    "        blank_id = processor.tokenizer.word_delimiter_token_id\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "print(\"Model loaded:\", model_name, \"vocab size:\", vocab_size)\n",
    "\n",
    "def get_log_probs(path):\n",
    "    wav, sr = sf.read(path)\n",
    "    if sr != 16000:\n",
    "        import torchaudio\n",
    "        wav = torchaudio.functional.resample(torch.tensor(wav).float(), sr, 16000).numpy()\n",
    "        sr = 16000\n",
    "    inputs = processor(wav, sampling_rate=sr, return_tensors=\"pt\", padding=True)\n",
    "    with torch.no_grad():\n",
    "        logits = model(inputs.input_values.cuda()).logits[0]\n",
    "    log_probs = torch.log_softmax(logits, dim=-1).cpu()\n",
    "    return log_probs\n",
    "\n",
    "def get_logits(audio_path):\n",
    "    \"\"\"Load audio and get model logits.\"\"\"\n",
    "    wav, sr = sf.read(audio_path)\n",
    "    \n",
    "    # Resample if needed\n",
    "    if sr != 16000:\n",
    "        import librosa\n",
    "        wav = librosa.resample(wav, orig_sr=sr, target_sr=16000)\n",
    "        sr = 16000\n",
    "    \n",
    "    # Handle stereo\n",
    "    if len(wav.shape) > 1:\n",
    "        wav = wav[:, 0]\n",
    "    \n",
    "    # Get logits\n",
    "    inputs = processor(wav, sampling_rate=sr, return_tensors=\"pt\", padding=True)\n",
    "    with torch.no_grad():\n",
    "        logits = model(inputs.input_values.cuda()).logits[0].cpu().numpy()\n",
    "    \n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ac4922f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import torch, k2, soundfile as sf, numpy as np\n",
    "from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC, AutoModelForCTC\n",
    "\n",
    "\n",
    "model_name = \"facebook/wav2vec2-base-960h\"\n",
    "processor  = Wav2Vec2Processor.from_pretrained(model_name)\n",
    "model      = AutoModelForCTC.from_pretrained(model_name).cuda().eval()\n",
    "\n",
    "vocab = list(processor.tokenizer.get_vocab().keys())\n",
    "id2tok = {v: k for k, v in processor.tokenizer.get_vocab().items()}\n",
    "blank_id = processor.tokenizer.pad_token_id\n",
    "\n",
    "def build_pattern_fsa(pattern, token2id, wildcard_ids):\n",
    "    \"\"\"\n",
    "    pattern: list of characters, '.' for wildcard\n",
    "    token2id: dict mapping from char -> token id\n",
    "    wildcard_ids: allowed token ids for wildcard positions\n",
    "    \"\"\"\n",
    "    arcs = []\n",
    "    state = 0\n",
    "    for i, ch in enumerate(pattern):\n",
    "        if ch == '.':\n",
    "            for wid in wildcard_ids:\n",
    "                arcs.append(f\"{state} {state+1} {wid} {wid} 0.0\")\n",
    "        else:\n",
    "            if ch not in token2id:\n",
    "                continue\n",
    "            tid = token2id[ch]\n",
    "            arcs.append(f\"{state} {state+1} {tid} {tid} 0.0\")\n",
    "        state += 1\n",
    "    arcs.append(f\"{state} 0.0\")\n",
    "    txt = \"\\n\".join(arcs)\n",
    "    fsa = k2.Fsa.from_str(txt, acceptor=False, openfst=True)\n",
    "    return k2.arc_sort(fsa)\n",
    "\n",
    "def wildcard_decode_k2_old(log_probs, pattern, wildcard_set):\n",
    "    \"\"\"\n",
    "    log_probs: (T, V) log probabilities\n",
    "    pattern: list of characters (with '.')\n",
    "    wildcard_set: list of allowed tokens for '.'\n",
    "    \"\"\"\n",
    "    T, V = log_probs.shape\n",
    "    dense = k2.DenseFsaVec(log_probs.unsqueeze(0), torch.tensor([[0, 0, T]], dtype=torch.int32))\n",
    "    ctc_topo = k2.arc_sort(k2.ctc_topo(V-1))\n",
    "    pattern_fsa = build_pattern_fsa(pattern, processor.tokenizer.get_vocab(), wildcard_set)\n",
    "    decoding_graph = k2.arc_sort(k2.compose(ctc_topo, pattern_fsa))\n",
    "    lattice = k2.intersect_dense_pruned(\n",
    "       decoding_graph, dense,\n",
    "       search_beam=20.0, output_beam=8.0,\n",
    "       min_active_states=30, max_active_states=10000)\n",
    "    # lattice = k2.intersect_dense(\n",
    "    #     decoding_graph, dense, output_beam=10.0)\n",
    "    best_path = k2.shortest_path(lattice, use_double_scores=False)\n",
    "    aux = k2.get_aux_labels(best_path)[0]\n",
    "    hyp_ids = [x for x in aux if x >= 0]\n",
    "    return \"\".join(id2tok[i] for i in hyp_ids)\n",
    "\n",
    "def wildcard_decode_k2(logits, pattern, wildcard_set):\n",
    "    \"\"\"\n",
    "    logits: (T, V) raw logits from model\n",
    "    pattern: list of characters (with '.')\n",
    "    wildcard_set: list of allowed tokens for '.'\n",
    "    \"\"\"\n",
    "    # Convert to torch tensor if needed\n",
    "    if isinstance(logits, np.ndarray):\n",
    "        logits = torch.from_numpy(logits)\n",
    "    \n",
    "    # Convert logits to log probabilities\n",
    "    log_probs = torch.log_softmax(logits, dim=-1)\n",
    "    \n",
    "    T, V = log_probs.shape\n",
    "    dense = k2.DenseFsaVec(log_probs.unsqueeze(0), torch.tensor([[0, 0, T]], dtype=torch.int32))\n",
    "    ctc_topo = k2.arc_sort(k2.ctc_topo(V-1))\n",
    "    pattern_fsa = build_pattern_fsa(pattern, processor.tokenizer.get_vocab(), wildcard_set)\n",
    "    decoding_graph = k2.arc_sort(k2.compose(ctc_topo, pattern_fsa))\n",
    "    lattice = k2.intersect_dense_pruned(\n",
    "       decoding_graph, dense,\n",
    "       search_beam=20.0, output_beam=8.0,\n",
    "       min_active_states=30, max_active_states=10000)\n",
    "    \n",
    "    best_path = k2.shortest_path(lattice, use_double_scores=False)\n",
    "    aux = k2.get_aux_labels(best_path)[0]\n",
    "    hyp_ids = [x for x in aux if x >= 0]\n",
    "    \n",
    "    result = \"\".join(id2tok[i] for i in hyp_ids)\n",
    "    \n",
    "    # Replace word delimiter with space\n",
    "    word_delim = processor.tokenizer.word_delimiter_token\n",
    "    if word_delim:\n",
    "        result = result.replace(word_delim, \" \")\n",
    "    \n",
    "    return result.strip()\n",
    "\n",
    "#Regular CTC Decode:\n",
    "def ctc_decode_k2_old(log_probs, search_beam=20.0, output_beam=8.0):\n",
    "    \"\"\"\n",
    "    CTC decoding using k2 with same beam settings (no pattern constraints).\n",
    "    This is equivalent to your constrained decoding but without the pattern FSA.\n",
    "    \"\"\"\n",
    "    T, V = log_probs.shape\n",
    "    dense = k2.DenseFsaVec(log_probs.unsqueeze(0), torch.tensor([[0, 0, T]], dtype=torch.int32))\n",
    "    ctc_topo = k2.arc_sort(k2.ctc_topo(V-1))\n",
    "    \n",
    "    # No pattern FSA - just CTC topology\n",
    "    lattice = k2.intersect_dense_pruned(\n",
    "        ctc_topo, dense,\n",
    "        search_beam=search_beam, \n",
    "        output_beam=output_beam,\n",
    "        min_active_states=30, \n",
    "        max_active_states=10000\n",
    "    )\n",
    "    \n",
    "    best_path = k2.shortest_path(lattice, use_double_scores=False)\n",
    "    aux = k2.get_aux_labels(best_path)[0]\n",
    "    hyp_ids = [x for x in aux if x >= 0]\n",
    "    return \"\".join(id2tok[i] for i in hyp_ids)\n",
    "\n",
    "def ctc_decode_k2(logits, search_beam=20.0, output_beam=8.0):\n",
    "    \"\"\"\n",
    "    CTC decoding using k2 with beam settings (no pattern constraints).\n",
    "    Takes raw logits as input.\n",
    "    \"\"\"\n",
    "    # Convert to torch tensor if needed\n",
    "    if isinstance(logits, np.ndarray):\n",
    "        logits = torch.from_numpy(logits)\n",
    "    \n",
    "    # Convert logits to log probabilities\n",
    "    log_probs = torch.log_softmax(logits, dim=-1)\n",
    "    \n",
    "    T, V = log_probs.shape\n",
    "    dense = k2.DenseFsaVec(log_probs.unsqueeze(0), torch.tensor([[0, 0, T]], dtype=torch.int32))\n",
    "    ctc_topo = k2.arc_sort(k2.ctc_topo(V-1))\n",
    "    \n",
    "    # No pattern FSA - just CTC topology\n",
    "    lattice = k2.intersect_dense_pruned(\n",
    "        ctc_topo, dense,\n",
    "        search_beam=search_beam, \n",
    "        output_beam=output_beam,\n",
    "        min_active_states=30, \n",
    "        max_active_states=10000\n",
    "    )\n",
    "    \n",
    "    best_path = k2.shortest_path(lattice, use_double_scores=False)\n",
    "    aux = k2.get_aux_labels(best_path)[0]\n",
    "    hyp_ids = [x for x in aux if x >= 0]\n",
    "    \n",
    "    result = \"\".join(id2tok[i] for i in hyp_ids)\n",
    "    \n",
    "    # Replace word delimiter with space\n",
    "    word_delim = processor.tokenizer.word_delimiter_token\n",
    "    if word_delim:\n",
    "        result = result.replace(word_delim, \" \")\n",
    "    \n",
    "    return result.strip()\n",
    "\n",
    "def ctc_decode_greedy(logits):\n",
    "    \"\"\"\n",
    "    Greedy CTC decoding: argmax at each frame, then collapse repeats and remove blanks.\n",
    "    Takes raw logits as input (not log_probs).\n",
    "    \"\"\"\n",
    "    # Convert to torch tensor if needed\n",
    "    if isinstance(logits, np.ndarray):\n",
    "        logits = torch.from_numpy(logits)\n",
    "    \n",
    "    # Get the most probable token at each frame (argmax on logits)\n",
    "    greedy_ids = logits.argmax(dim=-1)  # Shape: (T,)\n",
    "    \n",
    "    # Collapse repeats and remove blanks\n",
    "    output = []\n",
    "    prev_id = None\n",
    "    \n",
    "    for token_id in greedy_ids.tolist():\n",
    "        if token_id == blank_id:\n",
    "            prev_id = None  # Reset on blank\n",
    "            continue\n",
    "        if token_id != prev_id:  # Only add if different from previous\n",
    "            output.append(token_id)\n",
    "            prev_id = token_id\n",
    "    \n",
    "    result = \"\".join(id2tok[i] for i in output)\n",
    "    \n",
    "    # Replace word delimiter with space\n",
    "    word_delim = processor.tokenizer.word_delimiter_token\n",
    "    if word_delim:\n",
    "        result = result.replace(word_delim, \" \")\n",
    "    \n",
    "    return result.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0455e5a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added space token '|' to wildcard set\n",
      "Reference : THE|BIRCH|CANOE|SLID|ON|THE|SMOOTH|PLANK|GLUE|THE|SHEET|TO|THE|DARK|BLUE|BACKGROUND|IT|IS|EASY|TO|TELL|THE|DEPTH|OF|THE|WELL|THESE|DAYS|A|CHICKEN|LEG|IS|A|RARE|DISH|RICE|IS|OFTEN|SERVED|IN|ROUND|BOWLS|THE|JUSE|OF|LEMON|MAKES|FINE|PUNCH|THE|BOX|WAS|TONL||BESIDE|THE|PARK|TRUNK|THE|HOX|ARE|SED|CHOPPED|CORN|AND|GARBAGE|FOUR|HOURS|A|STEADY|WORK|FACED|US|A|LARGE|SIDE|IN|STOCKINGS|IS|HARD|TO|SELL\n",
      "CTC prediction  THE BIRCH CANOE SLIT ON THE SMOOTH PLANK GLE THE HEE TO THE DARK BLUE BACKGROUND IT IS EASY TO TELL THE DEPTH OF THE WELL THESE DAYS A CICK A MEG IS A RARE DISH RICE IS OXEN SERVED IN ROUND BULL THE JUSE OF LONDONS MAKES FINE PUNCH THE BOX WAS TONL  BESIDE THE PARK TRUK THE HOX ARE SED CHOPPED CORN AND GARBAGE FOUR HOURS A STEADY WORK FACED US E LARGE SIDE AN STOCKINGS IS HARD TO SELL\n",
      "CTC greedy prediction  THE BIRCH CANOE SLIT ON THE SMOOTH PLANK GLE THE HEE TO THE DARK BLUE BACKGROUND IT IS EASY TO TELL THE DEPTH OF THE WELL THESE DAYS A CICK A MEG IS A RARE DISH RICE IS OXEN SERVED IN ROUND BULL THE JUSE OF LONDONS MAKES FINE PUNCH THE BOX WAS TONL  BESIDE THE PARK TRUK THE HOX ARE SED CHOPPED CORN AND GARBAGE FOUR HOURS A STEADY WORK FACED US E LARGE SIDE AN STOCKINGS IS HARD TO SELL\n",
      "Pattern (vowels masked)     : TH.|B.RCH|C.N..|SL.D|.N|TH.|SM..TH|PL.NK|GL..|TH.|SH..T|T.|TH.|D.RK|BL..|B.CKGR..ND|.T|.S|..SY|T.|T.LL|TH.|D.PTH|.F|TH.|W.LL|TH.S.|D.YS|.|CH.CK.N|L.G|.S|.|R.R.|D.SH|R.C.|.S|.FT.N|S.RV.D|.N|R..ND|B.WLS|TH.|J.S.|.F|L.M.N|M.K.S|F.N.|P.NCH|TH.|B.X|W.S|T.NL||B.S.D.|TH.|P.RK|TR.NK|TH.|H.X|.R.|S.D|CH.PP.D|C.RN|.ND|G.RB.G.|F..R|H..RS|.|ST..DY|W.RK|F.C.D|.S|.|L.RG.|S.D.|.N|ST.CK.NGS|.S|H.RD|T.|S.LL\n",
      "Prediction (vowels filled)  : THE BIRCH CANOE SLID ON THE SMOOTH PLANK GLEW THE SHEET TO THE DARK BLUE BACKGROUND IT IS EASY TO TELL THE DEPTH OF THE WELL THESE DAYS A CHICKAN LEG IS A RARE DISH RICE IS OFTEN SERVED IN ROUND BUWLS THE JUSE OF LOMON MAKES FINE PUNCH THE BOX WAS TONL  BESIDE THE PARK TRUNK THE HOX ARE SED CHOPPED CORN AND GARBAGE FOUR HOURS A STEADY WORK FACED US E LARGE SIDE AN STOCKINGS IS HARD TO SELL\n",
      "Pattern (every other masked): T.E.B.R.H.C.N.E.S.I.|.N.T.E.S.O.T.|.L.N.|.L.E.T.E.S.E.T.T.|.H.|.A.K.B.U.|.A.K.R.U.D.I.|.S.E.S.|.O.T.L.|.H.|.E.T.|.F.T.E.W.L.|.H.S.|.A.S.A.C.I.K.N.L.G.I.|.|.A.E.D.S.|.I.E.I.|.F.E.|.E.V.D.I.|.O.N.|.O.L.|.H.|.U.E.O.|.E.O.|.A.E.|.I.E.P.N.H.T.E.B.X.W.S.T.N.|.B.S.D.|.H.|.A.K.T.U.K.T.E.H.X.A.E.S.D.C.O.P.D.C.R.|.N.|.A.B.G.|.O.R.H.U.S.A.S.E.D.|.O.K.F.C.D.U.|.|.A.G.|.I.E.I.|.T.C.I.G.|.S.H.R.|.O.S.L.\n",
      "Prediction (filled)         : THE BIRCH CANOE SLIT ON THE SMOOTH PLANK GLUE THE SHEET TO THE DARK BLUE BACKGROUND IT IS EASY TO TELL THE DEPTH OF THE WELL THESE DAYS A CHICKAN LEG IS A RARE DISH RICE IS OF EN SERVED IN ROUND BOULL THE JUSE OF LENON MAKES FINE PUNCH THE BOX WAS TONL  BESIDE THE PARK TRUCK THE HOX ARE SED CHOPPED CORN AND GARBAGE FOUR HOURS A STEADY WORK FACED US E LARGE SIDE IN STOCKINGS IS HARD TO SEL\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#test\n",
    "audio_path = \"samples/test_english.wav\"\n",
    "reference  = \"THE BIRCH CANOE SLID ON THE SMOOTH PLANK GLUE THE SHEET TO THE DARK BLUE BACKGROUND IT IS EASY TO TELL THE DEPTH OF THE WELL THESE DAYS A CHICKEN LEG IS A RARE DISH RICE IS OFTEN SERVED IN ROUND BOWLS THE JUSE OF LEMON MAKES FINE PUNCH THE BOX WAS TONL  BESIDE THE PARK TRUNK THE HOX ARE SED CHOPPED CORN AND GARBAGE FOUR HOURS A STEADY WORK FACED US A LARGE SIDE IN STOCKINGS IS HARD TO SELL\" \n",
    "\n",
    "# Mask all vowels\n",
    "reference = reference.replace(' ', '|')\n",
    "pattern_vowels = [\".\" if ch in \"AEIOUaeiou\" else ch for ch in reference]\n",
    "# Every other char\n",
    "pattern_everyother = [ch if i % 2 == 0 else '.' for i, ch in enumerate(reference)]\n",
    "\n",
    "log_probs = get_log_probs(audio_path)\n",
    "logits = get_logits(audio_path)\n",
    "# wildcard set = all lowercase letters\n",
    "wildcard_ids = [processor.tokenizer.get_vocab()[ch] for ch in processor.tokenizer.get_vocab().keys()\n",
    "                if ch.isalpha() and len(ch) == 1]\n",
    "\n",
    "\n",
    "# Add the space token to wildcard_ids\n",
    "space_token = '|'\n",
    "if space_token in processor.tokenizer.get_vocab():\n",
    "    wildcard_ids.append(processor.tokenizer.get_vocab()[space_token])\n",
    "    print(f\"Added space token '{space_token}' to wildcard set\")\n",
    "\n",
    "\n",
    "    \n",
    "regular_output = ctc_decode_k2(logits, search_beam=20.0, output_beam=8.0)\n",
    "greedy_output = ctc_decode_greedy(logits)\n",
    "hyp_vowels = wildcard_decode_k2(logits, pattern_vowels, wildcard_ids)\n",
    "hyp_everyother = wildcard_decode_k2(logits, pattern_everyother, wildcard_ids)\n",
    "\n",
    "print(\"Reference :\", reference)\n",
    "print(\"CTC prediction \", regular_output)\n",
    "print(\"CTC greedy prediction \", greedy_output)\n",
    "print(\"Pattern (vowels masked)     :\", \"\".join(pattern_vowels))\n",
    "print(\"Prediction (vowels filled)  :\", hyp_vowels)\n",
    "print(\"Pattern (every other masked):\", \"\".join(pattern_everyother))\n",
    "print(\"Prediction (filled)         :\", hyp_everyother)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e215b84",
   "metadata": {},
   "source": [
    "### Diacritization exps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "17a6107a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyarabic import araby\n",
    "\n",
    "diacritics = araby.DIACRITICS\n",
    "arabic_letters = [ch for ch in araby.LETTERS if ch not in araby.DIACRITICS]\n",
    "\n",
    "arabic_audio_path = \"samples/female_ab_00000.wav\"\n",
    "arabic_reference = open('samples/female_ab_00000.txt', 'r', encoding='utf-8').read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e946035",
   "metadata": {},
   "source": [
    "#### Nemo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6379f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nemo.collections.asr as nemo_asr\n",
    "from nemo.collections.common.data.utils import move_data_to_device\n",
    "from omegaconf import open_dict\n",
    "import soundfile as sf\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "asr_model = nemo_asr.models.EncDecHybridRNNTCTCBPEModel.from_pretrained(model_name=\"nvidia/stt_ar_fastconformer_hybrid_large_pcd_v1.0\")\n",
    "# Configure for greedy CTC\n",
    "with open_dict(asr_model.cfg.decoding):\n",
    "    asr_model.cfg.decoding.strategy = \"greedy\"\n",
    "    asr_model.cfg.decoding.compute_timestamps = False # Optional\n",
    "asr_model.change_decoding_strategy(decoder_type=\"ctc\")\n",
    "\n",
    "# test model\n",
    "output = asr_model.transcribe([arabic_audio_path])\n",
    "\n",
    "print(output[0].text)\n",
    "print(arabic_reference)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88225f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jiwer import wer,cer\n",
    "wer_val = wer(output[0].text, arabic_reference)\n",
    "cer_val = cer(output[0].text, arabic_reference)\n",
    "print(f\"WER: {wer_val}\")\n",
    "print(f\"CER: {cer_val}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0723f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get logits for nemo\n",
    "def get_logits(audio_path):\n",
    "    wav, sr = sf.read(audio_path)\n",
    "    \n",
    "    if sr != 16000:\n",
    "        import librosa\n",
    "        wav = librosa.resample(wav, orig_sr=sr, target_sr=16000)\n",
    "        sr = 16000\n",
    "\n",
    "        # get length of wav\n",
    "        length = len(wav)\n",
    "    else:\n",
    "        length = len(wav)\n",
    "\n",
    "    # convert to tensor\n",
    "    wav = torch.from_numpy(wav).unsqueeze(0)\n",
    "    length = torch.tensor([length])\n",
    "\n",
    "    wav = move_data_to_device(wav, device)\n",
    "    length = move_data_to_device(length, device)\n",
    "    # get logits from asr_model\n",
    "    encoded, encoded_length = asr_model.forward(input_signal=wav, input_signal_length=length)\n",
    "    logits = asr_model.ctc_decoder(encoder_output=encoded)\n",
    "\n",
    "    hypotheses = asr_model.ctc_decoding.ctc_decoder_predictions_tensor(\n",
    "            logits,\n",
    "            encoded_length,\n",
    "            return_hypotheses=True,\n",
    "        )\n",
    "\n",
    "    return hypotheses, logits.squeeze(0)\n",
    "\n",
    "asr_model.change_decoding_strategy(asr_model.cfg.decoding, decoder_type=\"ctc\")\n",
    "\n",
    "hypotheses, logits = get_logits(arabic_audio_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d9e4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, k2, soundfile as sf, numpy as np\n",
    "\n",
    "vocab = asr_model.ctc_decoder.vocabulary\n",
    "diac_in_vocab = [diac for diac in diacritics if diac in vocab]\n",
    "token2id = {v: k for k, v in enumerate(vocab)}\n",
    "id2tok = {v: k for k, v in token2id.items()}\n",
    "blank_id = asr_model.ctc_decoding.blank_id\n",
    "\n",
    "def build_pattern_fsa(pattern, wildcard_ids, token2id=token2id):\n",
    "    \"\"\"\n",
    "    pattern: list of characters, '.' for wildcard\n",
    "    token2id: dict mapping from char -> token id\n",
    "    wildcard_ids: allowed token ids for wildcard positions\n",
    "    \"\"\"\n",
    "    arcs = []\n",
    "    state = 0\n",
    "    for i, ch in enumerate(pattern):\n",
    "        if ch == '.':\n",
    "            for wid in wildcard_ids:\n",
    "                arcs.append(f\"{state} {state+1} {wid} {wid} 0.0\")\n",
    "        else:\n",
    "            if ch not in token2id:\n",
    "                continue\n",
    "            tid = token2id[ch]\n",
    "            arcs.append(f\"{state} {state+1} {tid} {tid} 0.0\")\n",
    "        state += 1\n",
    "    arcs.append(f\"{state} 0.0\")\n",
    "    txt = \"\\n\".join(arcs)\n",
    "    fsa = k2.Fsa.from_str(txt, acceptor=False, openfst=True)\n",
    "    return k2.arc_sort(fsa)\n",
    "\n",
    "# WFS Decoding\n",
    "def wildcard_decode_k2(logits, pattern, wildcard_set):\n",
    "    \"\"\"\n",
    "    logits: (T, V) raw logits from model\n",
    "    pattern: list of characters (with '.')\n",
    "    wildcard_set: list of allowed tokens for '.'\n",
    "    \"\"\"\n",
    "    # Convert to torch tensor if needed\n",
    "    if isinstance(logits, np.ndarray):\n",
    "        logits = torch.from_numpy(logits)\n",
    "    \n",
    "    # Convert logits to log probabilities\n",
    "    log_probs = torch.log_softmax(logits, dim=-1)\n",
    "    \n",
    "    T, V = log_probs.shape\n",
    "    dense = k2.DenseFsaVec(log_probs.unsqueeze(0), torch.tensor([[0, 0, T]], dtype=torch.int32))\n",
    "    ctc_topo = k2.arc_sort(k2.ctc_topo(V-1))\n",
    "    pattern_fsa = build_pattern_fsa(pattern, wildcard_set)\n",
    "    decoding_graph = k2.arc_sort(k2.compose(ctc_topo, pattern_fsa))\n",
    "    lattice = k2.intersect_dense_pruned(\n",
    "       decoding_graph, dense,\n",
    "       search_beam=20.0, output_beam=8.0,\n",
    "       min_active_states=30, max_active_states=10000)\n",
    "    \n",
    "    best_path = k2.shortest_path(lattice, use_double_scores=False)\n",
    "    aux = k2.get_aux_labels(best_path)[0]\n",
    "    hyp_ids = [x for x in aux if x >= 0]\n",
    "    \n",
    "    result = \"\".join(id2tok[i] for i in hyp_ids)\n",
    "    \n",
    "    # Replace word delimiter with space\n",
    "    word_delim = processor.tokenizer.word_delimiter_token\n",
    "    if word_delim:\n",
    "        result = result.replace(word_delim, \" \")\n",
    "    \n",
    "    return result.strip()\n",
    "\n",
    "# CTC Decoding\n",
    "def ctc_decode_k2(logits, search_beam=20.0, output_beam=8.0):\n",
    "    \"\"\"\n",
    "    CTC decoding using k2 with beam settings (no pattern constraints).\n",
    "    Takes raw logits as input.\n",
    "    \"\"\"\n",
    "    # Convert to torch tensor if needed\n",
    "    if isinstance(logits, np.ndarray):\n",
    "        logits = torch.from_numpy(logits)\n",
    "    \n",
    "    # Convert logits to log probabilities\n",
    "    log_probs = torch.log_softmax(logits, dim=-1)\n",
    "    \n",
    "    T, V = log_probs.shape\n",
    "    dense = k2.DenseFsaVec(log_probs.unsqueeze(0), torch.tensor([[0, 0, T]], dtype=torch.int32))\n",
    "    ctc_topo = k2.arc_sort(k2.ctc_topo(V-1))\n",
    "    \n",
    "    # No pattern FSA - just CTC topology\n",
    "    lattice = k2.intersect_dense_pruned(\n",
    "        ctc_topo, dense,\n",
    "        search_beam=search_beam, \n",
    "        output_beam=output_beam,\n",
    "        min_active_states=30, \n",
    "        max_active_states=10000\n",
    "    )\n",
    "    \n",
    "    best_path = k2.shortest_path(lattice, use_double_scores=False)\n",
    "    aux = k2.get_aux_labels(best_path)[0]\n",
    "    hyp_ids = [x for x in aux if x >= 0]\n",
    "    \n",
    "    result = \"\".join(id2tok[i] for i in hyp_ids)\n",
    "    \n",
    "    # Replace word delimiter with space\n",
    "    word_delim = processor.tokenizer.word_delimiter_token\n",
    "    if word_delim:\n",
    "        result = result.replace(word_delim, \" \")\n",
    "    \n",
    "    return result.strip()\n",
    "\n",
    "def ctc_decode_greedy(logits):\n",
    "    \"\"\"\n",
    "    Greedy CTC decoding: argmax at each frame, then collapse repeats and remove blanks.\n",
    "    Takes raw logits as input (not log_probs).\n",
    "    \"\"\"\n",
    "    # Convert to torch tensor if needed\n",
    "    if isinstance(logits, np.ndarray):\n",
    "        logits = torch.from_numpy(logits)\n",
    "    \n",
    "    # Get the most probable token at each frame (argmax on logits)\n",
    "    greedy_ids = logits.argmax(dim=-1)  # Shape: (T,)\n",
    "    \n",
    "    # Collapse repeats and remove blanks\n",
    "    output = []\n",
    "    prev_id = None\n",
    "    \n",
    "    for token_id in greedy_ids.tolist():\n",
    "        if token_id == blank_id:\n",
    "            prev_id = None  # Reset on blank\n",
    "            continue\n",
    "        if token_id != prev_id:  # Only add if different from previous\n",
    "            output.append(token_id)\n",
    "            prev_id = token_id\n",
    "    \n",
    "    result = \"\".join(id2tok[i] for i in output)\n",
    "    \n",
    "    # Replace word delimiter with space\n",
    "    word_delim = processor.tokenizer.word_delimiter_token\n",
    "    if word_delim:\n",
    "        result = result.replace(word_delim, \" \")\n",
    "    \n",
    "    return result.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f223b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern_diacritics = [ch if ch not in diac_in_vocab else '.' for ch in arabic_reference]\n",
    "\n",
    "hyp, logits = get_logits(arabic_audio_path)\n",
    "wildcard_ids = [ i for i,ch in enumerate(vocab)\n",
    "                if ch in diac_in_vocab]\n",
    "\n",
    "# select logits for the diacritics only\n",
    "logits_diacritics = logits[:, wildcard_ids]\n",
    "# regular_output = ctc_decode_k2(logits.squeeze(0), search_beam=20.0, output_beam=8.0)\n",
    "#################### Greedy CTC Decoding ####################\n",
    "# greedy_output = ctc_decode_greedy(logits.squeeze(0))\n",
    "# print(greedy_output)\n",
    "# print(arabic_reference)\n",
    "############################################################\n",
    "\n",
    "#################### WFS Decoding ############################\n",
    "hyp_diacritics = wildcard_decode_k2(logits_diacritics, pattern_diacritics, wildcard_ids)\n",
    "print(hyp_diacritics)\n",
    "print(arabic_reference)\n",
    "############################################################\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65a57c7",
   "metadata": {},
   "source": [
    "#### Wav2Vec2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "70750c56",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rufael/Projects/diac-btc/.venv/lib/python3.12/site-packages/transformers/configuration_utils.py:309: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoProcessor, AutoModelForCTC\n",
    "from jiwer import wer\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"jonatasgrosman/wav2vec2-large-xlsr-53-arabic\")\n",
    "model = AutoModelForCTC.from_pretrained(\"jonatasgrosman/wav2vec2-large-xlsr-53-arabic\")\n",
    "\n",
    "def get_logits(audio_path):\n",
    "    wav, sr = sf.read(audio_path)\n",
    "\n",
    "    if sr != 16000:\n",
    "        import librosa\n",
    "        wav = librosa.resample(wav, orig_sr=sr, target_sr=16000)\n",
    "\n",
    "    wav = wav[None, :]\n",
    "\n",
    "    processed_input = processor(\n",
    "        audio=wav,\n",
    "        sampling_rate=16000,\n",
    "        return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "    output = model(processed_input.input_values)\n",
    "    logits = output.logits\n",
    "    predicted_ids = torch.argmax(logits, dim=-1)\n",
    "\n",
    "    transcription = processor.decode(predicted_ids[0])\n",
    "\n",
    "    return logits, transcription\n",
    "\n",
    "logits, transcription = get_logits(arabic_audio_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d2629faf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "فَمِنَ الرَّامِ السُّوَيْديِّ - أُسْكُرْ سَواهِنْ - الَّذِي شَرَكَ فِي أُولَمْبِيَادِ أَلْفُتِسُعْمِئَةٍ وَعِشْرونَ - حِينَ كَانَ يَبْلُغُ اثْنَيْنِ وَسَبرينَ عَاماً - وَمِئتَيْنِ وَوَاحِدُ وثَمانينَ يَوْمًا - إِلَى الفارسَتَيْنِ أَنْْيُوزْلَندِيَّةِ - جيولِي بروغْهَامْ - والأَسْتْرَاليَّةِ مارِي هَنَا - الَّتَيْنِ تُشَارِكَانِ فِي أُولَمْبِيَادِرِيُو - وَهُمَا فِي الْوَاحِدِ وَالسِّتّينَ مِنَ الْعُمُرِ\n",
      "فَمِنَ الرَّامِي السُّوِيدِيِّ أُوسْكَار سَوَاهِنْ الَّذِي شَارَكَ فِي أُولُمْبِيَادِ  أَلْفْ وَتِسْعُمِئَةٍ وَعِشْرُونْ حِينَ كَانَ يَبْلُغُ إَثْنَينْ وَ سَبْعينَ  عَامًا وَمِئَتَينْ وَوَاحِدْ وَثَمَانِينَ يَوْمًا إِلَى الْفَارِسَتَيْنِ النِّيُوزِيلَنْدِيَّةِ جُولِي برُوغْهَام وَالْأُسْتُرَالِيَّةِ مَارِي هَانَا اللَّتَيْنِ تُشَارِكَانِ فِي أُولُمْبِيَّادِ رِيُو وَهُمَا فِي الْوَاحِدِ وَالسِّتِينَ مِنَ الْعُمُر\n",
      "0.8095238095238095\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(transcription)\n",
    "print(arabic_reference)\n",
    "\n",
    "# calculate wer\n",
    "wer = wer(arabic_reference, transcription)\n",
    "print(wer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "15501c83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "diacritics in vocab: ['ً', 'ٌ', 'ٍ', 'َ', 'ُ', 'ِ', 'ّ', 'ْ']\n",
      "vocab size: 51\n",
      "blank_id: 0\n",
      "token2id: {'<pad>': 0, '<s>': 1, '</s>': 2, '<unk>': 3, '|': 4, '-': 5, 'ء': 6, 'آ': 7, 'أ': 8, 'ؤ': 9, 'إ': 10, 'ئ': 11, 'ا': 12, 'ب': 13, 'ة': 14, 'ت': 15, 'ث': 16, 'ج': 17, 'ح': 18, 'خ': 19, 'د': 20, 'ذ': 21, 'ر': 22, 'ز': 23, 'س': 24, 'ش': 25, 'ص': 26, 'ض': 27, 'ط': 28, 'ظ': 29, 'ع': 30, 'غ': 31, 'ـ': 32, 'ف': 33, 'ق': 34, 'ك': 35, 'ل': 36, 'م': 37, 'ن': 38, 'ه': 39, 'و': 40, 'ى': 41, 'ي': 42, 'ً': 43, 'ٌ': 44, 'ٍ': 45, 'َ': 46, 'ُ': 47, 'ِ': 48, 'ّ': 49, 'ْ': 50}\n"
     ]
    }
   ],
   "source": [
    "vocab = list(processor.tokenizer.get_vocab().keys())\n",
    "id2tok = {v: k for k, v in processor.tokenizer.get_vocab().items()}\n",
    "token2id = {v: k for k, v in enumerate(vocab)}\n",
    "blank_id = processor.tokenizer.pad_token_id\n",
    "\n",
    "diac_in_vocab = [diac for diac in diacritics if diac in vocab]\n",
    "\n",
    "print(f\"diacritics in vocab: {diac_in_vocab}\")\n",
    "print(f\"vocab size: {len(vocab)}\")\n",
    "print(f\"blank_id: {blank_id}\")\n",
    "print(f\"token2id: {token2id}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "067f558e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, k2, soundfile as sf, numpy as np\n",
    "\n",
    "def build_pattern_fsa(pattern, wildcard_ids, token2id=token2id):\n",
    "    \"\"\"\n",
    "    pattern: list of characters, '.' for wildcard\n",
    "    token2id: dict mapping from char -> token id\n",
    "    wildcard_ids: allowed token ids for wildcard positions\n",
    "    \"\"\"\n",
    "    arcs = []\n",
    "    state = 0\n",
    "    for i, ch in enumerate(pattern):\n",
    "        if ch == '.':\n",
    "            for wid in wildcard_ids:\n",
    "                arcs.append(f\"{state} {state+1} {wid} {wid} 0.0\")\n",
    "        else:\n",
    "            if ch not in token2id:\n",
    "                continue\n",
    "            tid = token2id[ch]\n",
    "            arcs.append(f\"{state} {state+1} {tid} {tid} 0.0\")\n",
    "        state += 1\n",
    "    arcs.append(f\"{state} 0.0\")\n",
    "    txt = \"\\n\".join(arcs)\n",
    "    fsa = k2.Fsa.from_str(txt, acceptor=False, openfst=True)\n",
    "    return k2.arc_sort(fsa)\n",
    "\n",
    "# WFS Decoding\n",
    "def wildcard_decode_k2(logits, pattern, wildcard_set):\n",
    "    \"\"\"\n",
    "    logits: (T, V) raw logits from model\n",
    "    pattern: list of characters (with '.')\n",
    "    wildcard_set: list of allowed tokens for '.'\n",
    "    \"\"\"\n",
    "    # Convert to torch tensor if needed\n",
    "    if isinstance(logits, np.ndarray):\n",
    "        logits = torch.from_numpy(logits)\n",
    "    \n",
    "    # Convert logits to log probabilities\n",
    "    log_probs = torch.log_softmax(logits, dim=-1)\n",
    "    \n",
    "    T, V = log_probs.shape\n",
    "    dense = k2.DenseFsaVec(log_probs.unsqueeze(0), torch.tensor([[0, 0, T]], dtype=torch.int32))\n",
    "    ctc_topo = k2.arc_sort(k2.ctc_topo(V-1))\n",
    "    pattern_fsa = build_pattern_fsa(pattern, wildcard_set)\n",
    "    decoding_graph = k2.arc_sort(k2.compose(ctc_topo, pattern_fsa))\n",
    "    lattice = k2.intersect_dense_pruned(\n",
    "       decoding_graph, dense,\n",
    "       search_beam=20.0, output_beam=8.0,\n",
    "       min_active_states=30, max_active_states=10000)\n",
    "    \n",
    "    best_path = k2.shortest_path(lattice, use_double_scores=False)\n",
    "    aux = k2.get_aux_labels(best_path)[0]\n",
    "    hyp_ids = [x for x in aux if x >= 0]\n",
    "    \n",
    "    result = \"\".join(id2tok[i] for i in hyp_ids)\n",
    "    \n",
    "    # Replace word delimiter with space\n",
    "    word_delim = processor.tokenizer.word_delimiter_token\n",
    "    if word_delim:\n",
    "        result = result.replace(word_delim, \" \")\n",
    "    \n",
    "    return result.strip()\n",
    "\n",
    "# CTC Decoding\n",
    "def ctc_decode_k2(logits, search_beam=20.0, output_beam=8.0):\n",
    "    \"\"\"\n",
    "    CTC decoding using k2 with beam settings (no pattern constraints).\n",
    "    Takes raw logits as input.\n",
    "    \"\"\"\n",
    "    # Convert to torch tensor if needed\n",
    "    if isinstance(logits, np.ndarray):\n",
    "        logits = torch.from_numpy(logits)\n",
    "    \n",
    "    # Convert logits to log probabilities\n",
    "    log_probs = torch.log_softmax(logits, dim=-1)\n",
    "    \n",
    "    T, V = log_probs.shape\n",
    "    dense = k2.DenseFsaVec(log_probs.unsqueeze(0), torch.tensor([[0, 0, T]], dtype=torch.int32))\n",
    "    ctc_topo = k2.arc_sort(k2.ctc_topo(V-1))\n",
    "    \n",
    "    # No pattern FSA - just CTC topology\n",
    "    lattice = k2.intersect_dense_pruned(\n",
    "        ctc_topo, dense,\n",
    "        search_beam=search_beam, \n",
    "        output_beam=output_beam,\n",
    "        min_active_states=30, \n",
    "        max_active_states=10000\n",
    "    )\n",
    "    \n",
    "    best_path = k2.shortest_path(lattice, use_double_scores=False)\n",
    "    aux = k2.get_aux_labels(best_path)[0]\n",
    "    hyp_ids = [x for x in aux if x >= 0]\n",
    "    \n",
    "    result = \"\".join(id2tok[i] for i in hyp_ids)\n",
    "    \n",
    "    # Replace word delimiter with space\n",
    "    word_delim = processor.tokenizer.word_delimiter_token\n",
    "    if word_delim:\n",
    "        result = result.replace(word_delim, \" \")\n",
    "    \n",
    "    return result.strip()\n",
    "\n",
    "def ctc_decode_greedy(logits):\n",
    "    \"\"\"\n",
    "    Greedy CTC decoding: argmax at each frame, then collapse repeats and remove blanks.\n",
    "    Takes raw logits as input (not log_probs).\n",
    "    \"\"\"\n",
    "    # Convert to torch tensor if needed\n",
    "    if isinstance(logits, np.ndarray):\n",
    "        logits = torch.from_numpy(logits)\n",
    "    \n",
    "    # Get the most probable token at each frame (argmax on logits)\n",
    "    greedy_ids = logits.argmax(dim=-1)  # Shape: (T,)\n",
    "    \n",
    "    # Collapse repeats and remove blanks\n",
    "    output = []\n",
    "    prev_id = None\n",
    "    \n",
    "    for token_id in greedy_ids.tolist():\n",
    "        if token_id == blank_id:\n",
    "            prev_id = None  # Reset on blank\n",
    "            continue\n",
    "        if token_id != prev_id:  # Only add if different from previous\n",
    "            output.append(token_id)\n",
    "            prev_id = token_id\n",
    "    \n",
    "    result = \"\".join(id2tok[i] for i in output)\n",
    "    \n",
    "    # Replace word delimiter with space\n",
    "    word_delim = processor.tokenizer.word_delimiter_token\n",
    "    if word_delim:\n",
    "        result = result.replace(word_delim, \" \")\n",
    "    \n",
    "    return result.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e13f4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "greedy_output  : فَمِنَ الرَّامِ السُّوَيْديِّ  أُسْكُرْ سَواهِنْ  الَّذِي شَرَكَ فِي أُولَمْبِيَادِ أَلْفُتِسُعْمِئَةٍ وَعِشْرونَ  حِينَ كَانَ يَبْلُغُ اثْنَيْنِ وَسَبرينَ عَاماً  وَمِئتَيْنِ وَوَاحِدُ وثَمانينَ يَوْمًا  إِلَى الفارسَتَيْنِ أَنْْيُوزْلَندِيَّةِ  جيولِي بروغْهَامْ  والأَسْتْرَاليَّةِ مارِي هَنَا  الَّتَيْنِ تُشَارِكَانِ فِي أُولَمْبِيَادِرِيُو  وَهُمَا فِي الْوَاحِدِ وَالسِّتّينَ مِنَ الْعُمُرِ\n",
      "transcription  : فَمِنَ الرَّامِ السُّوَيْديِّ  أُسْكُرْ سَواهِنْ  الَّذِي شَرَكَ فِي أُولَمْبِيَادِ أَلْفُتِسُعْمِئَةٍ وَعِشْرونَ  حِينَ كَانَ يَبْلُغُ اثْنَيْنِ وَسَبرينَ عَاماً  وَمِئتَيْنِ وَوَاحِدُ وثَمانينَ يَوْمًا  إِلَى الفارسَتَيْنِ أَنْْيُوزْلَندِيَّةِ  جيولِي بروغْهَامْ  والأَسْتْرَاليَّةِ مارِي هَنَا  الَّتَيْنِ تُشَارِكَانِ فِي أُولَمْبِيَادِرِيُو  وَهُمَا فِي الْوَاحِدِ وَالسِّتّينَ مِنَ الْعُمُرِ\n",
      "reference      : فَمِنَ الرَّامِي السُّوِيدِيِّ أُوسْكَار سَوَاهِنْ الَّذِي شَارَكَ فِي أُولُمْبِيَادِ أَلْفْ وَتِسْعُمِئَةٍ وَعِشْرُونْ حِينَ كَانَ يَبْلُغُ إَثْنَينْ وَ سَبْعينَ عَامًا وَمِئَتَينْ وَوَاحِدْ وَثَمَانِينَ يَوْمًا إِلَى الْفَارِسَتَيْنِ النِّيُوزِيلَنْدِيَّةِ جُولِي برُوغْهَام وَالْأُسْتُرَالِيَّةِ مَارِي هَانَا اللَّتَيْنِ تُشَارِكَانِ فِي أُولُمْبِيَّادِ رِيُو وَهُمَا فِي الْوَاحِدِ وَالسِّتِينَ مِنَ الْعُمُر\n",
      "\n",
      "regular_output  : فَمِنَ الرَّامِ السُّوَيْديِّ  أُسْكُرْ سَواهِنْ  الَّذِي شَرَكَ فِي أُولَمْبِيَادِ أَلْفُتِسُعْمِئَةٍ وَعِشْرونَ  حِينَ كَانَ يَبْلُغُ اثْنَيْنِ وَسَبرينَ عَاماً  وَمِئتَيْنِ وَوَاحِدُ وثَمانينَ يَوْمًا  إِلَى الفارسَتَيْنِ أَنْْيُوزْلَندِيَّةِ  جيولِي بروغْهَامْ  والأَسْتْرَاليَّةِ مارِي هَنَا  الَّتَيْنِ تُشَارِكَانِ فِي أُولَمْبِيَادِرِيُو  وَهُمَا فِي الْوَاحِدِ وَالسِّتّينَ مِنَ الْعُمُرِ\n",
      "transcription   : فَمِنَ الرَّامِ السُّوَيْديِّ  أُسْكُرْ سَواهِنْ  الَّذِي شَرَكَ فِي أُولَمْبِيَادِ أَلْفُتِسُعْمِئَةٍ وَعِشْرونَ  حِينَ كَانَ يَبْلُغُ اثْنَيْنِ وَسَبرينَ عَاماً  وَمِئتَيْنِ وَوَاحِدُ وثَمانينَ يَوْمًا  إِلَى الفارسَتَيْنِ أَنْْيُوزْلَندِيَّةِ  جيولِي بروغْهَامْ  والأَسْتْرَاليَّةِ مارِي هَنَا  الَّتَيْنِ تُشَارِكَانِ فِي أُولَمْبِيَادِرِيُو  وَهُمَا فِي الْوَاحِدِ وَالسِّتّينَ مِنَ الْعُمُرِ\n",
      "reference       : فَمِنَ الرَّامِي السُّوِيدِيِّ أُوسْكَار سَوَاهِنْ الَّذِي شَارَكَ فِي أُولُمْبِيَادِ أَلْفْ وَتِسْعُمِئَةٍ وَعِشْرُونْ حِينَ كَانَ يَبْلُغُ إَثْنَينْ وَ سَبْعينَ عَامًا وَمِئَتَينْ وَوَاحِدْ وَثَمَانِينَ يَوْمًا إِلَى الْفَارِسَتَيْنِ النِّيُوزِيلَنْدِيَّةِ جُولِي برُوغْهَام وَالْأُسْتُرَالِيَّةِ مَارِي هَانَا اللَّتَيْنِ تُشَارِكَانِ فِي أُولُمْبِيَّادِ رِيُو وَهُمَا فِي الْوَاحِدِ وَالسِّتِينَ مِنَ الْعُمُر\n",
      "\n",
      "pattern         : ف.م.ن.|الر..ام.ي|الس..و.يد.ي..|أ.وس.ك.ار|س.و.اه.ن.|ال..ذ.ي|ش.ار.ك.|ف.ي|أ.ول.م.ب.ي.اد.||أ.ل.ف.|و.ت.س.ع.م.ئ.ة.|و.ع.ش.ر.ون.|ح.ين.|ك.ان.|ي.ب.ل.غ.|إ.ث.ن.ين.|و.|س.ب.عين.||ع.ام.ا|و.م.ئ.ت.ين.|و.و.اح.د.|و.ث.م.ان.ين.|ي.و.م.ا|إ.ل.ى|ال.ف.ار.س.ت.ي.ن.|الن..ي.وز.يل.ن.د.ي..ة.|ج.ول.ي|بر.وغ.ه.ام|و.ال.أ.س.ت.ر.ال.ي..ة.|م.ار.ي|ه.ان.ا|الل..ت.ي.ن.|ت.ش.ار.ك.ان.|ف.ي|أ.ول.م.ب.ي..اد.|ر.ي.و|و.ه.م.ا|ف.ي|ال.و.اح.د.|و.الس..ت.ين.|م.ن.|ال.ع.م.ر\n",
      "hyp_diacritics  : فَمِنَ الرَّامِي السُّوَيدِيِّ أُوسْكُار سَوَاهِنْ الَّذِي شَارَكَ فِي أُولَمْبِيَادِ أَلْفُ وْتِسُعْمِئَةٍ وَعِشْرُونَ حِينَ كَانَ يَبْلُغُ إَثْنَينِ وَ سَبْعينَ عَامًا وَمِئَتَينِ وَوَاحِدُ وَثَمَانِينَ يَوْمًا إِلَى الْفَارِسَتَيْنِ النْْيُوزْيلَنْدِيَّةِ جُولِي برُوغْهَام وَالْأَسْتْرَالِيَّةِ مَارِي هَانَا اللَّتَيْنِ تُشَارِكَانِ فِي أُولَمْبِيَّادِ رِيُو وَهُمَا فِي الْوَاحِدِ وَالسِّتّينَ مِنَ الْعُمُر\n",
      "transcription   : فَمِنَ الرَّامِ السُّوَيْديِّ  أُسْكُرْ سَواهِنْ  الَّذِي شَرَكَ فِي أُولَمْبِيَادِ أَلْفُتِسُعْمِئَةٍ وَعِشْرونَ  حِينَ كَانَ يَبْلُغُ اثْنَيْنِ وَسَبرينَ عَاماً  وَمِئتَيْنِ وَوَاحِدُ وثَمانينَ يَوْمًا  إِلَى الفارسَتَيْنِ أَنْْيُوزْلَندِيَّةِ  جيولِي بروغْهَامْ  والأَسْتْرَاليَّةِ مارِي هَنَا  الَّتَيْنِ تُشَارِكَانِ فِي أُولَمْبِيَادِرِيُو  وَهُمَا فِي الْوَاحِدِ وَالسِّتّينَ مِنَ الْعُمُرِ\n",
      "reference       : فَمِنَ الرَّامِي السُّوِيدِيِّ أُوسْكَار سَوَاهِنْ الَّذِي شَارَكَ فِي أُولُمْبِيَادِ أَلْفْ وَتِسْعُمِئَةٍ وَعِشْرُونْ حِينَ كَانَ يَبْلُغُ إَثْنَينْ وَ سَبْعينَ عَامًا وَمِئَتَينْ وَوَاحِدْ وَثَمَانِينَ يَوْمًا إِلَى الْفَارِسَتَيْنِ النِّيُوزِيلَنْدِيَّةِ جُولِي برُوغْهَام وَالْأُسْتُرَالِيَّةِ مَارِي هَانَا اللَّتَيْنِ تُشَارِكَانِ فِي أُولُمْبِيَّادِ رِيُو وَهُمَا فِي الْوَاحِدِ وَالسِّتِينَ مِنَ الْعُمُر\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def print_util(data):\n",
    "    col_width = max(len(row[0]) for row in data) + 2\n",
    "    for key, value in data:\n",
    "        print(f\"{key:<{col_width}}: {value}\")\n",
    "    print()  # extra newline for separation\n",
    "\n",
    "def clean_text(text, remove_diacritics=False):\n",
    "    if remove_diacritics:\n",
    "        text = araby.strip_diacritics(text)\n",
    "    # replace | and - with space\n",
    "    text = text.replace('|', ' ').replace('-', ' ')\n",
    "    # collapse extra spaces\n",
    "    text = text.replace('  ', ' ')\n",
    "\n",
    "    return text\n",
    "\n",
    "arabic_reference = arabic_reference.replace(' ', '|')\n",
    "pattern_diacritics = [ch if ch not in diac_in_vocab else '.' for ch in arabic_reference]\n",
    "pattern_everyother = [ch if ch not in diac_in_vocab else '.' for ch in araby.strip_diacritics(arabic_reference)]\n",
    "\n",
    "logits, transcription  = get_logits(arabic_audio_path)\n",
    "wildcard_ids = [ i for i,ch in enumerate(vocab)\n",
    "                if ch in diac_in_vocab]\n",
    "\n",
    "# select logits for the diacritics only\n",
    "# logits_diacritics = logits[:, wildcard_ids]\n",
    "# regular_output = ctc_decode_k2(logits.squeeze(0), search_beam=20.0, output_beam=8.0)\n",
    "#################### Greedy CTC ####################\n",
    "greedy_output = ctc_decode_greedy(logits.squeeze(0))\n",
    "data = [\n",
    "    (\"greedy_output\", clean_text(greedy_output)),\n",
    "    (\"transcription\",  clean_text(transcription)),\n",
    "    (\"reference\",      clean_text(arabic_reference))\n",
    "]\n",
    "print_util(data)\n",
    "############################################################\n",
    "\n",
    "#################### CTC K2 ########################\n",
    "regular_output = ctc_decode_k2(logits.squeeze(0), search_beam=20.0, output_beam=8.0)\n",
    "data = [\n",
    "    (\"regular_output\", clean_text(regular_output)),\n",
    "    (\"transcription\",  clean_text(transcription)),\n",
    "    (\"reference\",      clean_text(arabic_reference))\n",
    "]\n",
    "print_util(data)\n",
    "############################################################\n",
    "\n",
    "#################### CTC+WFS ############################\n",
    "hyp_diacritics = wildcard_decode_k2(logits.squeeze(0), pattern_diacritics, wildcard_ids)\n",
    "# Format the outputs as aligned columns in a table-like style\n",
    "\n",
    "data = [\n",
    "    (\"pattern\", \"\".join(pattern_diacritics)),\n",
    "    (\"hyp_diacritics\", clean_text(hyp_diacritics)),\n",
    "    (\"transcription\",  clean_text(transcription)),\n",
    "    (\"reference\",      clean_text(arabic_reference))\n",
    "]\n",
    "print_util(data)\n",
    "############################################################\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "eb83aae5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wsft WER: 0.3333333333333333\n",
      "greedy WER: 0.6666666666666666\n",
      "k2_ctc WER: 0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "from jiwer import wer\n",
    "\n",
    "def calculate_wer(hyp, ref):\n",
    "    hyp = clean_text(hyp)\n",
    "    ref = clean_text(ref)\n",
    "    return wer(ref, hyp)\n",
    "\n",
    "\n",
    "print(f\"wsft WER: {calculate_wer(hyp_diacritics, arabic_reference)}\")\n",
    "print(f\"greedy WER: {calculate_wer(greedy_output, arabic_reference)}\")\n",
    "print(f\"k2_ctc WER: {calculate_wer(regular_output, arabic_reference)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a1f08a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
